% document formatting
\documentclass[10pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[left=1in,right=1in,top=1in,bottom=1in]{geometry}
\usepackage[T1]{fontenc}
\usepackage{xcolor}

% math symbols, etc.
\usepackage{amsmath, amsfonts, amssymb, amsthm}

% lists
\usepackage{enumerate}

% images
\usepackage{graphicx} % for images
\usepackage{tikz} % draw stuff

% code blocks
\usepackage{minted, listings}

% links
\usepackage{hyperref}

% \mathbb{1} symbol
\usepackage{dsfont}

\graphicspath{{./assets/images}}

\newcommand{\solution}{\textbf{Solution:}} 

\title{COM SCI C121 Week 6}

\author{Aidan Jan}
\date{\today}

\begin{document}
\maketitle

\subsection*{EM Algorithm}
As review:
\begin{center}
    \includegraphics*[scale=0.4]{W5_6}
\end{center}
Essentially, we use the estimated transcript abundances to calculate the probability of the reads, then use the probabilities of the reads we attempt to maximize the probabilities by modifying the transcript abundances.
\begin{itemize}
    \item The sections of the pie chart are denoted by $\theta_i$, where $i$ is a number or some sort of identifier for the section.  In this case, we have three colors, so let $\theta_r$ represent the red abundance (beginning with 0.33), $\theta_g$ represent the green abundance, and $\theta_b$ represent the blue abundance.
    \item In the E-step, we take the ratios in the pie chart to give a probability to each of the reads. 
    \item In the M-step, we take the ratios to what we actually see on the reads to recalculate the abundances.  In the first step,
    \begin{itemize}
        \item Red sections take up $2\frac{1}{3}$ of the $5$ pie charts total.
        \item Green sections take up $1\frac{1}{3}$ of the $5$ pie charts.
        \item Blue sections take up $1\frac{1}{3}$ of the $5$ pie charts.
        \item Then, we update the $\theta_r, \theta_g, \theta_b$ values.
        \item $\theta_r = \frac{7}{3} / 5 = \frac{7}{15} \approx 0.47$, $\theta_g = \frac{4}{3} / 5 = \frac{4}{15} \approx 0.27$, $\theta_b = \frac{4}{3} / 5 = \frac{4}{15} \approx 0.27$
    \end{itemize}
    \item These steps are then repeated until the numbers stop changing.
\end{itemize}
In math terms:
\begin{flushleft}
\[L(\theta; x) = P(x \vert \theta) = \int P(x, z \vert \theta) \text{d}z = \int P(x \vert \theta, z) P(z \vert \theta) \text{d}z\]
\end{flushleft}
\begin{align*}
    \text{E-Step:~~~}& z \vert x, \theta^{(t)} = [\log(L(\theta; x, z))]\\
    \text{M-Step:~~~}& \theta^{(t + 1)} = \text{argmax}_\theta \:\: Q(\theta \vert \theta^{(t)})
\end{align*}
where 
\begin{itemize}
    \item $z$ is the assignment (or true origin) of the sample.
    \item $Q$ is the expectation of the log likelihood.
    \begin{align*}
        Q(\theta \vert \theta^{(t)}) &= E_{z \vert r, \theta^{(t)}} [\log P(r, z \vert \theta)]\\
        &= \sum_{n, i, j} E_{z \vert r, \theta^{(t)}} [\mathds{1}(z_{nij} = 1)] \log\left(\frac{\theta_i}{l_i} P(r \vert z)\right)\\
        \\
        E_{z \vert r, \theta^{(t)}} [\mathds{1}(z_{nij} = 1)] &= P_{z \vert r, \theta^{(t)}} (z_{nij} = 1 \vert r, \theta^{(t)})\\
        &= \frac{P(r_n, z_{nij} \vert \theta^{(t)})}{P(r_n \vert \theta^{(t)})}\\
        &= \frac{P(r_n \vert z_{nij} = 1) P(O_n = i) P(S_n = j \vert O_n = i)}{\sum_k \sum_l P(r_n \vert z_{nkl} = 1) P(O_n = k) P(S_n = l \vert O_n = k)}\\
        &= \frac{c \cdot \theta_i^{(t)} \cdot \frac{1}{\tilde{l_i}}}{\sum_k \sum_l \frac{\theta_k^{(t)}}{\tilde{l_k}} \cdot 1}
    \end{align*}
    \item $c$ is a constant, where $c = P(r_n \vert z_{nij} = 1)$.  (in this example, we assume $c = 1$.)
    \item For this point, assume $i = g$, referring to the colors of the pie chart ($g$ = green)
    \begin{itemize}
        \item The numerator of the last line of math represents the area of green on the chart, over the actual likelihood of green based on the probability.  The denominator represents the sum of all slices on the pie chart (e.g., the number of pies)
    \end{itemize}
    \item Note that this matches the intuition we had earlier about the $M$-step.
\end{itemize}


\subsection*{Where EM is Used}
\begin{itemize}
    \item Remember that when we know everything (like, the entire distribution), we can assume that $X_r \sim \text{Poisson}(\lambda)$
    \item However, we don't know everything.  So we have to use EM to estimate the distribution.
    \item In most real life applications of sequencing, there are a lot of errors, a lot of isomers, and a lot of reads.  EM can be used to determine the actual alignment more accurately than a naive bayes.
    \item EM is used in Kallisto:
    \begin{center}
        \includegraphics*[scale=0.5]{W6_1.png}
    \end{center}
\end{itemize}



\end{document}