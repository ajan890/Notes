% document formatting
\documentclass[10pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[left=1in,right=1in,top=1in,bottom=1in]{geometry}
\usepackage[T1]{fontenc}
\usepackage{xcolor}

% math symbols, etc.
\usepackage{amsmath, amsfonts, amssymb, amsthm}

% lists
\usepackage{enumerate, enumitem}

% images
\usepackage{graphicx} % for images

% code blocks
\usepackage{minted, listings} 

% verbatim greek
\usepackage{alphabeta}

\graphicspath{{./assets/images}}

\newcommand{\solution}{\textbf{Solution:}} 

\title{COM SCI M151B Week 7}

\author{Aidan Jan}
\date{\today}

\begin{document}
\maketitle

\subsection*{Cache Block}
\begin{itemize}
    \item Instead of storing 1 byte per row, we can store a block with multiple bytes.
    \begin{itemize}
        \item Every time we need to load something to the cache, we load it at block level.  (Spatial Locality)
        \item We still send things to the CPU at byte level
        \begin{itemize}
            \item How to do that? $\rightarrow$ We need \textit{block offset} to decide which byte within the block should be selected!
        \end{itemize}
        \item How big a block should be?  It depends!  Typically somewhere between 8B-64B.
    \end{itemize}
    \item Therefore, \textbf{Cache Address = \{tag, index, block offset\}}
\end{itemize}
\subsection*{A 64B Direct Mapped Cache}
\begin{center}
    \includegraphics*[scale=0.8]{W7_1.png}
\end{center}
\begin{itemize}
    \item Address is used to select index, of tag, which selects "rows", and block offset selects the word "columns".
\end{itemize}

\subsection*{A 64B 2-way Set Associative Cache}
\begin{center}
    \includegraphics*[scale=0.8]{W7_2.png}
\end{center}
\begin{itemize}
    \item Compared to the direct mapped cache, the tag is two bits shorter to make way for an index.
    \item Here, the tag is used to select which "Way" is used, and the two bits of the index selects the row.  The block offset then gets the column.
\end{itemize}

\subsection*{CBS}
\begin{itemize}
    \item C = log(bytes per cache)
    \item B = log(bytes per block)
    \item S = log(blocks per set)
\end{itemize}
\begin{center}
    \includegraphics*[scale=0.9]{W7_3.png}
\end{center}

\subsection*{How Big A Block Should Be?}
\begin{itemize}
    \item Bringing more data is nice because you have spatial locality
    \item However, it is not always the best idea because it increases overhead
    \item You are essentially making a trade off between miss rate and miss penalty.
\end{itemize}

\subsection*{Reducing Miss Rate}
\begin{itemize}
    \item Miss rate can be reduced by making blocks bigger, but that comes at the trade-off of miss penalty.
    \item What if we increase associativity?  (e.g., add more ways)
    \begin{itemize}
        \item More ways leads to higher hit time.  (As log(cache size) increases, miss rate drops, but it drops following an exponential decay function.  "diminishing returns")
        \item An 8-way set associative cache is as good as fully-associative.  After that, the limit is capacity miss.
    \end{itemize}
    \item We can also increase cache size.  But this leads to slower hit time.  Making cache larger also has diminishing returns!
    \item Prefetching:  Idea: if we can guess the access pattern we can bring data before it is needed!
\end{itemize}

\subsection*{Prefetching - Four Questions!}
\begin{itemize}
    \item \textbf{What} addresses to prefetch (i.e., address prediction algorithm)
    \item \textbf{When} to initiate a prefetch request (early, late, on time)
    \item \textbf{Where} to place the prefetched data (different layers of caches, separate buffer)
    \item \textbf{How} does the prefetcher operate and who operates it (software, hardware, hybrid)
\end{itemize}
Prefetchers look at the history of addresses accessed to predict the next address access.  Similar to how a branch predictor looks at the history of branches, the prefetcher looks at the history of addresses.\\
\begin{itemize}
    \item This reduces compulsory misses and therefore miss rate
    \item However, this leads to cache pollution
    \begin{itemize}
        \item Need to monitor prefetching accuracy to change its \textit{aggressiveness}
        \item Other than this, no other negative impacts!  No correctness issues!
    \end{itemize}
\end{itemize}

\subsection*{Software vs. Hardware Prefetch}
\begin{itemize}
    \item Software prefetching
        \begin{itemize}
            \item ISA provides prefetch instructions
            \item Programmer or compiler inserts prefetch instructions (effort)
            \item Usually works well only for "regular access patterns"
        \end{itemize}
    \item Hardware prefetching
    \begin{itemize}
        \item Hardware monitors processor accesses
        \item Memorizes or finds patterns/strides
        \item Generates prefetch addresses automatically
    \end{itemize}
\end{itemize}

\subsubsection*{Example: Hardware Prefetcher}
Next line prefetcher:
\begin{itemize}
    \item Always prefetch next N cache lines after a demand access
    \item Pros:
    \begin{itemize}
        \item Simple to implement
        \item No need for sophisticated pattern detection
        \item Works well for sequential/streaming access patterns (instrctions?)
    \end{itemize}
    \item Cons:
    \begin{itemize}
        \item Can waste bandwidth with irregular patterns
        \item Low prefetch accuracy if access stride = 2 or when the program is traversing memory from higher to lower addresses.
    \end{itemize}
    \item Better options?  Stride prefetcher, stream buffers, etc.
\end{itemize}

\subsubsection*{Victim Cache}
\begin{itemize}
    \item Idea: for heavily conflicting addresses, a few "extra" temporary sets could remove conflicts!
    \begin{itemize}
        \item Use a very small buffer (called victim cache) to save the recently discarded blocks.  Search through them as well.
        \item Reduce conflict misses
        \begin{itemize}
            \item Research shows a 4-entry victim cache can remote up to 90\% of conflicts.
        \end{itemize}
        \item Extra overhead
        \item More complex design.
    \end{itemize}
\end{itemize}

\subsubsection*{Compiler and Software}
\begin{itemize}
    \item Reorder accesses/arrays to increase locality.
    \item Combine loops with similar behavior
    \item Use "tiling" to access arrays region by region instead of whole
    \begin{itemize}
        \item If column-major, \texttt{x[i+1, j]} follows \texttt{x[i, j]} in memory.
        \item Meanwhile, \texttt{x[i, j + 1]} is far away from \texttt{x[i, j]}.
        \item Poor code:
\begin{verbatim}
    for i = 1:
        for j = 1:
            sum = sum + x[i, j]
\end{verbatim}
        \item Better code:
\begin{verbatim}
    for j = 1:
        for i = 1:
            sum = sum + x[i, j]
\end{verbatim}
    \end{itemize}
    \item Use compiler profiling to improve prefetching
\end{itemize}

\subsection*{Reducing Miss Rate}
\begin{itemize}
    \item Replacement policy
    \begin{itemize}
        \item LRU vs. PLRU vs. Random
        \item Storage vs. Accuracy tradeoff!
    \end{itemize}
\end{itemize}

\subsection*{Reducing Miss Penalty}
\begin{itemize}
    \item Write buffer: use a load store queue
    \item Pros:
    \begin{itemize}
        \item No wait for stores needed.
        \item Lower miss penalty for loads.
    \end{itemize}
    \item Cons:
    \begin{itemize}
        \item More overhead.
    \end{itemize}
\end{itemize}

\subsubsection*{What happens on a store?}
\begin{enumerate}
    \item Data exists in the cache?
    \begin{itemize}
        \item Should we update memory AND cache on every write?
        \begin{itemize}
            \item Write through strategy
        \end{itemize}
        \item Update the memory only when the line is evicted.
        \begin{itemize}
            \item Write back strategy
        \end{itemize}
        \item Tradeoff: Less writes vs. Storage overhead vs. Memory status.
    \end{itemize}
    \item Data does not exist.
    \begin{itemize}
        \item Should we bring it to the cache?  -write allocate
        \item We probably don't need it anymore, so don't bring it.  -write no allocate
    \end{itemize}
\end{enumerate}
\begin{itemize}
    \item \textit{Write back} often compined with \textit{write-allocate.}
    \item \textit{Write-through} often combined with \textit{write-no allocate.}
    \item How to pick?
    \begin{itemize}
        \item It depends!
        \begin{itemize}
            \item Could be different for each level!
            \item Can be optimized using simulation and architectural search!
        \end{itemize}
    \end{itemize}
\end{itemize}

\subsection*{Reducing Miss Penalty via Early Restart}
\begin{itemize}
    \item Instead of waiting for all bytes (in a block) to arrive, forward data to CPU as soon as the requested byte(s) arrives.
    \begin{center}
        \includegraphics*[scale=1]{W7_4.png}
    \end{center}
\end{itemize}
Early Restart \textit{with critical word first}:
\begin{itemize}
    \item Instead of waiting for all bytes (in a block) to arrive, forward data to CPU as soon as the requested  byte(s) arrives.
    \item To further optimize this, first read the requested byte!
\end{itemize}

\subsection*{Multi-level Cache}
\begin{itemize}
    \item We can also reduce miss penalty by adding more levels of cache:
    \begin{itemize} 
        \item Adding L2, L3, etc.
    \end{itemize}
    \item Higher level cache means bigger and slower.  However, it is still both smaller and faster than the main memory.
    \begin{center}
        \includegraphics*[scale=0.5]{W7_5.png}
    \end{center}
\end{itemize}

\subsection*{Cache Performance}
\begin{itemize}
    \item Average time to access the cache:
    \[AAT = HitTime + MissRate \times MissPenalty\]
    \item $HitTime$: Time it takes to access the (L1) cache.
    \item $MissRate$: The average frequency of misses (in L1).
    \item $MissPenalty$: The time required to access the main memory.
    \item What if there are multiple levels?
    \begin{itemize}
        \item The miss penalty is the average time required to grab the data from either the other caches or main memory.
    \end{itemize}
\end{itemize}

\subsection*{Inclusive vs. Exclusive Cache}
\begin{itemize}
    \item Inclusive: $L_i$ is a subset of $L_{i + 1}$
    \begin{itemize}
        \item (pro) Easier to find data
        \item (con) Wasted capacity
    \end{itemize}
    \item Exclusive: Data is \textbf{only} in one of the levels.
    \begin{itemize}
        \item (con) Difficult to find data
        \item (pro) Efficient capacity
    \end{itemize}
\end{itemize}

\subsubsection*{Modern Designs}
\begin{itemize}
    \item Split vs Unified "Caches"
    \begin{itemize}
        \item L1 I/D caches commonly split and asymmetrical
        \begin{itemize}
            \item Double bandwidth and no-cross pollution on disjoint I and D footprints
            \item i-cache is smaller, simpler with more spatial locality.  Usually a prefetcher and/or trace cache is connected to i-cache.
        \end{itemize}
        \item L2 and L3 are unified for simplicity
    \end{itemize}
    \item "Havard" design referred to a microarchitecture with \textbf{separate} instruction and data memory.
    \item "Princeton" design referred to von Neumann's \textbf{unified} instruction and data memory.  This is the most common design.
\end{itemize}

\subsection*{Sub-Blocking}
\begin{itemize}
    \item Higher block size improves miss rate but also increases miss penalty!
    \item Idea: keep a large block size, but divide it into smaller "subblocks".  Bring only a subset of subblocks on a miss.
    \begin{itemize}
        \item (pro) lower miss rate
        \item (pro) lower miss penalty
        \item (con) need separate storage for valid bits for each subblock
        \item (con) more complex circuitry.
    \end{itemize}
\end{itemize}


\subsection*{Reducing Hit Time via reducing associativity and size}
\begin{itemize}
    \item DM < FA (direct mapping < fully associative)
    \begin{itemize}
        \item Use SA to balance between the two
    \end{itemize}
    \item Use smaller cache in lower levels (L1, L2, \dots)
\end{itemize}

\subsection*{Reducing Hit Time via Parallel lookup}
\begin{itemize}
    \item Access tag and data in parallel.
    \item Access each way in parallel.
\end{itemize}

\subsection*{Reducing Hit Time via Speculative load}
\begin{itemize}
    \item Instead of waiting for a store (potentially conflicting), issue the load speculatively.
    \begin{itemize}
        \item Once store is resolved, check whether there was a conflict or not.  Recover if there was.
    \end{itemize}
\end{itemize}
\section*{Cache Summary}
\begin{itemize}
    \item Miss Rate
    \begin{itemize}
        \item Increase block size
        \item Increase associativity
        \item Increase cache size
        \item Prefetching
        \item Victim cache
        \item Compiler
        \item Replacement Policy
    \end{itemize}
    \item Miss Penalty
    \begin{itemize}
        \item Write buffer
        \item Early restart with critical block first
        \item Adding more levels
        \item Sub-blocking
    \end{itemize}
    \item Hit Time
    \begin{itemize}
        \item Set associative cache
        \item Add more levels
        \item Parallel lookup
        \item Speculative loads
    \end{itemize}
\end{itemize}

\section*{Main Memory}
\subsection*{Current Technology}
\begin{itemize}
    \item We see combination of DRAM and DISK as the main memory (usually call teh DRAM part Main Memory and the HardDrive part DISK).
    \item Access to the Main Memory (combination of disk and DRAM) is \textit{always a hit}.
    \begin{itemize}
        \item As we will describe alter, data could be in disk and/or DRAM, and we need to handle that.
        \item Huge difference between latency of DRAM to DISK (and of course to cache).
    \end{itemize}
\end{itemize}
\subsection*{Where to store different programs?}
\begin{center}
    \includegraphics*[scale=0.8]{W7_6.png}
\end{center}
The fundamental requirement is \textbf{Isolation}.
\begin{itemize}
    \item Each program typically has \textit{different} regions:
    \begin{itemize}
        \item code
        \item data
        \item heap
        \item stack
        \item \dots
    \end{itemize}
    \item Portion of the memory code can be \textbf{shared} among multiple programs
    \item Portion of the memory can be \textbf{reserved} for the OS and/or other privileged activities.
\end{itemize}

\subsection*{Memory Management}
\begin{itemize}
    \item Early (and simple) machines only run one program with unrestricted access to ALL memory.
    \item Modern systems run several programs, and memory should be shared between multiple programs.
    \item Memory management is controlled by OS or system software, or hardware.
\end{itemize}

\subsection*{How to share the memory between multiple programs?}
\textbf{Option 1:} Partition the memory (statically)!
\begin{itemize}
    \item \textbf{Protection:} Independent program should not be able to affect each other inadvertently.  (check bounds!)
    \item \textbf{Location-Independence:} Program might be moved anywhere in the memory.  (use a base pointer!)
    \item To do these, we use a \textit{virtual} address, which is the base pointer plus whatever pointer in the program.
    \begin{itemize}
        \item When switching programs, OS updates base and bound registers.
        \item Example: $PA = Base\_Reg + Logical\_Adr$
    \end{itemize}
\end{itemize}
\begin{center}
    \includegraphics*[scale=0.5]{W7_7.png}
    \includegraphics*[scale=0.6]{W7_8.png}
\end{center}

\hrulefill
\subsubsection*{How to share code/libraries?}
\begin{itemize}
    \item Multiple programs can access (read-only) shared memory spaces.
    \begin{itemize}
        \item Standard libraries (e..g, printf)
        \item Drivers
        \item \dots
    \end{itemize}
    \item To do this, the library will be stored in its own block of main memory, and each program has a pointer to it!
    \item What about the OS?  (e.g., system commands)  The OS is a program as well!  Therefore, it can be shared.
\end{itemize}
\hrulefill
~\\
What happens if we want to add a program or change the space the program takes up?\\\\
\textbf{Option 2:} Partition the memory (dynamically)!
\begin{itemize}
    \item Divide the memory into fixed-size blocks (called page).
    \item Assign pages to each program as needed.
    \item Pages can be \textit{scattered} in the memory
    \item Typically 4KB per page.
\end{itemize}
\begin{center}
    \includegraphics*[scale=0.8]{W7_9.png}
\end{center}
\begin{itemize}
    \item Now we have another problem: each page can be \textit{anywhere}!
    \item To solve this, we use a \textit{page table} which tracks mapping!
\end{itemize}

\subsection*{Virtual Addressing}
\begin{itemize}
    \item Since we are using page tables for translations, we no longer need to use real physical memory addresses in each program.
    \begin{itemize}
        \item Each program can start from (virtual) address 0x00.
        \item Each program sees a \textit{large, private}, and \textit{uniform} memory.  (this is just an illusion though!)
    \end{itemize}
    \begin{center}
        \includegraphics*[scale=0.6]{W7_10.png}\\
    \end{center}
    \begin{itemize}
        \item Each program sees a large, private, and unified memory.
    \end{itemize}
    \begin{center}
        \includegraphics*[scale=0.8]{W7_11.png}
    \end{center}
\end{itemize}

\subsubsection*{Virtual to Physical Address Translation}
\begin{itemize}
    \item $2^n$: virtual address size
    \item $2^m$: physical address size
    \item $2^p$: page size
\end{itemize}
\begin{center}
    \includegraphics*[scale=0.7]{W7_12.png}
\end{center}
\subsubsection*{Virtual and Physical Memory}
\begin{itemize}
    \item System:
    \begin{itemize}
        \item Virtual memory size: 4GB = $2^{32}$ bytes
        \item Physical memory size: 256MB = $2^{28}$ bytes
        \item Page size: 4KB = $2^{12}$ bytes
    \end{itemize}
    \item Organization:
    \begin{itemize}
        \item Virtual address: $32$ bits
        \item Physical address: $28$ bits
        \item Page offset: $12$ bits
        \item \# Virtual pages = $2^{32} / 2^{12} = 2^{20}$ (VPN = $20$ bits)
        \item \# Physical pages = $2^{28} / 2^{12} = 2^{16}$ (PPN = $16$ bits)
    \end{itemize}
\end{itemize}
\subsubsection*{Page Table}
\begin{center}
    \includegraphics*[scale=0.8]{W7_13.png}
\end{center}
\begin{itemize}
    \item Note that the Virtual Page Number (VPN) is the index to the table.
\end{itemize}
\subsubsection*{What else to store in each page table entry (PTE)?}
\begin{itemize}
    \item Protections and flags
    \begin{itemize}
        \item Whether we can write to this page or not (read-only)?
        \item Who is the owner?  Is it shared?
        \item Can we execute it?
    \end{itemize}
    \item Page tables must also be stored in the memory.
    \item Each memory access becomes two accesses, one for accessing the page table (i.e., adr + VPN + base).  The other for accessing the translated address (i.e., adr = PPN + page offset).
\end{itemize}
\subsubsection*{How large is a page table?}
\begin{itemize}
    \item \# Virtual pages = $2^{32} / 2^{12} = 2^{20}$  (VPN = $20$ bits)
    \item Physical address: 28
    \item Each entry in page table (PTE) = 28 - offset + flags $\approx 32$ bites = 4 bytes.
    \item $\rightarrow 2^{30} \times 32 = 4\text{MB}$ for each application!
    \item Number of bits double in a 64-bit system!
\end{itemize}

\subsection*{Hierarchical Page Table}
\begin{itemize}
    \item Store the page table as a page table
    \begin{center}
        \includegraphics*[scale=0.7]{W7_14.png}
    \end{center}
    \item Now we divide one translation into multiple.  
    \item The process of going from one level of page table to the next is called "walking".
    \item The problem is that now the page table is huge, and memory is limited.
    \item Solution: Remove some second level pages!  Empty page tables can be immediately removed, and reallocated in memory if it is needed.
    \item Hierarchical page tables can leverage the existing sparsity of virtual addressses and make the page table storage compact!
    \item New problem: Each memory access (e.g., L1 cache) needs multiple memory accesses!
    \begin{itemize}
        \item To address this, \textbf{cache} the most recent translations!
    \end{itemize}
\end{itemize}

\subsection*{Translation Lookaside Buffer (TLB)}
\begin{center}
    \includegraphics*[scale=0.9]{W7_15.png}
\end{center}
\begin{itemize}
    \item The hit time is $t_{TLB} + t_{L1}$.
    \item Cache translations in TLB
    \begin{itemize}
        \item TLB hit $\Rightarrow$ Single-cycle translation
        \item TLB miss $\Rightarrow$ Page-Table Walk to refill
    \end{itemize}
    \begin{center}
        \includegraphics*[scale=0.8]{W7_16.png}
    \end{center}
\end{itemize}

\subsubsection*{TLB Design}
\begin{itemize}
    \item Typically 32-128 entries, usually fully associative
    \begin{itemize}
        \item Each entry maps a large page, hence less spatial locality across pages.
        \item Larger TLBs (256-512 entries) are 4 or 8 way set-associative.
        \item Larger systems sometimes have multi-level (L1 and L2) TLBs.
        \item Random or FIFO replacement policy
        \item \textbf{TLB Reach:}
        \begin{itemize}
            \item Size of largest virtual address space that can be simultaneously mapped by TLB
            \item Example: 64 TLB entries, 4KB pages, one page per entry.  TLB Reach = $256$ KB
        \end{itemize}
        \item TLB miss causes an exception and results in a page table walk.
        \item OS typically is responsible to handle TLB miss (software handling).
        \item Alternatively, memory management unit (MMU) can handle TLB miss.
    \end{itemize}
\end{itemize}

\subsection*{How to utilize disk?}
\begin{itemize}
    \item Disk is our secondary storage unit with much bigger size, but much larger access time.
    \item How do we use this efficiently?
\end{itemize}

\subsubsection*{Demand Paging}
\begin{itemize}
    \item Use main memory and "swap" pages in the disk as automatically managed memory hierarchy levels.
    \item $\rightarrow$ Analogous to cache vs. main memory
    \item $M$ (DRAM + Disk capacity) bytes of storage.
    \begin{itemize}
        \item keep most frequently used $C$ bytes in DRAM ($C << M$)
        \item keep the rest in disk.
        \item If the page is not in DRAM, we call it a \textit{page fault}.
        \item Bring a page (from disk to main memory) when "demanded".
        \item In the page table, if the valid bit is 0, then the page is not in memory
        \begin{itemize}
            \item We call this a \textbf{page fault}.
        \end{itemize}
    \end{itemize}
\end{itemize}
\subsubsection*{Demand Paging Design}
\begin{itemize}
    \item Same basic issues as before (in cache)
    \begin{itemize}
        \item When to bring a page into DRAM?
        \item Which page to evict (we call it "swap") from DRAM to disk to free-up DRAM for new pages?
        \item Page size?
        \item \dots
    \end{itemize}
    \item OS handles everything (easier, fast enough)
    \begin{itemize}
        \item Pseudo-LRU replacement policy
    \end{itemize}
\end{itemize}

\subsection*{Memory Hierarchy Summary}
\begin{center}
    \includegraphics*[scale=0.8]{W7_17.png}
\end{center}
\begin{itemize}
    \item Page tables
    \begin{itemize}
        \item Why we need them and how to translate?
        \item How to store them?
        \item How to reduce access time?
    \end{itemize}
\end{itemize}
\subsection*{How to decrease hit time?}
\begin{itemize}
    \item Can we access L1 before TLB?
    \begin{itemize}
        \item Address in L1 should be stored with VA (i.e., no translation!)
        \item Problem?  Aliasing.  Therefore, L1 \textbf{cannot} cannot be accessed \textit{before} TLB.
        \begin{itemize}
            \item However, we can access them \textit{in parallel}.
        \end{itemize}
    \end{itemize}
\end{itemize}
\subsubsection*{Virtually-Indexed Physically-Tagged Cache}
\begin{center}
    \includegraphics*[scale=0.8]{W7_18.png}
\end{center}    
\subsubsection*{With and Without MMU}
\begin{itemize}
    \item Most embedded processors and DSPs provide \textit{physical addressing} only!
    \begin{itemize}
        \item Can't afford area/speed/power budget for virtual memory support
        \item Often there is no secondary storage to swap to!
    \end{itemize}
\end{itemize}

\end{document}
