% document formatting
\documentclass[10pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[left=1in,right=1in,top=1in,bottom=1in]{geometry}
\usepackage[T1]{fontenc}
\usepackage{xcolor}

% math symbols, etc.
\usepackage{amsmath, amsfonts, amssymb, amsthm}

% lists
\usepackage{enumerate}

% images
\usepackage{graphicx} % for images

% code blocks
\usepackage{minted, listings} 
 
% verbatim greek
\usepackage{alphabeta}

\graphicspath{{./assets/images}}

\newcommand{\solution}{\textbf{Solution:}} 

\title{COM SCI 132 Week 1}

\author{Aidan Jan}
\date{\today}

\begin{document}
\maketitle
\section*{Compilers vs Interpreters}
\begin{itemize}
    \item Compilers compile all the code into an executable file, and later, the OS can run the program straight through
    \item Interpreters read and execute code at the same time.
\end{itemize}
\section*{Compilers}
\subsection*{Qualities}
What are some important qualities of a compiler?
\begin{enumerate}
    \item Correct code
    \item Output runs fast
    \item Compiler runs fast
    \item Compile time proportional to program size
    \item Support for separate compilation
    \item Good diagnostics for syntax errors
    \item Works well with the debugger
    \item Good diagnostics for flow anomalies
    \item Cross language calls
    \item Consistent, predictable optimzation
\end{enumerate}
\subsection*{Abstract view}
\begin{itemize}
    \item Implications:
    \begin{itemize}
        \item recognize legal (and illegal) programs
        \item generate correct code
        \item manage storage of all variables and code
        \item agreement on format for object (or assembly) code
        \item Big step up from assembler to higher level notations
    \end{itemize}
    \begin{center}
        \includegraphics*[scale=1]{W1_1.png}
    \end{center}
\end{itemize}
\subsection*{Two-pass compilers}
\begin{itemize}
    \item Two pass compilers feature an *intermediate representation* (IR).
    \begin{center}
        \includegraphics*[scale=1]{W1_2.png}
    \end{center}
    \item Implications
    \begin{itemize}
        \item Intermediate representation (IR)
        \item Front end maps legal code into IR
        \item Back end maps IR onto target machine
        \item Simplify retargeting
        \item Allows multiple front ends
        \item Multiple passes $\Rightarrow$ better code
    \end{itemize}
\end{itemize}
\subsubsection*{Front end}
Responsibilities:
\begin{itemize}
    \item Recognize legal procedure
    \item Report errors
    \item Produce IR
    \item Preliminary storage map
    \item Shape the code for the back end
\end{itemize}
Much of front end construction can be automated.
\begin{center}
    \includegraphics*[scale=1]{W1_3.png}
\end{center}
\textbf{Scanner:}
\begin{itemize}
    \item Maps characters into \textit{tokens} - the basic unit of syntax
        \begin{verbatim} 
            x = x + y; 
        \end{verbatim} 
        becomes 
        \begin{verbatim}
            <id, x> = <id, x> + <id, y>;
        \end{verbatim}
    \item character string value for a \textit{token} is a \textit{lexeme}
    \item typical tokens: \textit{number}, \textit{id}, +, -, *, /, \texttt{do}, \texttt{end}
    \item eliminates white space (tabs, blanks, comments)
    \item a key issue is speed $\Rightarrow$ use specialized recognizer (as opposed to \texttt{lex})
\end{itemize}
\textbf{Parser:}
\begin{itemize}
    \item recognize context-free syntax
    \item guide context-sensitive analysis
    \item construct IR(s)
    \item produce meaningful error messages
    \item attempt error correction
\end{itemize}
Parser generators mechanize much of the work.
\subsubsection*{Back end}
\begin{itemize}
    \item translate IR into target machine code
    \item choose instructions for each IR operation
    \item decide what to keep in registers at each point
    \item ensure conformance with system interfaces
\end{itemize}
Automation has been less successful here.
\begin{center}
    \includegraphics*[scale=1]{W1_4.png}
\end{center}
\textbf{Instruction Selection:}
\begin{itemize}
    \item produce compact, fast code
    \item use available addressing modes
    \item pattern matching problem
    \begin{itemize}
        \item \textit{ad hoc} techniques
        \item tree pattern matching
        \item string pattern matching
        \item dynamic programming
    \end{itemize}
\end{itemize}
\textbf{Register Allocation:}
\begin{itemize}
    \item have value in a register when used
    \item limited resources
    \item changes instruction choices
    \item can move loads and stores
    \item optimal allocation is difficult
\end{itemize}
Modern allocators often use an analogy to graph coloring
\subsection*{Optimizing compiler}
Code improvement:
\begin{itemize}
    \item analyzes and changes IR
    \item goal is to reduce runtime
    \item must preserve values
\end{itemize}
\begin{center}
    \includegraphics*[scale=1]{W1_5.png}
\end{center}
\subsubsection*{Optimizer (middle end)}
Modern optimizers are usually built as a set of passes.
Typical passes:
\begin{itemize}
    \item constant propagation and folding
    \item code motion
    \item reduction of operator strength
    \item common subexpression elimination
    \item redundant store elimination
    \item dead code elimination
\end{itemize}
\section*{Compiler Example and Phases}
\begin{center}
    \includegraphics*[scale=0.8]{W1_7.png}
\end{center}
\begin{tabular}{| p{3.5cm} | p{12cm} |}
    \hline
    Lex & Break source file into individual words, or tokens \\
    \hline
    Parse & Analyse the phrase structure of program \\
    \hline
    Parsing Actions & Build a piece of \textit{abstract syntax tree} for each phrase \\
    \hline
    Semantic Analysis & Determine what each phrase means, relate uses of variables to their definitions, check types of expressions, request translation of each phrase\\
    \hline
    Frame Layout & Place variables, function parameters, etc., into activation records (stack frames) in a machine-dependent way\\
    \hline
    Translate & Produce \textit{intermediate representation trees} (IR trees), a notation that is not tied to any particular source language or target machine\\
    \hline
    Canonicalize & Hoist side effects out of expressions, and clean up conditional branches, for convenience of later phases\\
    \hline
    Instruction Selection & Group IR-tree nodes into clumps that correspond to actions of target-machine instructions\\
    \hline
    Control Flow Analysis & Analyse sequence of instructions into \textit{control flow graph} showing all possible flows of control program might follow when it runs\\
    \hline
    Data Flow Analysis & Gather information about flow of data through variables of program; e.g., \textit{liveness analysis} calculates places where each variable holds a still-needed (\textit{live}) value\\
    \hline
    Register Allocation & Choose registers for variables and temporary values; variables not simultaneously live can share same register\\
    \hline
    Code Emission & Replace temporary names in each machine instruction with registers\\
    \hline
\end{tabular}
\section*{Lexical Analysis}
\subsection*{Patterns for the Scanner}
Recall the scanner in the front end compiler.\\
\textit{A scanner must recognize various parts of the language's syntax}
For example, the easy parts:
\textit{white space}:
\begin{verbatim}
    <ws> ::= <ws> ' '
          |  <ws> '\t'
          |  ' '
          |  '\t'
\end{verbatim}
\textit{keywords and operators}
\begin{verbatim}
    do, end, etc.
\end{verbatim}
\textit{comments}:
\begin{verbatim}
    //, /* ... */
\end{verbatim}
Other parts are much harder:\\
\textit{identifiers}
\begin{itemize}
    \item alphabetic followed by $k$ alphanumerics (\_, \$, \&, \dots)
\end{itemize}
\textit{numbers}
\begin{itemize}
    \item integers: 0 or digit from 1-9 followed by digits from 0-9
    \item decimals: integer '.' digits from 0-9
    \item reals: (integer or decimal) 'E' (+ or -) digits from 0-9
    \item complex: '(' real ',' real ')'
\end{itemize}
These can be done easily with REGEX.

\section*{Recognizers}
From a regular expression we can construct a \textit{deterministic finite automaton} (DFA).
\begin{center}
    \includegraphics*[scale=1]{W1_8.png}
\end{center}
\begin{itemize}
    \item letter $\rightarrow (a \vert b \vert c \vert \dots \vert z \vert A \vert B \vert \dots \vert Z)$
    \item digit $\rightarrow (0 \vert 1 \vert 2 \vert \dots \vert 9)$
    \item id $\rightarrow letter ( letter \vert digit)^*$
\end{itemize}

\subsection*{Tables for the Recognizer}
\begin{center}
    \includegraphics*[scale=1]{W1_9.png}
\end{center}
To change languages, we can just change tables.

\subsection*{Automatic Construction}
Scanner generators automatically construct code from regular-expression-like descriptions
\begin{itemize}
    \item construct a \textit{dfa}
    \item use state minimization techniques
    \item emit code for the scanner (table driven or direct code)
\end{itemize}
A key issue in automation is an interface to the parser\\
For example, \texttt{lex} is a scanner generator supplied with UNIX.
\begin{itemize}
    \item emits C code for scanner
    \item provides macro definitions for each token (used in the parser)
\end{itemize}

\section*{LL Parsing}
Recall, the role of the Parser, the second part of the front-end compiler.

\subsection*{Syntax Analysis}
Context-free syntax is specified with a context-free grammar.\\
Formally, a CFG \textit{G} is a 4-tuple $(V_t, V_n, S, P)$, where
\begin{itemize}
    \item $V_t$ is the set of \textit{terminal} symbols in the grammar.  For our purposes, $V_t$ is the set of tokens returned by the scanner.
    \item $V_n$, the \textit{nonterminals}, is the set of syntactic variables that denote sets of (sub)strings occurring in the language.  These are used to impose a structure on the grammar.
    \item $S$ is the distinguished nonterminal ($S \in V_n$) denoting the entire set of strings in $L(G)$.  This is sometimes called a \textit{goal symbol}.
    \item $P$ is a finite set of \textit{productions} specifying how terminals and non-terminals can be combined to form strings in the language.  Each production must have a single non-terminal on its left hand side.
\end{itemize}
The set $V = V_t \cup V_n$ is called the \textit{vocabulary} of $G$.\\\\
Grammars are often written in Backus-Naur form (BNF).  For example:
\begin{verbatim}
    <goal> ::= <expr>
    <expr> ::= <expr><op><expr>
            |  num
            |  id
    <op>   ::= +
            |  -
            |  *
            |  /
\end{verbatim}
This describes simple expressions over numbers and identifiers.  In a BNF for a grammar, we represent
\begin{enumerate}
    \item non-temrinals with angle brackets or capital letters
    \item terminals with \texttt{typewriter} font or \underline{underline}
    \item productions as in the example
\end{enumerate}

\subsection*{Scanning vs. Parsing - where do we draw the line?}
\begin{verbatim}
    term ::= [a-zA-Z]([a-zA-Z=]|[0-9])*
          |  0|[1-9][0-9]*
    op   ::= +|-|*|/
    expr ::= (term op)*term
\end{verbatim}  
Regular expressions are used to classify:
\begin{itemize}
    \item identifiers, numbers, keywords
    \item REs are more concise and simpler for tokens than a grammar
    \item more efficient scanners can be built from REs (DFAs) than grammars
\end{itemize}
Context-free grammars are used to count:
\begin{itemize}
    \item brackets: \texttt{(), begin...end, if...then...else}
    \item imparting structure: expressions
\end{itemize}
Syntactic analysis is complicated enough: grammar for C has around 200 productions.  Factoring out lexical analysis as a separate phase makes the compiler more manageable.

\section*{Derivations}
We can view the productions of a CFG as rewriting rules.  Using our exmple CFG:
\begin{verbatim}
    <goal> => <expr>
           => <expr><op><expr>
           => <expr><op><expr><op><expr>
           => <id,x><op><expr><op><expr>
           => <id,x> + <expr><op><expr>
           => <id,x> + <num,2><op><expr>
           => <id,x> + <num,2> * <expr>
           => <id,x> + <num,2> * <id,y>
\end{verbatim}
\begin{itemize}
    \item We have derived the sentence \texttt{x + 2 * y}.\
    \item We denote this \texttt{<goal>=>* id + num * id}.
    \item Such a sequence of rewrites is a \textit{derivation} or a \textit{parse}.
    \item The process of discovering a derivation is called \textit{parsing}.
    \item At each step, we chose a non-terminal to replace.
    \item This choice can lead to different derivations.
    \item Two are of particular interest:
    \begin{itemize}
        \item leftmost derivation: the leftmost non-terminal is replaced at each step
        \item rightmost derivation: the rightmost non-terminal is replaced at each step
    \end{itemize}
    \item \textit{The previous example was a leftmost derivation.}
\end{itemize}

\subsection*{Precedence}
\begin{center}
    \includegraphics*[scale=1]{W1_10.png}
\end{center}
A tree can be generated using the CFG.  However, for the tree shown here, the \textit{treewalk evaluation} computes \texttt{(x + 2) * y} - the wrong answer!\\
The two derivations point out a problem with the grammar: it has no notion of precedence, or \textit{implied order} or evaluation.  To add precedence takes additional machinery:
\begin{verbatim}
    <goal>   ::= <expr>
    <expr>   ::= <expr> + <term>
              |  <expr> - <term>
              |  <term>
    <term>   ::= <term> * <factor>
              |  <term> / <factor>
              |  <factor>
    <factor> ::= num
              |  id
\end{verbatim} 
This grammar enforces a precedence on the derivation:
\begin{itemize}
    \item terms \textit{must} be derived from expressions
    \item forces the "correct" tree
\end{itemize}

\subsection*{Ambiguity}
If a grammar has more than one derivation for a single sentential form, then it is \textbf{ambiguous}.\\
Example:
\begin{verbatim}
    <stmt> ::= if <expr> then <stmt>
            |  if <expr> then <stmt> else <stmt>
            |  other stmts
\end{verbatim}
Consider deriving the sentential form:
\[\texttt{if } E_1 \texttt{ then if } E_2 \texttt{ then } S_1 \texttt{ else } S_2 \]
This has two derivations, which leads to ambiguity that is purely grammatical.\\
It is a \textit{context-free} ambiguity.\\

\subsubsection*{Eliminating Ambiguity} 
It may be possible to eliminate ambiguities by rearranging the grammar:
\begin{verbatim}
    <stmt>      ::= <matched>
                 |  <unmatched>
    <matched>   ::= if <expr> then <matched> else <matched>
                 |  other stmts
    <unmatched> ::= if <expr> then <stmt>
                 |  if <expr> then <matched> else <unmatched>
\end{verbatim}
This generates the same language as the ambiguous grammar, but applies the common sense rule:
\[\text{match each \texttt{else} with the closest unmatched \texttt{then}}\]
This is most likely the language designer's intent.

\subsubsection*{Overloading}
Ambiguity is often due to confusion in the context-free specification.  Context-sensitive confusions can arise from \textit{overloading}.
For example:
\begin{verbatim}
    a = f(17)
\end{verbatim}
In many Algol-like languages, \texttt{f} could be a function or subscripted variable.  Disambiguating this statement requires context:
\begin{itemize}
    \item need \textit{values} of declarations
    \item not \textit{context-free}
    \item really an issue of \textit{type}
\end{itemize}   
Rather than complicate parsing, we will handle this separately.

\subsection*{Parsing: the big picture}
\begin{center}
    \includegraphics*[scale=1]{W1_11.png}
\end{center}
Our goal is a flexible parser generator system.

\section*{Top-Down vs. Bottom-Up Parsing}
Top-down parsers:
\begin{itemize}
    \item start at the root of derivation tree and fill in
    \item picks a production and tries to match the input
    \item may require backtracking
    \item some grammars are backtrack-free (predictive)
\end{itemize}
Bottom-up parsers:
\begin{itemize}
    \item start at the leaves and fill in
    \item start in a state valid for legal first tokens
    \item as input is consumed, change state to encode possibilities (recognize valid prefixes)
    \item use a stack to store both state and sentential forms
\end{itemize}
\subsection*{Top-down Parsing}
A top-down parser starts with the root of the parse tree, labelled with the start or goal symbol of the grammar.  To build a parse, it repeats the following steps until the fringe of the parse tree matches the input string
\begin{enumerate}
    \item At a node labelled $A$, select a production $A \rightarrow \alpha$ and construct the appropriate child for each symbol of $\alpha$
    \item When a terminal is added to the fringe that doesn't match the input string, backtrack.
    \item Find the next node to be expanded (must have a label in $V_n$)
\end{enumerate}
The key in selecting the right production in step 1 $\Rightarrow$ should be guided by input string\\
Example using $x - 2 * y$:
\begin{center}
    \includegraphics*[scale=1]{W1_12.png}
\end{center}
Another possible parse for $x - 2 * y$
\begin{center}
    \includegraphics*[scale=1]{W1_13.png}
\end{center}
If the parser makes the worng choices, expansion doesn't terminate.  This isn't a good property for a parser to have.  (Parsers should terminate!)
\section*{Left-recursion}
Top-down parsers cannot handle left-recursion in a grammar.  Formally, a grammar is \textit{left-recursive} if:
\[\exists A \in V_n \text{ such that } A \Rightarrow^+ A\alpha \text{ for some string } \alpha\]
Our simple expression grammar is left-recursive.
\subsection*{Eliminating left-recursion}
To remove left-recursion, we can transform the grammar.  Consider the grammar fragment:
\begin{verbatim}
    <foo> ::= <foo> α
           |  β
\end{verbatim}
where $\alpha$ and $\beta$ do not start with <foo>\\
We can rewrite this as:
\begin{verbatim}
    <foo> ::= β <bar>
    <bar> ::= α <bar>
           | ε
\end{verbatim}
where <bar> is a new non-terminal.  This fragment contains no left-recursion.\\
\textbf{Example:} Our expression grammar contaions two cases of left-recursion.
\begin{verbatim}
    <expr>   ::= <expr> + <term>
              |  <expr> - <term>
              |  <term>
    <term>   ::= <term> * <factor>
              |  <term> / <factor>
              |  <factor>
\end{verbatim}
Applying the transformation gives:
\begin{verbatim}
    <expr>   ::= <term><expr'>
    <expr'>  ::= + <term><expr'>
              |  - <term><expr'>
              |  ε
    <term>   ::= <factor><term'>
    <term'>  ::= * <factor><term'>
              |  / <factor><term'>
              |  ε
\end{verbatim}
With this grammar, a top-down parser will:
\begin{itemize}
    \item terminate
    \item backtrack on some inputs
\end{itemize}
Although it is longer and harder to read, it factors out left-recursion.

\subsection*{How much lookahead is needed?}
We saw that top-down parsers may need to backtrack when they select the wrong production.\\
Do we need arbitrary lookahead to parse CFGs?
\begin{itemize}
    \item in general, yes
    \item use the Earley or Cocke-Younger, Kasami algorithms
\end{itemize}
Fortunately,
\begin{itemize}
    \item large subclasses of CFGs can be parsed with limited lookahead
    \item most programming language constructs can be expressed in a grammar that falls in these subclasses
\end{itemize}
Among the interesting subclasses are:
\begin{itemize}
    \item LL(1): left to right scan, \textbf{left}-most derivation, 1-token lookahead
    \item LR(1): left to right scan, \textbf{right}-most derivation, 1-token lookahead
\end{itemize}

\subsection*{Predictive Parsing}
\textbf{Basic idea:} For any two productions $A \rightarrow \alpha \vert \beta$, we would like a distinct way of choosing the correct produciton to expand.\\
For some RHS $\alpha \in G$, define FIRST($\alpha$) as the set of tokens that appear first in some string derived form $\alpha$.\\
That is, for some $w \in V_t^*$, $w \in$ FIRST($\alpha$) if and only if $\alpha \Rightarrow^* w\gamma$.\\
\textbf{Key property:} Whenever two productions $A \rightarrow \alpha$ and $A \rightarrow \beta$ both appear in the grammar, we would like
\[\text{FIRST}(\alpha)\cap\text{FIRST}(\beta) = \phi\]
This would allow the parser to make a correct choice with a lookahead of only one symbol!  The example grammar has this property!\\\\
If a grammar does not have this property, we can transform it to have the property through a process called \textbf{left factoring}:
\begin{itemize}
    \item For each non-terminal $A$ find the longest prefix $\alpha$ common to two or more of its alternatives.
    \item if $\alpha \neq \epsilon$ then replace all of the $A$ productions $A \rightarrow \alpha \beta_1 \vert \alpha \beta_2 \vert \cdots \vert \alpha\beta_n$ with:
    \begin{align*}
        A &\rightarrow \alpha A'\\
        A' &\rightarrow \beta_1 \vert \beta_2 \vert \cdots \vert \beta_n
    \end{align*}
    \begin{itemize}
        \item where $A'$ is a new non-terminal
    \end{itemize}   
    \item Repeat until no two altrernatives for a single non-terminal have a common prefix.
\end{itemize}
\subsubsection*{Example:}
Consider a right-recursive version of the expression grammar:
\begin{verbatim}
    1|  <goal>   ::= <expr>
    2|  <expr>   ::= <term> + <expr>
    3|            |  <term> - <expr>
    4|            |  <term>
    5|  <term>   ::= <factor> * <term>
    6|            |  <factor> / <term>
    7|            |  <factor>
    8|  <factor> ::= num
    9|            |  id
\end{verbatim}
To choose between productions 2, 3, and 4, the parser must see past the \texttt{num} or \texttt{id} and look at the $+$, $-$, $*$, or $/$.
\[\text{FIRST}(2) \cap \text{FIRST}(3) \cap \text{FIRST}(4) \neq \Phi\]
This grammar fails the test.  There are two non-terminals that must be left factored: \texttt{<expr>} and \texttt{<term>}.
Applying left factoring gives:
\begin{verbatim}
    <expr>   ::= <term><expr'>
    <expr'>  ::= + <expr>
              |  - <expr>
              |  ε
    <term>   ::= <factor><term'>
    <term'>  ::= * <term>
              |  / <term>
              |  ε
\end{verbatim}
Substituting this back makes it so selection requires only a single token lookahead.

\section*{Generality}
Question:\\
\begin{itemize}
    \item By left factoring and eliminating left-recursion, can we transform an arbitrary context-free grammar to a form where it can be predictively parsed with a single token lookahead?
\end{itemize}
Answer:\\
\begin{itemize}
    \item Given a context-free grammar that doesn't meet our conditions, it is undecidable whether an equivalent grammar exists that does meet our conditions.
\end{itemize}
Many context-free languages do not have such a grammar:
\[\{a^n 0 b^n \vert n \geq 1\} \bigcup \{a^n 1 b^{2n} \vert n \geq 1\}\]
Must look past an arbitrary number of a's to discover the 0 or the 1 and so determine the derivation.

\subsection*{Recursive Descent Parsing}
Now, we can produce a simple recursive descent parser from the (right-associative) grammar.
\begin{minted}{java}
    Token token;
    void eat(char a) {
        if (token == a) {token = next.token(); }
        else { error(); }
    }
    void goal() { token = next.token(); expr(); eat(EOF); }
    void expr() { term(); expr_prime(); }
    void expr_prime() {
        if (token == PLUS) { eat(PLUS); expr(); }
        else if (token == MINUS) { eat(MINUS); expr(); }
        else { }
    }
    void term() { factor(); term_prime(); }
    void term prime() {
        if (token = MULT) { eat(MULT); term(); }
        else if (token = DIV) { eat(DIV); term(); }
        else { }
    }
    void factor() {
        if (token = NUM) { eat(NUM); }
        else if (token = ID) { eat(ID); }
        else error();
    }
\end{minted}

\subsection*{NULLABLE}
For a string $\alpha$ with grammar symbols, define NULLABLE($\alpha$) as $\alpha$ can go to $\epsilon$.
\[\text{NULLABLE}(\alpha) \text{ if and only if } (\alpha \Rightarrow^* \epsilon)\]
How to compute NULLABLE($U$), for $U \in V_t \cup V_n$:
\begin{enumerate}
    \item For each $U$, let NULLABLE($U$) be a Boolean variable
    \item Derive the following constraints:
    \begin{enumerate}
        \item if $a \in V_t$, then NULLABLE($a$) = false.
        \item if $A \rightarrow Y_1 \dots Y_k$ is a production, then [NULLABLE($Y_1$) $\land \dots \land$ NULLABLE($Y_k$)] $\Rightarrow$ NULLABLE($A$)
    \end{enumerate}
    \item Solve the constraints.
    \[\text{NULLABLE}(X_1 \dots X_k) = \text{NULLABLE}(X_1) \land \dots \land \text{NULLABLE}(X_k)\]
\end{enumerate}

\subsection*{FIRST}
For a string $\alpha$ with grammar symbols, define FIRST($\alpha$) as the set of terminal symbols that begin strings derived from $\alpha$.
\[\text{FIRST}(\alpha) = \{a \in V_t \vert \alpha \Rightarrow ^* a\beta\}\]
How to compute FIRST($U$), for $U \in V_t \cup V_n$:
\begin{enumerate}
    \item For each $U$, let FIRST($U$) be a set variable.
    \item Derive the following constraints:
    \begin{enumerate}
        \item if $a \in V_t$, then FIRST($a$) = { $a$ }
        \item if $A \rightarrow Y_1Y_2\cdots Y_k$ is a production, then 
            \begin{itemize}
                \item FIRST($Y_1$) $\subseteq$ FIRST($A$)
                \item $\forall i \::\: 1 < i \leq k$, if NULLABLE($Y_1\cdots Y_{i - 1}$), then FIRST($Y_i$) $\subseteq$ FIRST($A$)
            \end{itemize}
    \end{enumerate}
    \item Solve the constraints.  Go for the $\subseteq$-least solution.
    \[\text{FIRST}(X_1\cdots X_k) = \bigcup \:_{i:1\leq i \leq k \land \text{NULLABLE}(X_1\cdots X_{i - 1})}\text{FIRST}(X_i)\]
\end{enumerate}

\subsection*{FOLLOW}
For a non-terminal $B$, define FOLLOW($B$) as the set of terminals that can appear immediately to the right of $B$ in some sentential form.
\[\text{FOLLOW}(B) = \{a \in V_t \vert G \Rightarrow^* \alpha B \beta \land a \in \text{FIRST}(\beta \$)\}\]
How to compute FOLLOW($B$):
\begin{enumerate}
    \item For each non-terminal $B$, let FOLLOW($B$) be a set variable.
    \item Derive the following constraints:
    \begin{enumerate}
        \item if $G$ is the start symbol and \$ is the end-of-file marker, then $\{\$\} \subseteq$ FOLLOW($G$)
        \item if $A \rightarrow \alpha B \beta$ is a production, then:
        \begin{itemize}
            \item FIRST($\beta$) $\subseteq$ FOLLOW($B$)
            \item if NULLABLE($\beta$), then FOLLOW($A$) $\subseteq$ FOLLOW($B$)
        \end{itemize}
    \end{enumerate}
    \item Solve the constraints.  Go for the $\subseteq$-least solution.
\end{enumerate}

\subsection*{LL1 Grammars}
\textbf{Intuition:} A grammar $G$ is LL(1) if and only if:
\begin{itemize}
    \item for all non-terminals $A$, each distinct pair of productions ($A \rightarrow \beta$) and ($A \rightarrow \gamma$) satisfies the condition FIRST($\beta$)$\cap$FIRST($\gamma$) = $\emptyset$.
\end{itemize}
\textbf{Question:} What if NULLABLE($A$)?\\
\textbf{Definition:} A grammar $G$ is LL(1) if and only if for each set of productions $A \rightarrow \alpha_1 \vert \alpha_2 \vert \cdots \vert \alpha_n$:
\begin{enumerate}
    \item FIRST($\alpha_1$), FIRST($\alpha_2$), \dots, FIRST($\alpha_n$) are pairwise disjoint, and
    \item If NULLABLE($\alpha_i$), then for all $j$, such that $1 \leq j \leq n \land j \neq i$: FIRST($\alpha_j$)$\cap$FOLLOW($A$) = $\emptyset$
\end{enumerate}
If $G$ is $\epsilon$-free, condition 1 is sufficient.\\\\
Provable facts about LL(1) grammars:
\begin{enumerate}
    \item No left-recursive grammar is LL(1)
    \item No ambiguous grammar is LL(1)
    \item Some languages have no LL(1) grammar
    \item A $\epsilon$-free grammar where each alternative expansion for $A$ begins with a distinct terminal is a \textit{simple} LL(1) \textit{grammar}.
\end{enumerate}
\textbf{Example:}
\[S \rightarrow aS \vert a\]
is not LL(1) because FIRST($aS$) = FIRST($a$) = \{a\}
\begin{align*}
    S &\rightarrow aS'\\
    S' &\rightarrow aS' \vert \epsilon
\end{align*}
accepts the same language and is LL(1).


\end{document}