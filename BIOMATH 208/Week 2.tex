% document formatting
\documentclass[10pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[left=1in,right=1in,top=1in,bottom=1in]{geometry}
\usepackage[T1]{fontenc}
\usepackage{xcolor}

% math symbols, etc.
\usepackage{amsmath, amsfonts, amssymb, amsthm}

% lists
\usepackage{enumerate}

% images
\usepackage{graphicx} % for images

% code blocks
\usepackage{minted, listings} 

% verbatim greek
\usepackage{alphabeta}

\graphicspath{{./assets/images}}

\newcommand{\solution}{\textbf{Solution:}} 
\newcommand{\example}{\textbf{Example: }}
\newcommand{\R}{\mathbb{R}}

\title{BIOMATH 208 Week 2}

\author{Aidan Jan}
\date{\today}

\begin{document}
\maketitle
\section*{Multilinear Algebra}
\subsection*{Motivation}
two very different types of vectors arise in imaging:
\begin{itemize}
    \item Tangents to curves/surfaces: Important for describing geometry and anatomy
    \item Image gradients: Important for optimization and feature identification.
\end{itemize}
If we chose a new coordinate system to represent our data, these vectors transform differently.\\\\
Next, we will apply a linear transform, a rotation matrix $A$, to some imaging data and see how these objects change.\\\\
Then we'll apply a different linear transform, a rotation and scale $B$, to some imaging data, and see how the objects change.
\subsection*{Example: Brain Image}
Consider this brain image represented by ellipses:
\begin{center}
    \includegraphics*[scale=1]{W2_1.png}
\end{center}
Now, assume this is an image of a patient's brain, and a surgeon is standing directly over it.
\begin{center}
    \includegraphics*[scale=1]{W2_2.png}
\end{center}
\begin{itemize}
    \item This is like zooming into the image, thus, Image $I$: $X \rightarrow R$
\end{itemize}
With this transformation, image gradients, tangents, and points are all preserved.
\begin{center}
    \includegraphics*[scale=0.9]{W2_3.png}
\end{center}
Now, suppose the surgeon turns his head, so he is viewing the head at a different angle:
\begin{itemize}
    \item A point in the domain of $I$ is rotated to a point in the domain of $J$ with the linear map $A\::\: X \rightarrow Y$.
\end{itemize}
\begin{center}
    \includegraphics*[width=\textwidth]{W2_4.png}
\end{center}
In this case, points and vectors are moved based on the rotation matrix multiplied.  However, this can be expressed in linear algebra as a change of basis, and thus, points and vectors are preseved relative to the new coordinate system.\\\\
Finally, consider if the surgeon moved his head lower, so now he is not looking at the patient's brain from straight top-down, but rather at an angle.
\begin{center}
    \includegraphics*[width=\textwidth]{W2_5.png}
\end{center}
Now, notice that the point may be at the same place relative to the new basis, but the gradient and tangent vectors have shifted.
\subsection*{Explanation}
A tangent $v$ connects two nearby points, $q_0, q_1$.  We can define $v = q_1 - q_0$.  If we transform the points, $q \rightarrow Aq$, we transform the vector.
\[Aq_1 - Aq_0 = A(q_1 - q_0) = Av\]
A gradient points from dark to bright in an image.  If we transform the image, $I(x) \rightarrow I(A^{-1}x)$, we transform the gradient.  Let the gradient at $q$, $g = \nabla l(q)$ be a column vector, then the chain rule says:
\[\nabla[I(A^{-1}x)]_{x = q} = A^{-T}\nabla I(A^{-1}x)_{x=q}\]
so that the transformed gradient is $A^{-T}g$.
\subsection*{Details}
What is the $k$-th component of $\nabla I(A^{-1}x)$?  Let $B = A^{-1}$ and the components of $B$ be written as $B_{ij}$.\\\\
Let $(x, y)$ map to $(B_{00} x + B_{01}y, B_{10}x + B_{11}y)$.  To find the gradient, we take the derivative.
\begin{align*}
    \frac{\partial}{\partial x} &I(B_{00}x + B_{01}y, B_{10}x + B_{11}y)\\
    \intertext{By chain rule, \dots}
    &= \partial_0 I(B_{00}x + B_{01}y, B_{10}x + B_{11}y) B_{00} + \partial_1 I(B_{00}x + B_{01}y, B_{10}x + B_{11}y) B_{10}\\
    \intertext{$\partial_0$ means "partial derivative with respect to the 0th argument"}
    \intertext{Now, we take the derivative in terms of $y$.}
    \frac{\partial}{\partial y} &I(B_{00}x + B_{01}y, B_{10}x + B_{11}y)\\
    &= \partial_0 I(B_{00}x + B_{01}y, B_{10}x + B_{11}y) B_{01} + \partial_1 I(B_{00}x + B_{01}y, B_{10}x + B_{11}y) B_{11}
\end{align*} 
What we have essentially done here is:
\begin{align*}
    \begin{pmatrix} B_{00} & B_{10} \\ B_{01} & B_{11} \end{pmatrix} &\cdot \begin{pmatrix}\partial_0 I(B_{00}x + B_{01}y, B_{10}x + B_{11}y) \\ \partial_1 I(B_{00}x + B_{01}y, B_{10}x + B_{11}y)\end{pmatrix}\\
    &= B^T \cdot \nabla I(B_{00}x + B_{01}y, B_{10}x + B_{11}y)\\
    &= A^{-T}\nabla I(B_{00}x + B_{01}y, B_{10}x + B_{11}y)
\end{align*}
$A^{-T}$ is the inverse and transpose of $A$. This is equal to $B^T$, since by our definition, $B = A^{-1}$.

\subsection*{Definition of Vector Space}
A set $V$ with two operations $+$ (plus) and $\cdot$ (times, i.e., scalar multiplication) is a vector space if the operations satisfy CANI-ADDU.  Here, let $u, v, w \in V$ and $a, b \in \R$.
\begin{enumerate}
    \item \textbf{C}ommutativity: $u + v = v + u$ 
    \item \textbf{A}ssociativity: $(u + v) + w = u + (v + w) = u + v + w$
    \begin{itemize}
        \item $+$ needs to be a function of two variables
    \end{itemize}
    \item \textbf{N}eutral element: $\exists O \in V$ such that $O + u = u + O = u$
    \begin{itemize}
        \item This is often the zero vector, or the origin.
    \end{itemize}
    \item \textbf{I}nverse element: $\exists (-v)$ such that $v + (-v) = (-v) + v = 0$
    \item \textbf{A}ssociativity: $a \cdot (b \cdot u) = (a \cdot b) \cdot u$, where $(b \cdot u) \rightarrow \R \times V \rightarrow V$, and $(a \cdot b) \rightarrow \R \times \R \rightarrow \R$
    \item \textbf{D}istributivity over $+$ on $V$: $a \cdot (u + v) = a \cdot u + a \cdot v$
    \item \textbf{D}istributivity over $\cdot$ on $\R$: $(a + b) \cdot u = a \cdot u + b \cdot u$
    \item \textbf{U}nitary element: $\exists 1$ such that $1 \cdot u = u$.
\end{enumerate}
\subsection*{Example Vector Spaces}
\subsubsection*{Example: Real Numbers}
Real numbers form a vector space with the usual definition of $+$ and $\cdot$.
\begin{itemize}
    \item a single number, not pairs or triples
    \item not "direction and magnitude" but still obey vector space axioms
\end{itemize}
\subsubsection*{Example: The Cartesian Plane}
Set of ordered pairs, $\R \times \R$.  $(a, b) \rightarrow$ a vector in the cartesion plane.\\\\
We can define $+$ and $\cdot$ as the following:
\begin{itemize}
    \item $+$: $(a, b) + (c, d) = (a + c, b + d)$, where all $a, b, c, d, a + c, b + d \in \R$.
    \item $\cdot$: $c \cdot (a, b) = (c \cdot a, c \cdot b)$, where $a, b, c, c \cdot a, c \cdot b \in \R$.
\end{itemize}
Using these rules, we can build a more complex vector space, using definitions from a simpler one.
\subsubsection*{Example: Arrows on paper with tail at a fixed origin}
A vector is an arrow you can draw on paper.  We can define $+$ and $\cdot$ as the following:
\begin{itemize}
    \item $+$: "translate the second arrow so its tail lines up with the tip of the first arrow, draw a new arrow from the origin to the tip of the second."  Basically, add the vectors.
    \item $\cdot$: "shrink or grow the vector by this factor, and if the factor is negative, reflect it over the origin"
\end{itemize}
\subsubsection*{Example: Functions from $\R \rightarrow \R$}
Let $f, g$ be functions $\rightarrow$ choose a set of functions with properties we like
\begin{itemize}
    \item Define $f + g$ by evaluating any point $x \in \R$, define $(f+g)(x) = f(x) + g(x)$.  $f$ and $g$ are functions $\R \rightarrow \R$.
    \item Define $a \cdot f$ by evaluating at any point $x \in \R$, define $(a \cdot f)(x) = a \cdot f(x)$, where $(a \cdot f \in V)$ and $a \cdot f(x) \in \R$.
\end{itemize}
\subsubsection*{Example: Probabilities}
Let $p$ represent the probability that an event happens, and $q$ represent the probability that another event happens.
\begin{itemize}
    \item $+$: $p + q$ might be bigger than 1, which is not a probability!
    \item $\cdot$: $(-1) \cdot p$ is less than 0, which is also not a probability!
\end{itemize}
However, if we use logits, $\log \frac{p}{1-p}$, we can define $+$ and $\cdot$ without any problems.  (This will be covered at the end of the course.)
\subsubsection*{Example: Arrows anywhere on paper}
The idea with this one is that we do not necessarily have a fixed origin.  Because of this, we do not have a good definition of $+$ and $\cdot$.  Or at least, that is when we imagine the vectors are on a flat sheet of paper.\\\\
Consider if the vectors were drawn on a sphere.  
\begin{itemize}
    \item If two vectors are tangent to the same point, we can add them tip to tail.
    \item If they are not, then there is no way to add the vectors.  We would consider them to be vectors in different vector spaces.
\end{itemize}
\section*{Linear Algebra Definitions}
\begin{itemize}
    \item Linear Independence: A set of vectors $v_i$ are linearly independent if 
    \[\sum a_i v_i = 0 \Longleftrightarrow a_i = 0 \forall i\] 
    Otherwise they are linearly dependent.
    \item Basis: A linear independent set of vectors $B$ is called a basis if any vector in $V$ can be written as a unique linear combination of its elements.
    \item Dimension: The number of vectors in this set is called the dimension of the vector space.
    \item Coordinates: If the dimension is finite, we write basis elements as $e_i$, then 
    \[v = \sum_i v^i e_i\]
    for $v^i \in \R$.  The $v^i$ are the coordinates of the vector with respect to this basis.
    \item Einstein summation notation: In expressions like the above we will usually remove the summation sign and write
    \[v = v^i e_i\]
    Any pair of indices with one up and one down is summed.
\end{itemize}
\subsection*{Color Convention}
We will use the convention
\begin{itemize}
    \item vectors in red
    \item scalars in black
\end{itemize}
The previous example will be written
\[\textcolor{red}{v} = v^i \textcolor{red}{e_i}\]
\subsection*{Example: Polynomials}
\subsection*{Linear Maps}
A linear map $L \::\: U \rightarrow V$, is a map between two vector spaces $(U, V)$ that is compatible with addition and scalar multiplication.  For $\textcolor{red}{u}, \textcolor{red}{v} \in U$, and $a \in \R$, this means:
\begin{enumerate}
    \item $+$: $L(\textcolor{red}{u + v}) = L(\textcolor{red}{u}) + L(\textcolor{red}{v})$
    \begin{itemize}
        \item "Adding the inputs is the same as adding the outputs"
    \end{itemize}
    \item $\cdot$: $L(a \cdot \textcolor{red}{u}) = a \cdot L(\textcolor{red}{u})$
\end{enumerate}
\subsubsection*{Linear Maps are a vector space.}
\textbf{Definition: The vector space of linear maps:}\\
The set of linear maps from $U$ to $V$ is a vector space, $\text{Hom}(U, V)$ (homomorphisms).  We define $+$ and $\cdot$ for each input, borrowing the definition from $V$.  Let $\text{Hom}(U, V) = H, A, B \in H, \textcolor{red}{u} \in U$, and $a \in \R$.
\begin{itemize}
    \item $+$: $A +_H B$, where $+_H$ is a homomorphism, $(A +_H B)(\textcolor{red}{u}) = A(\textcolor{red}{u}) +_V B(\textcolor{red}{u})$
    \item $\cdot$: $a \cdot_H A$, $(a \cdot_H A)(\textcolor{red}{u}) = a \cdot_V A(\textcolor{red}{u})$
    \item e.g., we can add matrices elementwise, and we can scale them elementwise.
\end{itemize}
\textbf{Definition: Endomorphism:}\\
The set of linear maps $\text{Hom}(V, V)$ are called endomorphisms.
\subsection*{Covectors}
A covector is the vector space $\text{Hom}[V, \R]$, denoted $V^*$ are called covectors.  $V^*$ is called the dual space (or just dual) of $V$.  $V$ is called primal.
\subsubsection*{Example: Row and Column Vectors}
Often we write the components of vectors in $V$ as a column.  Then the components of $V^*$ are written as a row, and act on the left using matrix multiplication.
\subsubsection*{Example: Derivatives}
Partial derivatives of a function are covectors, and are often written as a row.

\subsection*{Color Convention Pt. 2}
We will expand our convention:
\begin{itemize}
    \item vectors in red
    \item covectors in blue
    \item scalars in black.
\end{itemize}
Vectors will generally be denoted with Roman letters near the end of the alphabet (e.g., $\textcolor{red}{u, v, w, x, y, z}$).\\\\
Covectors will generally be denoted with Greek letters (e.g., $\textcolor{blue}{\mu, \upsilon, \omega, \chi}$)\\\\
Scalars will generally be denoted with Roman letters near the beginning of the alphabet (e.g., $a, b, c$).

\subsection*{A Basic for Covectors}
\textbf{Definition: Dual Basis:}\\
Once a basis for $V$ is fixed ($\textcolor{red}{e_i}$), we can uniquely define a basis of $V^*$, $\textcolor{blue}{\epsilon}^i$ by the relationship
\[\textcolor{blue}{\epsilon^i}(\textcolor{red}{e_j}) = \delta^i_j = \begin{cases} 1 & \text{ if } i = j \\ 0 & \text{ otherwise}\end{cases}\]

\subsection*{Recovering Vector Coordinates}
If a vector is written in coordinates as $\textcolor{red}{v} = v^i \textcolor{red}{e_i}$, then the $i$-th coordinate is found by
\[v^i = \textcolor{blue}{\epsilon^i}(\textcolor{red}{v})\]
\textbf{Proof:}
\begin{align*}
    \textcolor{blue}{\epsilon^i}(\textcolor{red}{v}) &= \textcolor{blue}{\epsilon^i}(v^j \textcolor{red}{e}_j)\\
    &= v^j \cdot \textcolor{blue}{\epsilon^i}(\textcolor{red}{e}_j)\\
    &= v^j \delta_j^i \hspace{1.9cm} \text{Definition of dual basis}\\
    &= v^i \hspace{2.2cm} \text{All terms are 0 except for the $i$-th term}
\end{align*}

\subsection*{Recovering Covector Coordinates}
If a covector is written in coordinates as $\textcolor{blue}{\mu} = \mu^i \textcolor{blue}{\epsilon^i}$, then the $i$-th coordinate is found by:
\[\mu^i = \textcolor{blue}{\mu}(\textcolor{red}{e_i})\]
\textbf{Proof:}
\begin{align*}
    \textcolor{blue}{\mu}(\textcolor{red}{e_i}) &= (\mu_j \textcolor{blue}{e^j})(\textcolor{red}{e_i})\\
    &= \mu_j \textcolor{blue}{\epsilon^j} (\textcolor{red}{e_i}) \\
    &= \mu_j \delta_i^j \hspace{2cm} \text{Definition of dual}\\
    &= \mu_i
\end{align*}

\subsection*{Dual of the dual}
The vector space of linear maps from $V^* \rightarrow \R$ is denoted $V^{**}$, and is isomorphic to $V$.\\\\
If $\textcolor{blue}{\mu} \in V^*$, and $\textcolor{red}{w} \in V^{**} \simeq V$, then we define
\[\textcolor{red}{w}(\textcolor{blue}{\mu}) \doteq \textcolor{blue}{\mu}(\textcolor{red}{w})\]
We may use this notation when we consider a sequence of maps acting from right to left.  Often we will leave out parentheses.

\subsection*{Example: Dual Basis for Ordered Pairs}
Let $V = \R^2$, and a basis be given by $\textcolor{red}{e_0} = \begin{pmatrix} 1 \\ 0 \end{pmatrix}$ and $\textcolor{red}{e_1} = \begin{pmatrix} 1 \\ 1 \end{pmatrix}$.  Considering $V^*$ as row vectors, find the dual basis.\\\\
Essentially, we are solving for $\textcolor{blue}{\epsilon^0}$ and $\textcolor{blue}{\epsilon^1}$.
\begin{align*}
    \textcolor{blue}{\epsilon^0}(\textcolor{red}{e_0}) &= \delta_0^0\\
    &= 1 = \epsilon^{00} \cdot 1 + \epsilon^{01} \cdot 0 \\
    &= \epsilon^{00}\\
    \intertext{The multiplication by 1 and 0 are the components of $\textcolor{red}{e_0}$.}
    \textcolor{blue}{\epsilon^0}(\textcolor{red}{e_1}) &= \delta_1^0\\
    &= 0 = \epsilon^{00} \cdot 1 + \epsilon^{01} \cdot 1\\
    &= \epsilon^{00} + \epsilon^{01} = 0\\
    \intertext{In this case, we multiply by 1 for both $\epsilon$ terms since the components of $\textcolor{red}{e_1}$ are both 1.}
    \textcolor{blue}{\epsilon^1}(\textcolor{red}{e_0}) &= \delta_0^1\\
    &= 0 = \epsilon^{10} \cdot 1 + \epsilon^{11} \cdot 0\\
    &= \epsilon^{10}\\\\
    \textcolor{blue}{\epsilon^1}(\textcolor{red}{e_1}) &= \delta_1^1\\
    &= 1 = \epsilon^{10} \cdot 1 + \epsilon^{11} \cdot 1\\
    &= \epsilon^{10} + \epsilon^{11} = 1\\
    \intertext{At this point, we have four linear equations and four variables, $\epsilon^{00}, \epsilon^{01}, \epsilon^{11}, \epsilon^{10}$.  Solving, we get:}
    \epsilon^0 &= (1, -1)\\
    \epsilon^1 &= (0, 1)
\end{align*}
\subsection*{Adjoint of a Linear Map}
\textbf{Definition:}
\begin{itemize}
    \item If $L\::\: U \rightarrow V$, with $\textcolor{red}{v} \in U$ and $\textcolor{blue}{\mu} \in V^*$, then we can define the map $L^* \::\: V^* \rightarrow U^*$ implicitly by:
    \[\textcolor{blue}{\mu}(L(\textcolor{red}{v})) \doteq (L^*(\textcolor{blue}{\mu}))(\textcolor{red}{v})\]
    \item If $f$ is a function from $U$ to $V$, then $Df$ (the derivative) is a linear map from $U$ to $V$, and $Df^*$ is a linear map from $V^*$ to $U^*$.  The latter is used for "backpropagation" when training neural networks.
\end{itemize}

\subsection*{Example: Adjoint of a Matrix}
Let $U, V$ be vector spaces, $\textcolor{red}{u} \in U, \textcolor{blue}{v} \in V^*$, and $A \in \text{Hom}(U, V)$.  Let's identify $A^*$, using matrix multiplication:
\begin{itemize}
    \item Let $\textcolor{red}{u}$ be columns, and $\textcolor{blue}{v}$ be rows.
\end{itemize}
\[\textcolor{blue}{\mu}(A\textcolor{red}{u}) \rightarrow \R\]
\begin{itemize}
    \item Notice that $A$ can be applied on the left of the colum input, or ght right of the row, due to how matrix multiplication works.
    \item "adjoint of $A$ acts to its left on a row"
\end{itemize}
Now, let both $\textcolor{red}{u}$ and $\textcolor{blue}{v}$ be columns.  Then,
\[\textcolor{blue}{\mu}^T(A\textcolor{red}{u}) = (A^T \textcolor{blue}{\mu})^T \textcolor{red}{v}\]
\begin{itemize}
    \item "adjoint of $A$ acts with its transpose to the right on a column"
\end{itemize}

\subsection*{Coordinates of a linear map}
\textbf{Definition:}
\begin{itemize}
    \item If $L$ is a linear map from $U$ to $V$, we can express its coordinates using the bases of $U$ and $V^*$.
    \[L_j^i = \textcolor{blue}{\epsilon_V^i}[L(\textcolor{red}{e_j^U})]\]
\end{itemize}
To build a linear map from coordinates, we can write:
\[L = L_j^i \textcolor{red}{e_i^V} \textcolor{blue}{\epsilon_U^j}\]

\subsection*{Example: Matrix Vector Multiplication}
What is the $i$-th coordinate of $L(\textcolor{red}{v})$?
\begin{align*}
    \textcolor{blue}{\epsilon^i}(L(\textcolor{red}{v})) &= \textcolor{blue}{\epsilon^i}(L(v^j\textcolor{red}{e_j}))\\
    &= v^j \textcolor{blue}{\epsilon^i}(L(\textcolor{red}{e_j}))\\
    &= L_j^i v^j
\end{align*}
Note that for any value mentioned here, exponents are rows, subscripts are columns.
This can be written as:
\[\begin{pmatrix} L_0^0 & L_1^0 \\ L_0^1 & L_1^1\end{pmatrix} \begin{pmatrix} v^0 \\ v^1 \end{pmatrix} = \begin{pmatrix}L_0^0 v^0 + L_1^0 v^1 \\ L_0^1 v^0 + L_1^1 v^1\end{pmatrix}\]

\subsection*{Coordinates of an adjoint map}
\textbf{Definition:}
\begin{itemize}
    \item If $L*$ is the adjoint of $L$ and a map from $V^*$ to $U^*$, we can express its coordinates using the bases of $V$ and $U^*$.
    \[[L^*]_j^i = [L^*(\textcolor{blue}{\epsilon_V^i})](\textcolor{red}{e_j^U}) = \textcolor{blue}{\epsilon_V^i}[L(\textcolor{red}{e_U^j})] = L_j^i\]
    \item These are the same coordinates as $L$, even though this is a map between different vector spaces!
\end{itemize}
To build an adjoint map from coordinates, we can write:
\[L^* = L_j^i \textcolor{blue}{\epsilon_U^j} \textcolor{red}{e_i^V} \]
We act from right to left, and identify $e_i$ with an element of $V^*$.

\subsection*{Example: Matrix covector multiplication}
What is the $i$-th coordinate of $L^*(\textcolor{blue}{\mu})$?
\begin{align*}
    L^*(\textcolor{blue}{\mu})(\textcolor{red}{e_i}) &= \textcolor{blue}{\mu}(L(\textcolor{red}{e_i}))\\
    &= \mu_j \textcolor{blue}{\epsilon^j}(L(\textcolor{red}{e_i}))\\
    &= \mu_j L_i^j
\end{align*}
Notice that:
\begin{itemize}
    \item This is the same set of formulas as for vectors.
    \item Instead of multiplying across columns (like vectors), we are multiplying across rows.  Specifically, this is similar to multiplication with the transpose, or matrix multiplication to the right of a row vector.
\end{itemize}

\subsection*{Change of basis}
Let $A$ and $B$ be two bases, with elements $\textcolor{red}{e_i^A}$ and $\textcolor{red}{e_i^B}$.  Their elements can be related by a linear map $L$, with
\[L(\textcolor{red}{e_i^A}) = \textcolor{red}{e_i^B}, \hspace{0.5cm}L^{-1}(\textcolor{red}{e_i^B}) = \textcolor{red}{e_i^A}\]
The corresponding relationship for covectors is
\[L^*(\textcolor{blue}{\epsilon_i^B}) = \textcolor{blue}{\epsilon_i^A}, \hspace{0.5cm}L^{-*}(\textcolor{blue}{\epsilon_i^A}) = \textcolor{blue}{\epsilon_i^B}\]

\subsection*{Components of Change of Basis}
\begin{itemize}
    \item The components are the same in either bases, $L_j^i = \textcolor{blue}{\epsilon_A^i} \textcolor{red}{e_j^B}$
\end{itemize}
\subsubsection*{Proof:}
In the basis $A$:
\[[L_A]_j^i = \textcolor{blue}{\epsilon_A^i}(L(\textcolor{red}{e_j^A})) = \textcolor{blue}{\epsilon_A^i}(\textcolor{red}{e_j^B})\]
In the basis $B$:
\[[L_B]_j^i = \textcolor{blue}{\epsilon_B^i}(L(\textcolor{red}{e_j^B})) = (L^*(\textcolor{blue}{\epsilon_B^i})(\textcolor{red}{e_j^B})) = \textcolor{blue}{\epsilon_A^i}(\textcolor{red}{e_j^B})\]
Similarly for the inverse:
\[[L^{-1}]_j^i = \textcolor{blue}{\epsilon_B^i}(\textcolor{red}{e_j^A})\]

\subsection*{Components of Basis Vectors}
We have that $\textcolor{red}{e_i^B} = L_i^j \textcolor{red}{e_j^A}$ and $\textcolor{blue}{\epsilon_A^i} = L_j^i \textcolor{blue}{\epsilon_B^j}$.

\subsection*{Components of a vector under change of basis}
If basis elements change with the map $L$, vector coordinates change with $L^{-1}$.
\subsubsection*{Proof:}
Let $a^i$ be components of $\textcolor{red}{v}$ in basis $A$, and $b^i$ be components in basis $B$, then
\[b^i = \textcolor{blue}{\epsilon_B^i}(\textcolor{red}{v}) = \textcolor{blue}{\epsilon_B^i}(a^j \textcolor{red}{e_j^A}) = \textcolor{blue}{\epsilon_B^i}(\textcolor{red}{e_j^A}) a^j = (L^{-1})_j^i a^j\]
\begin{itemize}
    \item If the basis vectors grow, the components have to shrink to give the same vector.
\end{itemize}

\subsection*{Components of a covector under change of basis}
Relative to vectors, components of covectors transform using the inverse transpose of $L$.
\subsubsection*{Proof:}
Let $\alpha_i$ be components of $\textcolor{blue}{\mu}$ in basis $A$, and $\beta_i$ be components in basis $B$, then,
\begin{align*}
    \textcolor{blue}{\mu} &= \textcolor{blue}{\alpha_i \epsilon_A^i} = \textcolor{blue}{\beta_i \epsilon_B^i}\\
    \beta_i &= \textcolor{blue}{\mu}(\textcolor{red}{e_i^B}) = \alpha_j \textcolor{blue}{\epsilon_A^j}(\textcolor{red}{e_i^B}) = \alpha_j L_i^j
\end{align*}
Note: If $L$ is unitary, these two are the same.

\subsection*{Visualizing a Change of Basis for Vectors}
First, let's draw some vectors on a coordinate grid.  Notice that the two basis vectors are \textbf{not} orthonormal.
\begin{center}
    \includegraphics*[scale=0.9]{W2_6.png}
\end{center}
However, we can still define $v$ in terms of $e_1$ and $e_2$.\\\\
For an example, let's stretch the basis vector $e_1$ by a factor of 2.  Note that now although $v$ is the same vector, its coordinates has changed.
\begin{center}
    \includegraphics*[scale=0.9]{W2_7.png}
\end{center}
Now, let's add some covectors.  (Note that you \textbf{cannot} add vectors and covectors.)\\
We will also add a coordinate plane, but this time in terms of $\epsilon_1$ and $\epsilon_2$.\\
\textbf{Additionally, note that vector $v$ is now covector $\mu$.}  Essentially, that vector is expressed in two different basis, but are the same vector.
\begin{center}
    \includegraphics*[scale=0.9]{W2_8.png}
\end{center}
Now, suppose we stretch $e_1$ again.
\begin{center}
    \includegraphics*[scale=0.9]{W2_9.png}
\end{center}
Notice now that this time, $\epsilon_1$ has also changed length.
\subsection*{Summary of Vector and Covector identities}
Consider vector spaces, $U$, $V$, with $u \in U$, $v \in V$, $\mu \in U^*$, and $v \in V^*$.
\begin{center}
    \includegraphics*[scale=0.9]{W2_10.png}
\end{center}

\subsection*{Multilinear Maps}
\textbf{Definition: ((p, q) Tensor)}
\begin{itemize}
    \item A (p, q) tensor is a multilinear map that inputs $p \in \mathbb{N}$ different covectors in $V^*$, and $q \in \mathbb{N}$ different vectors in $V$, and outputs a real number.
    \item Multilinear means it is linear in each input individually, with all the other inputs fixed.
    \item Given a basis of $V$ and a dual basis of $V^*$, we can write coordinates
    \[T = T_{b_1, \dots, b_q}^{a_1, \dots, a_p} \textcolor{red}{e_{a_1}} \otimes \cdots \otimes \textcolor{red}{e_{a_p}} \otimes \textcolor{blue}{\epsilon^{b_1}} \otimes \cdots \otimes \textcolor{blue}{\epsilon^{b_q}}\]
    where $a$ and $b$ are sequences taking values in $\{0, d - 1\}$ where $d$ is the dimension of $V$.
\end{itemize}
\subsection*{Example: A (1, 1) Tensor}
We can find components $T_j^i = T(\textcolor{blue}{\epsilon^i}, \textcolor{red}{e_j})$, and write $T = T_j^i \textcolor{red}{e_i} \otimes \textcolor{blue}{\epsilon^j}$.\\\\
Its action on a pair of inputs $\textcolor{blue}{\mu}, \textcolor{red}{v}$ is:
\begin{align*}
    T(\textcolor{blue}{\mu}, \textcolor{red}{v}) &= T(\mu_i\textcolor{blue}{\epsilon^i}, v^i \textcolor{red}{e_j})\\
    &= \mu_i T(\textcolor{blue}{\epsilon^i}, v^j \textcolor{red}{e_j})\\
    &= \mu_i v^j T(\textcolor{blue}{\epsilon^i}, \textcolor{red}{e_j})\\
    &= \mu_i v^j T_j^i
\end{align*}
A linear map can be thought of as a (1, 1) tensor if we allow it to act "partially" on one vector.

\subsection*{Covectors and vectors as tensors}
\begin{itemize}
    \item A covector is an example of a (0, 1) tensor, because it inputs one vector, and outputs a real number.
    \item A vector is an example of a (1, 0) tensor, because it inputs one covector (thinking of it in $V^{**}$, and outputs a real number.)
    \item A scalor in $\R$ is technically a (0, 0) tensor.
\end{itemize}

\subsection*{Inner Products}
\textbf{Definition:} An inner product is a (0, 2) tensor (which means 2 vector input, real number output), $g \::\: V \times V \rightarrow \R$, often written $g(\cdot, \cdot) = \langle \cdot, \cdot \rangle$.  It must have the following properties:
\begin{enumerate}
    \item Symmetry: $g(\textcolor{red}{u}, \textcolor{red}{v}) = g(\textcolor{red}{v}, \textcolor{red}{u})$
    \item Non degeneracy: $g(\textcolor{red}{u}, \textcolor{red}{u}) \geq 0$, and it is only equal to 0, if $\textcolor{red}{\mu} = \textcolor{red}{\vec{0}}$
\end{enumerate}

\subsection*{Lengths and Angles}
\begin{itemize}
    \item \textbf{Definition: Length}: We define $\sqrt{g(\textcolor{red}{u}, \textcolor{red}{u})} = |\textcolor{red}{u}|$ as the length of vector $\textcolor{red}{u}$.
    \item \textbf{Definition: Angle}: The angle between $\textcolor{red}{u}$ and $\textcolor{red}{v}$ is given by $\cos(\theta) = g\left(\frac{\textcolor{red}{u}}{|\textcolor{red}{u}|}, \frac{\textcolor{red}{v}}{|\textcolor{red}{v}|}\right)$
\end{itemize}
Note that $g(\textcolor{red}{u}, \textcolor{red}{v}) = |\textcolor{red}{u}||\textcolor{red}{v}| \cdot \cos(\theta) = \textcolor{red}{u} \cdot \textcolor{red}{v}$.

\subsection*{Euclidian Distance}
Often the specification of a dot product is ignored, and is chosen implicitly as "multiply the components and add them up".  This corresponds to:
\[g(\textcolor{red}{u}, \textcolor{red}{v}) = \delta_{ij} \textcolor{red}{u^i v^i} = \sum_i \textcolor{red}{u^i v^i}\]
This depends on a choice of basis!
\begin{itemize}
    \item We cannot write that sum down in Einstein convention, because there is no up/down index!
    \item We can 'fix' the definition by adding the components of a (0, 2) tensor, $T_{ij} = \delta_{ij}$, where $T_{ij}$ is like an identity matrix.
\end{itemize}

\subsection*{Example: Polynomials on [0, 1]}
Let $V$ be the space of order $d - 1$ polynomials from $[0, 1] \rightarrow \R$, with basis vectors $\textcolor{red}{e_i}(x) = x^i$.  Let $\textcolor{red}{f}, \textcolor{red}{h} \in V$ and define
\[\langle \textcolor{red}{f}, \textcolor{red}{h} \rangle = \int_0^1 \textcolor{red}{f}(x) \textcolor{red}{h}(x) \text{d}x\]
This integral is basically multiply the components (f and h), and add them up (the integral)\\\\
What are the components?
\begin{align*}
    g_{ij} &= g(\textcolor{red}{e_i}, \textcolor{red}{e_j})\\
    &= \int_0^1 x^i x^j \text{d}x\\
    &= \int_0^1 x^{i + j} \text{d}x\\
    &= \left.\frac{1}{i + j + 1} x^{i + j + 1} \right|_0^1\\
    &= \frac{1}{i + j + 1}
\end{align*}
In this example, we get $g_{ij} = \frac{1}{i + j + 1}$, which is not the identity matrix.

\subsection*{Change of Basis for Inner Products}
An inner product matrix transforms under a change of basis differently than a linear map.\\\\
\subsubsection*{Proof:}
For a metric $g$,
\begin{align*}
    g_{ij}^B &= g(\textcolor{red}{e_i^B, e_j^B})\\
    &= g(L_i^k \textcolor{red}{e_k^A}, L_j^l \textcolor{red}{e_l^A})\\
    &= L_i^k L_j^l g(\textcolor{red}{e_k^A}, \textcolor{red}{e_l^A})\\
    &= L_i^k L_j^l g_{kl}
\end{align*}
But for a linear map $M$, 
\begin{align*}
    (M^B)_j^i &= \textcolor{blue}{\epsilon_B^i}(M(\textcolor{red}{e_j^B}))\\
    &= \textcolor{blue}{\epsilon_B^i}(M(L_j^k \textcolor{red}{e_k^A}))\\
    &= L_j^k \textcolor{blue}{\epsilon_B^i} M(\textcolor{red}{e_k^A})\\
    &= (L^{-1})_l^i \textcolor{blue}{\epsilon_n^l}(M(L_j^k \textcolor{red}{e_k^m}))\\
    &= (L^{-1})_l^i L_j^k \textcolor{blue}{\epsilon_A^l}(M(\textcolor{red}{e_k^A}))\\
    &= (L^{-1})_l^i L_j^k M_k^l
\end{align*}
Notice that when you change components, these turn out to be very different.

\subsection*{Example: The Mahalanobis Distance}
Choosing an inverse covariance matrix as a metric distance is called the Mahalanobis distance.\\\\
Principal Component Analysis (PCA) allows us to find a basis in which this metric is the identity matrix.
\begin{itemize}
    \item E.g., find a new basis, such that in this basis, the metric is the identity
\end{itemize}

\subsection*{The Musical Map (something we use an inner product for)}
\textbf{Definition: The Musical Map}: Given an inner product, $g$, we define the flat map $\flat \::\: V \rightarrow V^*$ by
\[\flat(\textcolor{red}{u})(\textcolor{red}{v}) = g(\textcolor{red}{u}, \textcolor{red}{v})\]
In coordinates, we write
\begin{align*}
    \flat(\textcolor{red}{u})_i &= \flat(\textcolor{red}{u})(e_i)\\
    &= g(\textcolor{red}{u}, e_i)\\
    &= g(\textcolor{red}{u^j} e_j, e_i)\\
    &= \textcolor{red}{u^j} g(e_j, e_i)\\
    &= \textcolor{red}{u^j} g_{ij}
\end{align*}
It lowers indices from up to down (like a flat lowers a musical note).\\\\
The sharp map $\sharp \::\: V^* \rightarrow V$ is defined as its inverse.
\subsubsection*{Uses of the Musical Map}
\begin{enumerate}
    \item Convert between covectors (gradients) and vectors (parameter perturbations) for "natural gradient descent".  
    In complicated spaces, we can specify this instead of the inner product.
    \item In other words, converts between vectors and covectors using inner product and dual space.
\end{enumerate}



\end{document}