% document formatting
\documentclass[10pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[left=1in,right=1in,top=1in,bottom=1in]{geometry}
\usepackage[T1]{fontenc}
\usepackage{xcolor}

% math symbols, etc.
\usepackage{amsmath, amsfonts, amssymb, amsthm}
\usepackage{dsfont}

% lists
\usepackage{enumerate}

% images
\usepackage{graphicx} % for images

% code blocks
\usepackage{minted, listings} 

% verbatim greek
\usepackage{alphabeta}

\graphicspath{{./assets/images}}

\newcommand{\solution}{\textbf{Solution:}} 
\newcommand{\example}{\textbf{Example: }}
\newcommand{\R}{\mathbb{R}}
\newcommand{\dd}{\text{d}}

\title{BIOMATH 208 Week 8}

\author{Aidan Jan}
\date{\today}

\begin{document}
\maketitle
\subsection*{Naive Gradient Descent in Vector Spaces}
Given a differentiable function $f \::\: \R^d \rightarrow \R$, the naive gradient descent algorithm procedes as follows:
\begin{enumerate}
    \item Choose an initial guess for some point $p$
    \item Compute $f(p)$, and $\dd f(p)$  (list of partial derivatives)
    \item Update $p \mapsto p - \varepsilon \dd f(p)$, $\varepsilon > 0 \in \R$, "small"
    \item Repeat 2 and 3, until we converge
    \begin{itemize}
        \item Stop after some number of repeats
        \item if $f(p)$ changes less than threshold, then stop
        \item if $p$ changes less than a threshold, then stop
        \item etc.
    \end{itemize}
    \item maybe, change $\varepsilon$ (step size, learning rate, etc.) based on some criteria.
\end{enumerate}

\subsection*{Gradient descent is a minimization method}
Gradient descent converges to a stationary point for small enough $\varepsilon$.

\subsubsection*{Proof}
Taylor expand $f$ about $p_0$.
\[f(p) = f(p_0) + \dd f(p_0) \cdot (p - p_0) + O(|p - p_0|^2)\]
\begin{itemize}
	\item $(p - p_0)$ is our step size.  Also, $p - p_0 = -\epsilon \dd f(p_0)$
	\item Notice we are taking the inner product of a covector and a vector.
	\item $f$ should be smooth, e.g., a derivative in at least one direction must be well defined.
	\item Essentially, $f$ of the new point $p$ is equal to $f$ of the old point $p_0$, plus the change when we took a step $\dd f(p_0) \cdot (p - p_0)$, plus our loss function $O(|p - p_0|^2)$.
\end{itemize}
If we substitute $p - p_0 = -\epsilon \dd f(p_0)$ for $(p - p_0)$, then we get
\[f(p_0) + (-\varepsilon) \cdot |\dd f(p_0)|^2 + O(|p - p_0|^2)\]
$|\dd f(p_0)|^2$ is a euclidean norm, and therefore that term is always $\leq f(p_0)$.  If it is equal to $f(p_0)$, then we have converged.

\subsection*{Problems with Gradient Descent}
\begin{enumerate}
    \item The components of $\dd f$ may have widely different scales.  Choosing "small enough" $\varepsilon$ may be dominated by the largest scale, making the algorithm impractically slow.
    \item The components of $\dd f$ depend on a choice of chart or basis.  Some charts may have good performance and others bad.  (e.g., preconditioning)
    \item Since $\dd f$ is a covector, it doesn't make sense to add a scalar multiple of it to a point in our vector space $\R^d$.  (you can't add a covector to a vector; they have different units!)
    \begin{itemize}
        \item if $f$ was unitless, and $p$ has units of length, then $\dd f(p)$ has units of "per length".
    \end{itemize}
\end{enumerate}

\subsection*{Example: Minimization with Different Units}
Consider a 2D vector space, where the first quantity is measured in centimeters, and the second in millimeters.  Then let 
\[f(p) = (p^0)^2 + \frac{2}{10^2} (p^1)^2\]
be the distance from the origin in centimeters.  The minimum is $p = 0$.
\begin{center} 
	\includegraphics*[scale=0.8]{W8_3.png} 
\end{center}

\subsubsection*{Example: Sequernce of Points in Gradient Descent}
The gradient $\dd f = (2p^0, 2/10^2 p^1)$, and a sequence of points for different values of $\epsilon$ are shown.
\begin{center} 
	\includegraphics*[width=\textwidth]{W8_4.png} 
\end{center}
\begin{itemize}
	\item The highest point on both images are the initial points.
	\item The $\dd f$ is the list of partial derivatives.
	\item The red dot is the origin.
\end{itemize}
Because of the gradient, the gradient in the $p^1$ direction is 100x smaller than the gradient in the $p_0$ direction.  This is why the step goes down very fast, and left very slow.\\\\
The right graph uses a much higher step size ($\varepsilon$), and therefore it takes a much larger step per iteration.

\subsection*{Natural Gradient Descent in a Coordinate Chart}
We can convert the gradient covector $\dd f$ to a vector $\dd f^\sharp$.  Its direction will be independent of the choice of chart.  This algorithm was invented by Shun'ichi Amari in the early 21st century.
\begin{enumerate}
    \item Choose an initial point $p$.  (On a Lie group we could pick identity)
    \item Compute $f(p)$, $\dd f(p)$, $g(p)$.  $g(p)$ is a metric that acts on two vectors, usually just a symmetric matrix.
    \item $x^i \mapsto x^i - \epsilon(g^{-1}(p))^{ij} \dd f_j(p)$, for small enough $\varepsilon$.
    \item Repeat until convergence
    \item Possibly, modify $\varepsilon$ based on same heuristic.
\end{enumerate}

\subsection*{One Problem}
Note: while the direction (tangent vector), is independent of a choice of chart, the new point is not.  (We still have to walk along a curve in that direction, which is not unique.)

\subsection*{Natural Minimization with Different Units}
It is natural to choose a metric that takes the different units into account, and returns a length in centimeters.  For the example before, let's define
\[g_{ij} = \begin{pmatrix} 1 & 0 \\ 0 & \frac{1}{100} \end{pmatrix}_{ij}\]
Therefore,
\[(g^{-1})^{ij} = \begin{pmatrix} 1 & 0 \\ 0 & 10^2 \end{pmatrix}_{ij}\]
The components of our gradient are then
\[(g^{-1})^{ij} \dd f(p)_j = \begin{pmatrix} 1 & 0 \\ 0 & 10^2 \end{pmatrix} \begin{pmatrix}
2 p^0 \\ \frac{2}{100} p^1 \end{pmatrix} = \begin{pmatrix} 2 p^0 \\ 2 p^1 \end{pmatrix}\]
\begin{center} 
	\includegraphics*[scale=0.8]{W8_5.png} 
\end{center}
\begin{itemize}
	\item Because we understand the geometry of this scenario better, we can scale the gradient of $p_1$, so now we walk straight towards the origin, instead of being dominated by the smaller component.
	\item Our step can be big, but not overstep the valley, since we choise a better direction.
    \item We are lucky that in this scenario, this is a quadratic optimization problem.  If it were more complicated, the results would not be as nice, but it still will help.
    \item Note, notice that the first matrix, $\begin{pmatrix} 1 & 0 \\ 0 & 10^2 \end{pmatrix}$ is actually the Hessian of $f$ (second derivative).  So this is an example of Newton's method.  In general, H$f$ is not positive definite, and in general, $g$ and $f$ are not related.
\end{itemize}

\section*{How to Choose a Metric}
\begin{itemize}
	\item Affine registration is a $d \times (d + 1)$ dimensional optimization problem, e.g., 12 dimensions for 3D registration.
	\item Choosing a metric means choosing $d(d + 1)[d(d + 1) - 1] / 2$ different numbers, e.g., 66 numbers for 3D registration.
	\item AND, we must do this at every point on the manifold.
	\item Luckily we have developed tools to make this problem simpler.
	\item For the previous problem, we just used distance squared, but most of the time, the problem isn't this simple.
\end{itemize}

\subsection*{Pull back metrics}
\begin{itemize}
	\item There is no "natural" way to put a metric on affine transforms $T$, but there is a natural way to put a metric on the points they act on, the standard Euclidean metric.
	\item Let $Q$ be a set of points and $\delta Q$, $\delta P$ two tangent vectors to the points at $Q$, then
	\[g_Q^{\R^{d \times N}} (\delta Q, \delta P) = \text{tr} (\delta Q \delta P^T)\]
    \begin{itemize}
        \item We are multiplying the components and adding them up
        \item $Q$ and $P$ are perturbations to points, $d \times N$ matrices for $N$ points
    \end{itemize}
    \item If $X$, $Y$ are tangent vectors at $T$, and $Q$ is a set of points, let $\phi(T) = T \cdot Q$.  Then we can define the pullback metric
    \[g_T^{\text{affine}}(X, Y) = [\phi^* g_Q^{\R^{d \times N}}](x, y) = g_Q^{\R^{d \times N}} (\phi_* X, \phi_* Y) = g_Q^{\R^{d \times N}} (XQ, YQ) = \text{tr}(XQ(YQ)^T)\]
    \begin{itemize}
        \item The bracketed term is the pull back.  The parenthetical (right side) is the push forward.
        \item Recall, push forward is evaluated by multiplying by the Jacobian matrix of $\phi$.
        \item Jacobian of $\phi$ is just $Q$.
    \end{itemize}
\end{itemize}

\subsection*{Natural Gradient Descent for Affine Registration in 1D}
Consider aligning a set of points $Q$ to another set of points $P$, over affine transformations $T$ with $T^0$ the linear part and $T^1$ the translation.  We seek to minimize
\[f(T) = \Vert T \cdot Q - P \Vert_F^2 = \sum_{i = 1}^N (T^0 Q_i + T^1 - P_i)^2\]
The gradient, $\dd f(T)$, is given by
\[\dd f_j(T) = \begin{pmatrix} 2 \sum_{i = 1}^N (T^0 Q_i + T^1 - P_i) Q_i \\ 2 \sum_{i = 1}^N (T^0 Q_i + T^1 - P_i) \end{pmatrix}\]
\begin{itemize}
	\item Now that we know how to evaluate the function and its gradient, we can apply naive gradient descent.
\end{itemize}
Points $Q$ and $P$ are shown on left, and trajectory is shown on right.
\begin{center} 
	\includegraphics*[width=\textwidth]{W8_6.png} 
\end{center}
Notice that the points are correlated, so we can approximate one as an affine function of the other.
\begin{itemize}
	\item Notice that in this graph, the translation seems to be an order of magnitude larger than the transformation.
	\item Also, notice that the graph of $P$ and $Q$ does not start at the origin.  Therefore, we need to find the center of mass of the points and scale accordingly.
	\item We will converge in $T^2$ quickly, but $T^1$ slowly.
\end{itemize}

\subsubsection*{Example: The Pullback Metric}
For $\phi(T) = T^0Q + T^1$, the push forward is
\begin{align*}
    \phi_*^T(u) &= (Q, 1)u
\end{align*}
\begin{itemize}
	\item $u$ is a tangent vector
\end{itemize}
So the pullback metric is
\begin{align*}
    g_T(u, v) &= (u^0 Q + u' 1)(v^0 Q + v' 1)^T\\
    &= \begin{pmatrix} u^0 & u^1 \end{pmatrix} \begin{pmatrix} QQ^T & Q 1^T \\ 1 Q^T & 1 \:1^t \end{pmatrix} \begin{pmatrix} v^0 \\ v^1 \end{pmatrix}\\
    &= \begin{pmatrix} u^0 & u^1 \end{pmatrix} \begin{pmatrix} \sum_{i = 1}^N Q_i^2 & \sum_{i = 1}^N Q_i \\ \sum_{i = 1}^N Q_i & N \end{pmatrix} \begin{pmatrix} v^0 \\ v^1 \end{pmatrix}
\end{align*}

\subsubsection*{Example: The Inverse Metric}
The inverse of the metric is then:
\[g^{-1}(T)_{ij} = \frac{1}{N \sum_{i = 1}^N Q_i^2 - \left(\sum_{i = 1}^N Q_i\right)^2} \begin{pmatrix} N & -\sum_{i = 1}^N Q_i \\ -\sum_{i = 1}^N Q_i & \sum_{i = 1}^N Q_i^2 \end{pmatrix}\]
Note that if the center of mass is 0, the metric is diagonal.

\subsubsection*{Example: The natural gradient descent trajectory}
Natural gradient descent improves the trajectory dramatically
\begin{center} 
	\includegraphics*[width=0.8\textwidth]{W8_7.png}
\end{center}
\begin{itemize}
	\item We have scale on the $y$-axis and translation on the $x$-axis.  
	\item We head straight to the origin this time!
	\item We can choose a bigger step size and converge quickly in both parameters.
\end{itemize}

\subsection*{Invariant Metrics}
\begin{itemize}
	\item Above, our metrics were independent of $p \in \mathcal{M}$, but this is not generally possible.  With Lie groups, we can define the metric at $p = I$, and use a pullback to define it for any other point.
	\item For $p, q \in \mathcal{M}$ we can define a map at each $p$ by $\phi_p (q) = p^{-1} \circ q$.
	\begin{itemize}
        \item If this is a matrix group, $\phi$ is matrix multiplication, so $\phi_*$ is just a matrix.  (Jacobian is just a matrix.)
    \end{itemize}
	\item We push forward a vector in $T_p \mathcal{M}$ to $T_I \mathcal{M}$ with $\phi_*(p)$, and define
    \[g_p(u, v) = g_I(\phi_*(p)u, \phi_* (p)v)\]
	\item This is called a "left invariant metric".  We could design a "right invariant metric" using $\phi_p(q) = q \circ p^{-1}$.
\end{itemize}


\section*{Review}
[FILL]

In general we cannot find analytic solutions.  We used the gradient descent algorithm, an iterative algorithm that has... [FILL]

\subsection*{Natural Gradient Descent}
\begin{enumerate}
    \item Start with an initial guess for our parameter, $p$.
    \item Compute $f(p)$, $\dd f(p)$, $g(p)$
    \item Replace the $i$-th component of $p$:  $p^i \mapsto p^i - \varepsilon(g(p)^{-1ij}) \dd f_j(p)$.  (Note that $g^{ij}$ means $(g^{-1}_{ij})$)
    \item Same as gradient descent
    \item Same as gradient descent
\end{enumerate}
\begin{itemize}
	\item This is especially helpful when compoennts of $p$ have different units
    \item Also gives a better search direction allowing a bigger step size $\varepsilon$.
\end{itemize}

\subsection*{How to choose a metric?}
\begin{itemize}
	\item In a $d$ dimensional manifold, $d(d - 1)/2$ functions, and they have to always be positive definite.
	\item Pull back metrics
	\begin{itemize}
        \item Find a map $\phi \::\: \mathcal{M} \rightarrow$ a nice space where choosing a metric is easy
    \end{itemize}
    \[g_p^{\mathcal{M}}(x, y) = g_{\phi(p)}^{\text{nice space}} (\phi_{\phi(p)}X, \phi_{\phi(p)}Y)\]
    \item We showed how to use this metric for affine transforms acting on points, because it's easy to define a metric on the points themselves.
    \item Invariant metric, for Lie groups:
    \begin{itemize}
        \item Define a metric at one point $p = I$, then for any other point $p$, we just use a pull back metric, pull back with $\phi = p^{-1}$.
    \end{itemize}
\end{itemize}

\section*{Image Registration}
(for images as functions, NOT landmark points.)\\\\
Consider a piar of grayscale images $I, J \::\: \R^d \rightarrow \R$, and a transformation acting via $[T \cdot I](x) = I(T^{-1}x)$.  We will minimize the integral square error objective function
\[f(T) = \int (I(*T^{-1} x) - J(x))^2 \dd x\]
using gradient based methods.\\\\
Note that even though this is a square error cost, it is \textbf{not quadratic} in $T$, (because $I$ is not a linear function of its argument) and therefore cannot be solved analytically

\subsection*{Translation}
We will start with a very simple transformation group: the translation.\\\\
Consider $T$ as a translation in $\R^d$ with $[T \cdot I](x) = I(x - T)$.  (The $\cdot$ is a group action.)  The gradient is
\[\dd f(T) = -2 \int [IU(x - T) - J] \dd I(x - T) \dd x\]
\begin{itemize}
    \item $f$ is integral square error
    \item $I(x - T) - J$ is the error term.
    \item $\dd I$ is the gradient of the image with respect to space
    \item $I(x - T)$ is the transformed images, not the original
\end{itemize}

\subsubsection*{Proof}
Consider a curve $\gamma(t) = T + t\delta T$ for $\delta T$ an arbitrary translation, and consider the velocity
\[\frac{\dd}{\dd t} f(T + t\delta t) \bigg|_{t = 0} = v_{\gamma, T}(f)\]
Plugging in our definition of $f$ gives:
\begin{align*}
    \frac{\dd}{\dd t} &\int (I(x - T - t\delta T) - J(x))^2 \dd x \bigg|_{t = 0}\\
    \intertext{By the chain rule,}
    &= \int 2(I(x - T) - J(x)) \dd I(x - T)(-\delta T)\\
    &= [-2 \int(I(x - T) - J(x)) \dd I(x - T) \dd x] \delta T
\end{align*}
\begin{itemize}
	\item $T$ is a vector
	\item everything before the $\dd x$ is the direction, or $\dd f(T)$.
\end{itemize}

\subsection*{Affine Group Transformations}
The gradient covector, as a $(d + 1) \times (d + 1)$ matrix, with bottom row all zeros, is given by
\[\dd f(T) = -2 \int[I(T^{-1} x) - J(x)] \dd[I(T^{-1} x)] (T^{-1} x)^T \dd x\]
This can be written in a matrix as:
\[\begin{pmatrix} L & T \\ 0 & 1 \end{pmatrix}\]
where
\begin{itemize}
	\item $L$ is linear
	\item $T$ is translation
	\item bottom row is fixed (homogeneous coordinates)
	\item $[I(T^{-1} x) - J(x)]$ is the error term, transformed $I$ minus $J$.
	\item $\dd[I(T^{-1}x)]$ is the gradient of the transformed $I$.
\end{itemize}

To prove this equation, we first need to define some other things:
\subsubsection*{Part 1: Derivative of inverse matrix}
\[\frac{\dd}{\dd t} (T + t \delta T)^{-1} = -T^{-1} \delta TT^{-1}\]
where
\begin{itemize}
	\item $\delta T$ is direction
	\item $t$ is time
	\item This is very well defined if $t$ is small, and $\delta T$ has zeros on its bottom row.
\end{itemize}

The derivative of a matrix with respect to another matrix is multiplying the inverse matrix on both sides of the direction.  In this case, it is the $T^{-1} \delta T T^{-1}$ section.  We can show this because
\[I = (T + t \delta T)(T + t\delta T)^{-1}\]
Now, we take the derivative of both sides:
\begin{align*}
    0 = \frac{\dd}{\dd t} I &= \frac{\dd}{\dd t} (T + t \delta T)(T + t\delta T)^{-1}\\
    \intertext{By product rule,}
    &= \delta T(T + t\delta T)^{-1} + (T + t \delta T) \frac{\dd}{\dd t} (T + t \delta T)^{-1} \bigg|_{t = 0}
    \intertext{When we evaluate at $t = 0$, a lot of things disappear}
    &= \delta T T + T \frac{\dd}{\dd t}(T + t\ delta T)^{-1} \bigg|_{t = 0}\\
    -\delta T T^{-1} &= T \frac{\dd}{\dd t} (T + t\ delta T)^{-1} \bigg|_{t = 0}\\
    -T^{-1} \delta T T^{-1} &= \frac{\dd}{\dd t} (T + t \delta T)^{-1} \bigg|_{t = 0}
\end{align*}
If big $T$ were a scalar, this is equivalent to the "quotient rule" for taking derivatives

\subsubsection*{Part 2: Derivative of image with affine}
\[\dd I(T^{-1} x) T^{-1} = \dd[I(T^{-1}x)]\]
\begin{itemize}
	\item Left side: gradient of image, transformed
	\item Right side: gradient of the transformed image
\end{itemize}
\textbf{Proof:}\\\\
Start with the right side and apply the chain rule.
\begin{align*}
    \dd[I(T^{-1} x)] &= \dd I \bigg|_{T^{-1}x} \cdot \frac{\dd}{\dd x}T^{-1} x\\
    &= \dd I\bigg|_{T^{-1}x} \cdot T^{-1}\\
    &= \dd I[T^{-1}x] T^{-1}
\end{align*}
Note: $\dd I \doteq [\dd I] \neq [\dd][I]$

\subsubsection*{Part 3: The gradient}
Find the directional derivative
\[\frac{\dd}{\dd t} \int [I((T + t\delta T)x) - J(x)]^2 \dd x \bigg|_{t = 0}\]
\begin{align*}
    &= \int 2[I((T + t \delta T)_{x}) - J(x)] \cdot \dd I((T + t \delta T)^{-1}x) \cdot (-T^{-1} \delta T T^{-1})x \dd x\\
    &= \int 2[I(T^{-1}x) - J(x)] \dd I(T^{-1}x) T^{-1} \delta T T^{-1} x \dd x\\
    &= \int -2[I(T^{-1}x) - J(x)] \dd [I(T^{-1}x)] \delta T T^{-1} x \dd x
\end{align*}
At this point, we would like to isolate $\delta T$, but it doesn't factor nicely because these are matrices.  Therefore, we will work with the trace of the matrix.\\\\
Since the whole quantity is scalar, we can take the trace:
\begin{align*}
    &= -2 \int (I(T^{-1}x) - J(x)) \text{tr} \left(\dd[I(T^{-1} x)] \delta T T^{-1} x\right) \dd x
    \intertext{With the trace, we have the cyclic permutation property, so we can rearrange the terms.}
    &= \cdots \text{tr} \left(\delta T T^{-1} x \dd [I(T^{-1} x)]\right)\\
    &= \cdots \text{tr} \left(\dd(I(T^{-1}x))^T (T^{-1}x)^T \delta T^T\right)\\
    &= \det (\dd(I(T^{-1}x)) (T^{-1} x)^T, \delta T)\\
    &[FILL]\\
    &= \text{tr} \left(-2 \int (I(T^{-1} x) - J(x)) \dd(I(T^{-1}x))^T x^T (T^{-1})^T \dd x \cdot \delta T^T\right)
\end{align*}
Notice that at this point, we have factored the equation into a vector times a covector, and therefore the group action on the derivative.  This completes the proof.

\subsection*{Metrics for Image Registration}
One simple metric is to choose every voxel as a point, and use our pull back metric for point sets.  (Use what we already do for point sets.  [FILL])

\subsection*{Automatic Differentiation}
Automatic differentiation works by defining a computation graph with data as edges, and functions as nodes.\\\\
For every function ($f(x)$), we define push forward ($f_*(x)[X]$), and a pull back ($f^*(x)[\chi]$).  The former tells us how a perturbation of inputs affects outputs.  The latter tells us how gradients should be pulled back (chain rule) for optimization.
\begin{itemize}
	\item The push forward represents the forward pass (calculating the gradients)
	\item The pull back is how the nodes should be modified through backpropagation on the gradient to adjust parameters.
	\item In some applications, the push forward isn't needed sometime.
\end{itemize}

\subsubsection*{Example: (Computation graph)}
Build a computation graph for the function:
\[f(x) = \cos(ax) + \sin(x^2)\]
Forward propagation in black, backpropagation in red.
\begin{itemize}
	\item When you go backwards, you apply the derivative of each function.
\end{itemize}
\begin{center} 
	\includegraphics*[width=\textwidth]{W8_1.png} 
\end{center}
\begin{itemize}
	\item pull back, multiply by transpose of Jacobian, often can be done without computing or storing Jacobian matrix.
\end{itemize}

\section*{Riemannian Manifolds and Geodesics}
\begin{itemize}
	\item A Riemannian manifold is one with a metric defined at every point
	\item Geodesics is the shortest path between two points
\end{itemize}

\subsection*{Motivation}
\begin{itemize}
	\item Here we consider the implications of putting an inner product on a manifold.
	\item This will allow us to measure lengths and angles, and define straight lines.
	\item These operations will allow us to compute a distance between any pair of points, which can serve as an input to many machine learning algorithms.
	\item These operations will extend the definitions of a lot of familiar data processing techniques: filtering, averaging, regression 
\end{itemize}

\subsection*{Riemannian Manifolds}
\begin{itemize}
	\item A Riemannian manifold is a smooth manifold with a (0, 2) tensor field that describes an inner product at every point
	\item This is usually denoted by the symbol $g_p$ for a metric tensor at the point $p$.  It is a nonlinear map $T_p \mathcal{M} \times T_p \mathcal{M} \rightarrow \R$.
	\item [FILL]
\end{itemize}

\subsection*{The length of a curve}
Given a curve $\gamma \::\: [0, 1] \rightarrow \mathcal{M}$, its length is given by
\[L(\gamma) = \int_0^1 \sqrt{g_{\gamma(t)}(v_{\gamma, \gamma(t)}, v_{\gamma, \gamma(t)})} \dd t\]
This corresponds to the familiar definition "distance equals speed times time", but note the difference between speed and velocity.\\\\
Velocity is more fundamental, whereas speed requires us to add additional structural to our manifolds.

\subsection*{Distances and Geodesics}
We define the distance between two points on a manifold as the length of the shortest curve that connects them.
\[d(p, q) = \min_{\gamma\::\: [0, 1] \rightarrow \mathcal{M}\\ \gamma(0) = p, \gamma(1) = q} L(\gamma)\]
These length minimizing curves are called geodesics.\\\\
"Geodesic" has two definitions.
\begin{itemize}
	\item the shortest path between two points
	\item a stationary point in the above optimization problem
\end{itemize}
Most of the time, they coincide, but consider the following:
\begin{center} 
	\includegraphics*[scale=0.8]{W8_2.png} 
\end{center}
The red path meets the second definition, but not the first.

\subsection*{Action Integrals}
Because there are an infinite number of parameterizations of the same curve $\gamma$, we choose to work with a constant speed geodesics.  These are minimizers of the action integral
\[A(\gamma) = \int_0^1 g_{\gamma(t)} (v_{\gamma, \gamma(t)}, v_{\gamma, \gamma(t)}) \dd t\]
In a coordinate chart $x$, with $x(\gamma(t)) \doteq q(t)$ and $\dot{\gamma}_{(x)}(t) \doteq \dot{q}(t)$, and $g_{x^{-1} (q(t))} \left(\frac{\partial}{\partial x^i}, \frac{\partial}{\partial x^j}\right) \doteq g_{ij}(q(t))$, we have:
\[A(q) = \int_0^1 g_{ij} (q(t)) \dot{q}^i(t) \dot{q}^j(t) \dd t\]

\subsection*{Constant Speed Geodesics}
With fixed endpoints minimizers of $A$ are constant speed, and are also minimizers of $L$.
\subsubsection*{Proof:}
Let $f(t) = \sqrt{g_{\gamma(t)}}\dots$ [FILL]
[FILL]

Now, consider another function $h(t)$ (arbitrary), and consider the $L_2$ inner product $\int_0^1 f(t) h(t) \dd t$.  By Cauchy-Schwartz, we have
\[\left(\int_0^1 f(t) h(t) \dd t\right)^2 \leq \int |f|^2(t) \dd t \int |h|^2 (t) \dd t\]
Choosing $h(t) = 1$ gives
\[\left(\int_0^1 f(t) \dd t\right)^2 \leq \int_0^1 |f|^2(t) \dd t \cdot 1\]
Notice that this is the action integral.  This implies that $L^2(\gamma) \leq A(\gamma)$.
If $\gamma$ has a constant speed $c$, then the left and right side are equal to $c^2$, and the inequality becomes an equality ($f, h$ are colinear).\\\\
So $A$ achieves the lower bound of $L$ over reparameterizations.
\begin{itemize}
	\item $L$ does not change when we reparameterize $\gamma$.
	\item $A$ obtains its smallest value when it is constant speed \underline{and} when $A$ is using a [FILL]
	\item Suppose for the purpose of contradiction that $\gamma$ is a minimizer of $A$ but not a minimizer of $L$.  Then there is another curve $\alpha$ with constant speed reparameterization $\tilde{\alpha}$ such that
	\[L(\alpha) < L(\gamma)\]
    \item But the left side is $\sqrt{A(\tilde{\alpha})}$ and the right side is $\sqrt{A(\gamma)}$, meaning $\gamma$ is not a minimizer of $A$.  We therefore reject our assumption.
\end{itemize}

\subsection*{The constant speed geodesic equation}
In a given coordinate chart, with components of a metric tensor field $g$ written as $g_{ij}$, and its inverse written $g^{ij}$, constant speed geodesics are determined by
\[\ddot{q^i} + \frac{1}{2} g^{ij} (-\partial_j g_{kl}) [FILL]\]


\end{document}