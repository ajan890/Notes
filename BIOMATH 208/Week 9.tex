% document formatting
\documentclass[10pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[left=1in,right=1in,top=1in,bottom=1in]{geometry}
\usepackage[T1]{fontenc}
\usepackage{xcolor}

% math symbols, etc.
\usepackage{amsmath, amsfonts, amssymb, amsthm}
\usepackage{dsfont}

% lists
\usepackage{enumerate}

% images
\usepackage{graphicx} % for images

% code blocks
\usepackage{minted, listings} 

% verbatim greek
\usepackage{alphabeta}

\graphicspath{{./assets/images}}

\newcommand{\solution}{\textbf{Solution:}} 
\newcommand{\example}{\textbf{Example: }}
\newcommand{\R}{\mathbb{R}}
\newcommand{\dd}{\text{d}}

\title{BIOMATH 208 Week 9}

\author{Aidan Jan}
\date{\today}

\begin{document}
\maketitle
\section{Review}
[FILL]

\subsubsection*{Image Registration}
\begin{itemize}
	\item Minimize the sum of square error:
	\[\text{SSE} (T \cdot I, J)\]
    \item $T$ is a translator or affine transformation
    \item For $T$ translator:
    \[\dd f(T) = \int(I(x - T) - J(x)) \cdot \dd I(x - T) \dd x\]
    \item For affine:
    \[\dd f(T) = \int(I(T^{-1}x) - J(x)) \cdot \dd[I(T^{-1}x)]^T \cdot [T^{-1} x]^T \dd x\]
\end{itemize}

\subsubsection*{Metric Manifolds (Riemannian Manifolds)}
[FILL manifold drawing]
\begin{align*}
    L(\gamma) &= \int_0^1 g_{\gamma(t)} (\dot{\gamma} (t), \dot{\gamma}(t)) \dd t\\
    A(\gamma) &= \int_0^1 \text{no square root} \dd t
\end{align*}
\begin{itemize}
	\item Any minimizer of $A$ is also a minimizer of $L$
	\item Any minimizer of $A$ is constant speed
	\item Optimizing over $A$ makes things easier (no square root) and gives a unique parameterization: constant speed geodesics
\end{itemize}

\subsection*{The Constant Speed Geodesic Equation}
In a given coordinate chart, with components of a metric tensor field $g$ written as $g_{ij}$, and its inverse written $g^{ij}$, constant speed geodesics are determined by
\[\ddot{q}^i + \frac{1}{2} g^{ij} (-\partial_j g_{kl} + \partial_k g_{jl} + \partial_l g_{kj}) \dot{q}^k \dot{q}^l = 0\]

\subsubsection*{Proof}
\begin{itemize}
	\item We will consider a path $q(t)$ and a perturbation $q \mapsto q + \epsilon \delta q$.
	\item This denotes a corresponding perturbation in the components of velocity $\dot{q} \mapsto \dot{q} + \epsilon \delta \dot{q}$ (with $\delta \dot{q} = \frac{\dd}{\dd t} \delta q$).
	\item We seek to find stationary solutions via
	\[\frac{\dd}{\dd \epsilon} A(q + \epsilon \delta q) \bigg|_{\epsilon = 0}\]
    \begin{itemize}
        \item e.g., find the directional derivative in the direction of $\delta q$.
    \end{itemize}
\end{itemize}
First, we plug into our equation
\begin{align*}
    &= \frac{\dd}{\dd \epsilon} A(q + \epsilon \delta q) \bigg|_{\epsilon = 0}\\
    \intertext{We'll apply chain rule and get derivatives of $g$}
    &= \frac{\dd}{\dd \epsilon} \int_0^1 g_{ij} (q(t) + \epsilon \delta q(t)) (\dot{q}^i(t) + \epsilon \delta \dot{q}^i(t)) (q^{ij}(t) + \epsilon \delta \dot{q}^{j}(t))
    \intertext{Notice that the last two terms are quadratic in velocity.}
    \intertext{Not trivial: We will apply integration by parts to write things as gradient dot direction for direction = $\delta q$.  e.g., $\delta q(0) = \delta q(1) = 0$ (fixed endpoints).  The boundary terms are zero in integration by parts.}
    &= \int \partial_k g_{ij} (q(t)) \delta g^k(t) \dot{q}^i(t) \dot{q}^j (t) + g_{ij}(q(t)) \delta \dot{q}^i(t) \dot{q}^j(t) + g_{ij} (g(t)) \dot{q}^i(t) \delta \dot{q}^j (t) \dd t
    \intertext{The first term is something acting linearly on the direction.  The next two terms are not, since $\delta \dot{q}^i(t)$ is the time derivative of the direction.}
    &= \int_0^1 \partial_k g_{ij} \dot{q}i (t) \dot{q}^j(t) \delta q^k(t) - \frac{\dd}{\dd t} (g_{ij}(q)\dot{q}^j) \delta q^i - \frac{\dd}{\dd t} (g_{ij}(q) \dot{q}^i) \delta q^j \dd t
    \intertext{Note that $\delta q$ has 3 different indices, but they are just dummy variables}
    &= \int_0^1 \partial_k g_{ij} \dot{q}^i \dot{q}^j \delta q^k - \frac{\dd}{\dd t} (g_{kj}(q) \dot{q}^j) \delta q^k - \frac{\dd}{\dd t} (g_{ik}(q) \dot{q}^i) \delta q^k \dd t\\
    &= \int partial_k g_{ij} (q) \dot{q}^i \dot{q}^j \delta q^k - \partial_i g_{kj}(q) \dot{q}^i \dot{q}^j \delta q^k - g_{kj} (q) \ddot{q} + \delta q^k \\
    &\hspace{0.5cm}- \partial_j g_{ik}(q) \dot{q}^j \dot{q}^i \delta q^k - g_{ik}(q) \dot{q}^i \delta \dot{q}^k \dd t
    \intertext{Now we simplify the equation.  We want to simplify to the form}
    \int_0^1 \text{something} (t)^k \cdot q^k (t) \dd t = 0
    \intertext{Since every term has a $q^k$, we can factor that out.}
    0 &= \partial_k g_{ij} (q) \dot{q}^i \dot{q}^j - \partial_i g_{kj} (q) \dot{q}^i \ddot{q}^j - g_{kj} (q) \ddot{q}^j - \partial_j g_{ik}(g) \dot{q}^j \dot{q}^k - g_{ik}(q) \ddot{q}^i
    \intertext{Now, we can (1) combine the pairs of items that look the same, and (2) act with the matrix inverse of $g$ (sharp map.)}
    0 &= g_{ik}(q) \ddot{g}^i + \frac{1}{2}(-\partial_k g_{ij}(q) + \partial_i g_{kj} (q) + \partial_j g_{ik}(q)) \dot{q}^i \dot{q}^j\\
    0 &= \ddot{q}^l + \frac{1}{2} (g^{-1})^{kl} (-\partial_k g_{ij}(q) + \partial_i g_{kj}(q) + \partial_j g_{ik}(q)) \dot{q}^i \dot{q}^j
\end{align*}
[FILL]

\subsection*{Christoffel Symbols}
We simplify by introducing the notation $\ddot{q}^k + \Gamma_{ij}^k \dot{q}^i \dot{q}^j = 0$.
\textbf{Definition: Christoffel Symbol of the first kind}
\[\Delta_{kij} = \frac{1}{2} (-\partial_k g_{ij} + \partial_i g_{kj} + \partial_j g_{ik})\]
\textbf{Definition: Christoffel symbol of the second kind}
\begin{align*}
    \Gamma_{ij}^l = g^{lk} \Gamma_{kij}\\
    &= \frac{1}{2} g^{lk} (-\partial_k g_{ij} + \partial_i g_{kj} + \partial_j g_{ik})
\end{align*}
Note they are symmetric in the last two indices.

\subsection*{Geodesics in Euclidean Space}
If $g$ is constant everywhere in some chart, then the resulting geodesics are linear equations in this chart
\[q^i(t) = a^i t + b^i\]
\subsubsection*{Proof:}
If $g$ is constant then $\partial_k g_{ij} = 0$ for all $i, j, k$.  The equation becomes
\[\ddot{q}^i = 0\]
Integrating once gives
\[\dot{q}^i = a^i\]
for some $a$.  Integrating again gives the solution for arbitrary $b$.

\subsection*{Geodesics in polar coordinates}
Earlier we showed that the Euclidean dot product in polar coordinates is
\[g_{ij} (r, \theta) = \begin{pmatrix} 1 & 0 \\ 0 & r^2 \end{pmatrix}_{ij}\]
Geodesics in this coordinate system are given by
\begin{align*}
    \ddot{r} - r \dot{\theta}^2 &= 0\\
    \ddot{\theta} + \frac{2}{r} \dot{r} \dot{\theta} &= 0
\end{align*}

\subsubsection*{Proof}
[FILL]

Now, start calculating partial derivatives.  Note they are only nonzero when we take the derivative of the $\theta - \theta$ component ($i = 1, j = 1$), with respect to $r$.
\begin{align*}
    [FILL]
\end{align*}

Now Christoffel symbols of the first kind.
\begin{align*}
    \Gamma_{kij} &= -\frac{1}{2}(-\partial_k g_{ij} + \partial_i g_{kj} + \partial_j g_{ik})\\
    \Gamma_{000} &= 0\\
    \Gamma_{010} &= 0\\
    \Gamma_{100} &= 0\\
    \Gamma_{110} &= \Gamma_{101} = r
    \Gamma_{001} &= [FILL]
    \Gamma_{011} &= 
    \Gamma_{101} &=
    \Gamma_{111} &=
\end{align*}

Now, Christoffel symbols of the second kind
\begin{align*}
    \Gamma_{ij}^l &= g^{lk} \Gamma_{kij}\\ 
    \Gamma_{00}^0 = g^{00} \Gamma_{000} + g^{01} \Gamma_{100} &= 0 \\ 
    \Gamma_{01}^0 = g^{00} \Gamma_{001} + g^{01} \Gamma_{101} &= 0 \\
    \Gamma_{10}^0 = g^{00} \Gamma_{001} + g^{01} \Gamma_{101} &= 0 \\
    \Gamma_{10}^0 = g^{00} \Gamma_{010} + g^{01} \Gamma_{110} &= -r \\
    \Gamma_{00}^1 = g^{10} \Gamma_{000} + g^{11} \Gamma_{100} &= 0 \\
    \Gamma_{01}^1 = g^{10} \Gamma_{001} + g^{11} \Gamma_{101} &= y_r \\
    \Gamma_{10}^1 = g^{10} \Gamma_{001} + g^{11} \Gamma_{101} &= y_r \\
    \Gamma_{10}^1 = g^{10} \Gamma_{010} + g^{11} \Gamma_{110} &= 0
\end{align*}

An example of geodesic curves are shown below:
\begin{center} 
	\includegraphics*[width=\textwidth]{W9_1.png} 
\end{center}

\subsection*{Christoffel Symbols are not Tensors}
Why?  Because a tensor that is 0 in one chart induced basis will be 0 in all others.
\begin{itemize}
	\item Change of basis involved multiplying by Jacobians, these are always invertible matrices (smoothly compatible atlas).
\end{itemize}

\subsection*{Size Data}
Size data is very common in medical imaging, where it is often called volumetry or morphometry.  It gives a means to quantify data in structural images.

\textbf{Geodesics on size}
Consider $\mathcal{M}$ the space of sizes (or the scale group), with the "natural" coordinate chart (i.e., a number in $\R^+$).  Let $g(q) = \frac{1}{q^2}$, and so $g^{-1}(q) = q^2$.  Note this metric is left invariant.  To see this, just push forward vectors with $q^{-1}$.
\[g(q)(u, v) = \frac{1}{q^2}uv = g(I) (\frac{1}{q}u, \frac{1}{q}v)\]
\begin{itemize}
	\item e.g., we push forward, back to identity, with $q^{-1}$.
\end{itemize}
For $a, b \in \R$, geodesics are given by
\[q(t) = \exp(at + b)\]

\subsubsection*{Proof}
First calculate $g^{-1}$
\[\left(\frac{1}{q^2}^{-1}\right) = q^{2} \hspace{1cm} \rightarrow \text{ sharp map}\]
Now, calculate the derivative of $g$.
\[\partial_0 g_{00} = \frac{\dd}{\dd q} \frac{1}{q^2} = -2q^{-3}\]
Now calculate the Christoffel symbol of the first kind.
\[\Gamma_{000} = \frac{1}{2} (-2 q^{-3})\]
Note, in 1D, two of the terms cancel out because of a minus sign.\\\\
Now,calculate the Christoffel symbol of the second kind.
[FILL]

Write the geodesic equation:
\[\ddot{q} - \frac{1}{q} \dot{q}^2 = 0\]
Notice that the coefficients are the Christoffel symbols.\\\\
Show exponentials are a solution:
\begin{align*}
    q(t) &= \exp(at + b)\\
    \dot{q}(t) &= a \exp(at + b)\\
    \ddot{q}(t) &= a^2 \exp(at + b)\\
\end{align*}
Plugging in,
\[a^2 \exp(at + b) - \frac{1}{\exp(at + b)} (a \exp(at + b))^2 = 0\]

\subsection*{Distances on Size Data}
The Riemannian distance between two points $0 < a < b$ is given by the log of their ration
\[d(a, b) = \log(b/a) = \log(b) - \log(a)\]
Note the distance between $sa$ and $sb$ is the same as that between $a$ and $b$ (left invariance).

\subsubsection*{Proof}
Let $q(t)$ be any increasing function with $q(0) = a$ and $q(1) = b$.  Then,
\begin{align*}
    d(a, b) = \int_0^1 \sqrt{\frac{\dot{q}^2(t)}{q^2(t)}} \dd t\\
    [FILL]
\end{align*}

\subsection*{Normal coordinates on sizes}
We chose a base of point 1, and use a chart where
\[x(p) = d(1, p) \text{sign}(p - 1) = \log(p)\]
This chart has the property that the Euclidean distance from $1$ to $p$ in the chart, is the Riemnanian distance on the manifold.\\\\
This property defines "normal coordinates" on a manifold.

\subsection*{Probability data}
Probability data shows up in image segmentation/classification tasks, where an image or a pixel is assigned to a given category with some probability (e.g., foreground vs. background)
\begin{itemize}
	\item Consider $\mathcal{M}$ the space of probabilities, with the "natural" coordinate chart (i.e., a number in (0, 1)).  Let's choose the metric tensor $g(q) = \frac{1}{q^2 (1-q)^2}$ which says that the parameter changes near the endpoints are larger than those near the middle.  Note, $g$ is invariant under $q \mapsto 1 - q$.\\\\
	\item For $a, b \in \mathcal{R}$, geodesics are given by the logistic function (sigmoid)
	\[q(t) = \frac{1}{1 + \exp(at + b)}\]
\end{itemize}

\section*{Review}
\underline{Geodesics}: Shortest path between two endpoints on a Riemannian manifold (metric tensor at every $p \in \mathcal{M}$)
[FILL drawing/equations]
\[A[\gamma] = \int_0^1 \text{no square root} \dd t\]
Minimizers of $A$ also minimize $L_1$ and are constant speed.  We optimized the action integral by taking a directional derivative (in the space of curves), and setting the result to 0.\\\\
For a curve $g(t)$ in a chart, $\Rightarrow \ddot{q}^i(t) + \Gamma_i$ [FILL]

\subsubsection*{Examples of Geodesics}
\begin{itemize}
	\item Euclidean space, linear functions of time
	\item Polar coordinates, it was hard, but we did get the same answer.
	\item Scale group, left invariant metric $g_q(x, y) = \frac{1}{q^2} xy$
	\item A solution:
    \[q(t) = \exp(at + b)\]
    \begin{itemize}
        \item $a$ and $b$ describe initial velocity and position (although they are not exactly initial velocity and position)
    \end{itemize}
    \[\text{distance}(p, q) = \log(q/p) \hspace{1cm}\{q > p\}\]
\end{itemize}

\subsection*{Probability data}
Probability data shows up in image segmentation/classification tasks, where an image or a pixel is assigned a to a given category with some probability (e.g. foreground versus background).\\\\
Consider $\mathcal{M}$ in the space of probabilities, with the "natural" coordinate chart (i.e., a number in (0, 1)).  Let's choose the metric tensor $g(q) = \frac{1}{q^2 (1 - q)^2}$, which says that marameter changes near the endpoints are larger than those near the middle.  Note, $g$ is invariant under $q \mapsto 1 - q$.\\\\
For $a, b \in \mathcal{R}$, geodesics are given by the logistic function (sigmoid)
\[q(t) = \frac{1}{1 + \exp(at + b)}\]

\subsubsection*{Proof:}
[FILL]

\subsection*{Distances on Probabilities}
For $a < b \in (0, 1)$, the distance between two probabilities is the difference between their log of odds
\[d(a, b) = \log\left(\frac{b}{1 - b}\right) - \log\left(\frac{a}{1 - a}\right)\]
\begin{itemize}
	\item Each term here is called a "log of odds", aka a "logit"
\end{itemize}

\subsubsection*{Proof:}
Note $\log(q / (1 - q))$ is the inverse of the logistic function (function inverse, not matrix inverse).  Let $q(t) = 1 / (1 + \exp(ct + d))$ (for $c < 0$).\\\\
Consider the sigmoid curve:
\[d(a, b) = \int \sqrt{\frac{\dot{q}^2(t)}{q(t) (1 - q(t))}}\dd t\]
If we define:
\begin{align*}
    \dot{q} &= cq(1 - q)\\
    \dot{q}^2 &= c^2 q^2 (1 - q^2)
\end{align*}
then, we can simplify:
\begin{align*}
    &= \int_0^1 \sqrt{c^2} \dd t\\
    &= \int_0^1 c \dd t
\end{align*}
Note that $c$ must be negative for an increasing curve.  (Take the correct sign when doing a square root)\\\\
Now, work out $c$ as a function of $a$ and $b$.\\\\
\begin{itemize}
	\item If $t = 0$, then $q(0) = \frac{1}{1 + \exp(d)} = a \Rightarrow d = \log(a / 1 - q)$.
	\item If $t = 1$, then $q(t) = \frac{1}{1 + \exp(c + d)} = b$
\end{itemize}
Therefore,
\begin{align*}
    c + d &= \log\left(\frac{b}{1 - b}\right)\\
    -c &= d - (c + d) = \log\left(\frac{b}{1 - b}\right) - \left(\frac{a}{1 - a}\right)
\end{align*}

\subsection*{Normal Coordinates on Probabilities}
Choose a base point of 0.5 and write
\[x(p) = \text{sign}(p - 0.5)\dd (0.5, p) = \log\left(\frac{p}{1 - p}\right)\]
These are normal coordinates for this 1D manifold.  A statistics, this is called the "canonical parameterization" (of a Bernoulli random variable)
\begin{itemize}
	\item In normal coordinates, every straight line moving away from the base point is a constant speed geodesic
\end{itemize}

\subsection*{Positive Definite Symmetric Matrix Data}
Diffusion tensor imaging outputs a positive definite symmetric matrix at each pixel.  This helps us understand local orientation of tissue microstructure, which is crucial for identifying white matter tracts in the brain, or fiber orientations in muscle such as heart.\\\\
With $A$ a square matrix, we denote its exponentail as $\exp(A)$.  It can be defined using a Taylor series
\[\exp(A) = \sum_{n = 0}^\infty \frac{1}{n!} A^n\]
or with a "compound interest formula"
\[\exp(A) = \lim_{n \rightarrow \infty} \left(I + \frac{1}{n}A\right)^n\]

\subsection*{Properties of the Matrix Exponential}
\begin{enumerate}
    \item $\exp(A)A = A \exp(A)$
    \item $\frac{\dd}{\dd t} \exp(At) = A \exp(At) = \exp(At)A$
    \item If $A$ and $B$ commute, then 
    \[\exp(A + B) = \exp(A)\exp(B) = \exp(B)\exp(A)\]
\end{enumerate}

\subsection*{Positive Definite Symmetric Matrix Data}
The matrix logarithm is the inverse of the matrix exponential, i.e., $\exp(\log(A)) = A$.  It can also be defined by a Taylor series
\[\log(I + A) = \sum_{n = 1}^\infty (-1)^{n + 1} \frac{1}{N} A^n\]

\subsection*{Geodesics on Positive Definite Symmetric Matrices}
Consider $\mathcal{M}$ the set of positive definite symmetric matrices, with the metric
\begin{align*}
    g_A(U, V) &= \text{tr} [(A^{-\frac{1}{2}}UA^{-\frac{1}{2}})(A^{-\frac{1}{2}}VA^{-\frac{1}{2}})]\\
    &= \text{tr}[(A^{-1}U)(A^{-1}V)]
\end{align*}
Here $U, V$ are symmetric, but not necessarily positive definite.  Geodesics take the form $A(t) = b^{\frac{1}{2}} \exp(at) b^{\frac{1}{2}}$, where $a$ is symmetric and $b$ is symmetric positive definite.

\subsubsection*{Proof}
We will work in \underline{extrinsic coordinates}, and find stationary paths of the action integral directly.  Consider a path $A(t)$, perturbed to $A(t) + \epsilon \delta A(t)$.  Here, $\delta A$ is symmetric, with $\delta A(0) = \delta A(1) = 0$.  Take the derivative of our action integral
\begin{align*}
    &\frac{\dd}{\dd \epsilon}\int_0^1 \text{tr} \left[(A + \epsilon \delta A)^{-1} (\dot{A} + \epsilon \delta \dot{A})(A + \epsilon \delta A)^{-1} (\dot{A} + \epsilon \delta \dot{A})\right] \dd t \bigg|_{\epsilon = 0}\\
    &= \int \text{tr} \left[-A^{-1} \delta AA^{-1} \dot{A} A^{-1} \dot{A}\right] + \text{tr} \left[A^{-1} \delta \dot{A} A^{-1} \dot{A}\right] + \text{tr}\left[-A^{-1} \dot{A} A^{-1} \delta AA^{-1} \dot{A}\right] + \text{tr} \left[A^{-1} \dot{A} A^{-1} \delta \dot{A}\right] \dd t\\
    &= \int_0^1 \partial \text{tr}[-A^{-1} \dot{A} A^{-1} \dot{A} A^{-1} \delta A + A^{-1} \dot{A} A^{-1} \delta \dot{A}] \dd t\\
    \intertext{By integration by parts:}
    &= \int_0^1 \partial \text{tr} \left[(-A^{-1} \dot{A} A^{-1} \dot{A} A^{-1} - \frac{\dd}{\dd t} A^{-1} \dot{A} A^{-1}) \delta A\right] \dd t\\
    &= \frac{\dd}{\dd t} \left(A^{-1} \dot{A} A^{-1}\right)\\
    &= -A^{-1} \dot{A} A^{-1} \dot{A} A^{-1} + A^{-1} \ddot{A} A^{-1} - A^{-1} \dot{A} A^{-1} \dot{A} A^{-1}\\
    \intertext{Now, we can rewrite the whole expression and set it to 0.}
    0 &= A^{-1} \ddot{A} A^{-1} - A^{-1} \dot{A} A^{-1} \dot{A} A^{-1}
    \intertext{We can multiply all the terms by $A^{-\frac{1}{2}}$ to further simplify:}
    0 &= A^{-\frac{1}{2}} \ddot{A} A^{-\frac{1}{2}} - (A^{-\frac{1}{2}} \dot{A} A^{-\frac{1}{2}})(A^{-\frac{1}{2}} \dot{A} A^{-\frac{1}{2}}) \\
    0 &= \ddot{A} - \dot{A} A^{-1} \dot{A}
\end{align*}
Now we show our solution satisfies this equation:
\[A(t) = b^{\frac{1}{2}} \exp(at) b^{\frac{1}{2}}\]
\begin{align*}
    \dot{A}(t) &= b^{\frac{1}{2}} a \exp(at) b^{\frac{1}{2}} - b^{\frac{1}{2}} a^{\frac{1}{2}} \exp(at) a^{\frac{1}{2}} b^{\frac{1}{2}}\\
    \ddot{A} &= b^{\frac{1}{2}} [FILL]
\end{align*}

\subsection*{Distances on Positive Definite Matrices}
If $A$ and $B$ are two positive definite symmetric matrices, then the Riemannian distance between them is
\[d(A, B) = \sqrt{\text{tr} \left[\log(A^{-\frac{1}{2}}BA^{-\frac{1}{2}}) \log(A^{-\frac{1}{2}}BA^{-\frac{1}{2}})\right]}\]

\subsubsection*{Proof}
Consider the path $C(t) = A^{\frac{1}{2}} \exp(\log(A^{-\frac{1}{2}}BA^{-\frac{1}{2}})t)$.  Note that
\[C(0) = [FILL]\]
and that
\[C(1) = [FILL]\]

The initial velocity is then
\[\dot{C}(0) = A^{\frac{1}{2}} \log(A^{-\frac{1}{2}}BA^{-\frac{1}{2}}) A^{\frac{1}{2}}\]
Its initial speed squared is then:
\begin{itemize}
	\item the velocity times $A^{-1}$, then product with itself, then trace.
\end{itemize}
\[\text{tr} \left[A^{-\frac{1}{2}} \dot{C}(0) A^{-\frac{1}{2}} A^{-\frac{1}{2}} \dot{C}(0)A^{-\frac{1}{2}}\right]\]
To find distance squared from $A$ to $B$, it is $\dot{C}(0)$.  Constant speed, integrating for 1 unit of time, then initial speed squared is equal to distance squared.

\subsection*{The Riemannian Exponential}
We have seen two examples where geodesics turn out to be exponentials.  This motivates the use of this language in other situations.  We define the Riemannian exponential as
\[\exp_p(v)\]
for $p \in \mathcal{M}$ and $v \in T_p\mathcal{M}$ to be the endpoint of a constant speed geodesic starting at $p$ with initial velocity $v$.  Note that the square distance between endpoints is given by
\[d^2(p, \exp_p(v)) = g_p(v, v)\]

\subsection*{Normal Coordinates}
We can use the matrix exponential to build a coordinate system.  Given a base point $p_0$, we chose a coordinate map
\[x^i(p) = \dd x^i (\log_{p_0}(p))\]
With this chart all straight lines through the origin are geodesics!
\begin{itemize}
	\item Note that $\log_{p_0}(p)$ is the inverse of the Riemannian exponential
\end{itemize}

\section*{Averaging, Filtering, and Regression}
\subsection*{Motivation}
\begin{itemize}
	\item Here we show how the Riemannian exponential and logarithm can be used to define averages by minimizing sum of square distance problems
	\item These averages will be used to define means, interpolation and extrapolation, and regression.
\end{itemize}

\subsection*{Sample Mean in Vector Spaces}
For $x_i$ a set of $N$ points in a vector space $V$, and using the definitions of plus and times, we define the sample mean
\[\bar{x} = \frac{1}{N} \sum_{i = 1}^N x_i\]
or the weighted sample mean
\[\bar{x}_w = \frac{\sum_{i = 1}^N w_i x_i}{\sum_{i = 1}^N w_i}\]
for weights $w_i \in \R^+$.  (Note subscript does not refer to covector components here).\\\\
\textbf{This is the common definition of 'mean' that was taught in high school.}

\subsection*{Locally weighted averages}
In imaging, locally weighted averages are used for processing such as denoising or antialiasing, for example
\[\bar{I}[i, j] = \frac{1}{5} (I[i, j] + I[i + 1, j] + I[i - 1, j] + I[i, j + 1] + I[i, j - 1])\]
This is a weighted average where weights are 0 for pixels away from $(i, j)$.  These types of local averages are usually called "filters".

\subsection*{Denoising}
Here is an example of the above filter applied to denoising
\begin{center} 
	\includegraphics*[width=\textwidth]{W9_2.png} 
\end{center}

\subsection*{Antialiasing}
Filtering before downsampling is critical, otherwise we will end up with noise aliasing.
\begin{center} 
	\includegraphics*[width=\textwidth]{W9_3.png} 
\end{center}

\subsection*{The Frechet Mean in Metric Spaces}
A metric space is one where we can define a distance between any two points.  An alternative definition is to find a point that minimizes the sum of square distances to our sample
\[\bar{x} = \arg \min_y \sum_{i = 1}^N w_i d^2(y, x_i)\]

\subsection*{The Frechet Mean in Vector Spaces}
If data lies in a vector space with the Euclidian metric, this definition is equivalent to the previous.

\subsubsection*{Proof.}
Search for a stationary point:
\begin{align*}
    &\frac{\dd}{\dd \epsilon}\sum_{i = 1}^N w_i |y + \epsilon \delta y - x_i|^2 \bigg|_{\epsilon = 0}\\
    &= \sum_{i = 1}^N w_i (y - x_i)^T \delta y\\
    \intertext{The part before the $\delta y$ acts linearly on the velocity to give the directional derivative}
    \intertext{Next, we find a stationary point by setting it to 0}
    &= \sum_{i = 1}^N w_i y = \sum_{i = 1}^N [FILL]
\end{align*}

\subsection*{Averages on Manifolds}
On a Riemannian Manifold we can't apply the first definition because we have no plus or times, but we can apply the second:
\textbf{Frechet Mean on Riemannian Manifolds:}\\\\
For $N$ samples, $p_i \in \mathcal{M}$, the Frechet mean is characterized by 
\[\sum_{i = 1}^N w_i \log_{\bar{p}_w} (p_i) = 0\]
That is, we find a point such that our samples are $0$ mean in normal coordinates.

\subsubsection*{Proof}
Let $f_i(p) = d^2(p, p_i)$, and $f(p) = \sum_{i = 1}^N f_i (p)$ and work out
\[\frac{\dd}{\dd \epsilon} f_i (p + \epsilon \delta p) \bigg|_{\epsilon = 0}\]
In a chart, let $q(t) + \epsilon \delta q(t)$ be a geodesic curve such that $q(1) = p_i$ and $q(0) = p + \epsilon \delta p$ (i.e., $\delta p = \delta q(0)$).  Note this curve has a fixed endpoint, but not a fixed start point.\\\\
Plug in the definition of distance
\[= \frac{\dd}{\dd \epsilon} \int_0^1 g_{ij} (q + \epsilon \delta q)(\dot{q}^i +\epsilon \delta \dot{q}^i) (\dot{q}^j + \epsilon \delta \dot{q}^j) \dd t \bigg|_{\epsilon = 0}\]
This is the same calculation we already did to derive the geodesic equation.  BUT there is an extra term that comes from integration by parts without fixed endpoints.  We can continue to simplify:
\begin{align*}
    &= \int_0^1 \partial_k g_{ij} (q) \delta q^k \dot{q}^i \dot{q}^j + g_{ij}(q) \delta \dot{q}^i \dot{q}^j + g_{ij}(q) \dot{q}^i \delta \dot{q}^j \dd t\\
    &= 0 - g_{ij} (q) \delta q^i(0) \dot{q}^j (0) - g_{ij} (q) q^i(0) \delta q^j(0)
\end{align*}
Since $\delta q(0) = \delta p$, and $\dot{q}(0) = \log_p(p_i)$, we must have [FILL].\\\\
Therefore,
\[\dd f(p) = [FILL]\]
and since the flat map is always invertible we recover our definition by setting the gradient to 0 for a stationary solution
\[\sum_{i = 1}^N \log_p(p_i) = 0\]

\subsection*{Interpolation by Weighted Averaging}
When we have only two datapoints, this equation is very easy to solve.  With $w_i \in [0, 1]$ and $w_0 = 1 - w_1$, this corresponds to linear interpolation between the two points.\\\\
When $w_1 = 1$ this will give $p_1$, when $w_1 = 0$ this will give $p_0$.  If $w_1$ is outside of [0, 1] this corresponds to extrapolation.\\\\
The weighted average interpolation between two points $p_0$ and $p_1$ is found by minimizing
\[(1 - w_1)d^2(\bar{p}_w, p_0) + w_1 d^2 (\bar{p}_w, p_1)\]
is given by a point a fraction $w_1$ along the geodesic from $p_0$ to $p_1$.

\subsubsection*{Proof}
With two points, the above solution gives
\[(1 - w_1) \log_{\bar{p}_w}(p_0) + w_1 \log_{\bar{p}_w} (p_1) = 0\]
This means the initial velocities are colinear and correspond to a single geodesic connecting $p_0$ to $p_1$.\\\\
Also, the distance from $\bar{p}_w$ to $p_0$ is $w_1 / (1 - w_1)$ times as long as the distance from $\bar{p}_w$ to $p_1$.  This is true when $\bar{p}_w$ is a fraction $w_1$ of the way from $p_0$ to $p_1$.

\subsection*{Weighted avverages in a chart will leave the manifold}
Here we show an interpolation between two 2D rotation matrices, through their action on an image.  Simple weighted averages in a chart leave the manifold.
\begin{center} 
	\includegraphics*[width=\textwidth]{W9_4.png} 
\end{center}

\subsection*{Interpolation between PDS Matrices}
If $A$ is PDS, then $x^T A^{-1} x = 1$ defines an ellipse(oid) to be visualized.  Note $A^{\frac{1}{2}} (\cos(\theta), \sin(\theta))^T$ is a solution for all $\theta$.
\begin{center} 
	\includegraphics*[width=0.7\textwidth]{W9_5.png} 
\end{center}


\end{document}