% document formatting
\documentclass[10pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[left=1in,right=1in,top=1in,bottom=1in]{geometry}
\usepackage[T1]{fontenc}
\usepackage{xcolor}

% math symbols, etc.
\usepackage{amsmath, amsfonts, amssymb, amsthm}
\usepackage{xcolor}
\usepackage[dvipsnames]{xcolor}

% lists
\usepackage{enumerate}

% images
\usepackage{graphicx} % for images

% code blocks
\usepackage{minted, listings} 

% verbatim greek
\usepackage{alphabeta}

\newcommand{\dd}{\text{d}}

\graphicspath{{./assets/images/Week 3}}

\title{CS 188 Robotics Week 3} 

\author{Aidan Jan}

\date{\today}

\begin{document}
\maketitle 

\section*{Model Predictive Control}
\begin{center} 
	\includegraphics*[width=\textwidth]{L1_1.png} 
\end{center}
\textbf{Model predictive control (MPC)} is an optimal control technique in which the calculated control actions minimize a cost function for a constrained dynamical system over a finite, receding, horizon.
\begin{itemize}
	\item Predicts future system behavior using a \textbf{model}
	\item Solves an \textbf{optimization} problem at each step.
	\item Applies only the first control input at each step (iterative).
	\item Repeats this process continuously (receding horizon).
	\item Handles input and output \textbf{constraints} directly.
\end{itemize}

\subsection*{System Identification}
\textbf{\dots building a mathematical model} of a dynamic system \textbf{from measured data}

\begin{center} 
	\includegraphics*[width=0.7\textwidth]{L1_2.png} 
\end{center}

\subsection*{Model Predictive Contouring Control (MPCC++)}
\begin{center} 
	\includegraphics*[width=0.9\textwidth]{L1_3.png} 
\end{center}

\section*{Cameras and 2D Perception}
\subsection*{Color Camera}
\begin{itemize}
	\item Cameras are the primary sensor for many robotic platforms
	\item One of the cheapest and richest sensors is a camera
	\item Many other sensors are built on top of the color camera
\end{itemize}

\subsection*{Images Representation}
\begin{itemize}
	\item An image is basically a 2D array of intensity/color values
	\item Image types:
	\begin{center} 
        \includegraphics*[width=0.9\textwidth]{L1_4.png} 
    \end{center}
\end{itemize}

\subsubsection*{Grayscale Images}
A grid (2D matrix) of intensity values:
\begin{center} 
	\includegraphics*[width=\textwidth]{L1_5.png} 
\end{center}
\begin{itemize}
	\item \underline{Pixel:} A "picture element" that contains the light intensity at some location $(x, y)$ in the image Referred to as $I(x, y)$
	\item \underline{Image Resolution:} expressed in terms of Width and Height of the image
\end{itemize}

\subsection*{Camera and Image Formation}
\begin{itemize}
	\item How we get the image (Image formation)
	\item Pinhole Camera Model
	\item 2D computer vision tasks and challenges
\end{itemize}

\subsubsection*{Image Formation}
\begin{center} 
	\includegraphics*[width=\textwidth]{L1_6.png} 
\end{center}

\subsubsection*{Pinhole Camera Model}
\textbf{Pinhole image:} Natural phenomenon, known during classical period in China and Greece (e.g., 470 BCE to 390 BCE).  
\begin{itemize}
    \item Used for \underline{art creation} and religious ceremony in the ancient times.
	\item Expensive to record the image (drawing)
\end{itemize}
\begin{center} 
	\includegraphics*[width=0.8\textwidth]{L1_7.png} \\
    \includegraphics*[width=0.8\textwidth]{L1_8.png} 
\end{center}
\begin{itemize}
	\item Light sensitive material was used as film.  
	\item Hard to store, loses color after a while
\end{itemize}
\textbf{Today:} photon sensors are CCD, CMOS, etc.

\subsubsection*{Human Vision}
\begin{center} 
	\includegraphics*[width=0.8\textwidth]{L1_9.png} 
\end{center}
\subsubsection*{Why do we need a pinhole?}
\begin{center} 
	\includegraphics*[width=\textwidth]{L1_10.png} 
\end{center}
Light rays from many different parts of the scene strike the same point on the paper.
\begin{center} 
	\includegraphics*[width=\textwidth]{L1_11.png} 
\end{center}
Each point on the image plane \underline{sees light from only one direction} - the one that passes through the pinhole.

\subsubsection*{Problem with Pinhole Camera}
The pinhole size:
\begin{itemize}
	\item If large, blurry
	\item If small, not enough light
	\item When the pinhole size is extremely small, we will see the diffraction effect through the pinhole, resulting in the blurry image
\end{itemize}
\textbf{Solution:} refraction (lenses)
\begin{itemize}
	\item Essentially add multiple pinhole images
	\item Shift them to align using the light \textbf{refraction}
	\item However, this alignment works only for \underline{one depth} (need the object and image plane to stay in focus.)
\end{itemize}
\begin{center} 
	\includegraphics*[width=\textwidth]{L1_12.png} 
\end{center}

\pagebreak
\subsubsection*{Lenses Issues (depth of field)}
Only objects on focus plane are in "perfect" focus
\begin{center} 
	\includegraphics*[width=\textwidth]{L1_13.png} 
\end{center}
\[\frac{1}{D} + \frac{1}{D'} = \frac{1}{f}\]
where $D$ is the distance of a focus plane to the lens plane, $D'$ is the distance of the image plane to the lens plane, and $f$ is the focal length of the lens.
\begin{itemize}
	\item Objects close to the focus plane are in better focus
	\item Objects further away are not.
\end{itemize}

\subsection*{Camera Terminology}
These terms will be defined below.
\begin{itemize}
	\item Focal length
	\item Field of view
	\item Aperture
	\item Camera intrinsic
	\item Camera extrinsic
\end{itemize}

\subsection*{Pinhole Camera Geometry}
Motivation
\begin{itemize}
	\item Physics of real cameras are all different (too tedious to model all of them).
	\item But they all try their best to approximate pinhole camera.
	\item So \textbf{in most of computer vision subjects, we model all cameras mathematically as a pinhole camera.}
\end{itemize}

\subsubsection*{Field of View (FOV)}
\begin{center} 
	\includegraphics*[width=0.9\textwidth]{L1_14.png} 
\end{center}
\[\alpha = 2 \arctan \frac{d}{2f}\]
\begin{itemize}
	\item The unit of FoV $\alpha$ is a degree.
	\item Each camera has two FoV: vertical and horizontal.
\end{itemize}

\subsubsection*{Focal Length}
\begin{center} 
	\includegraphics*[width=0.9\textwidth]{L1_15.png} 
\end{center}
\subsubsection*{Camera Projection}
\begin{center} 
	\includegraphics*[width=\textwidth]{L1_16.png} 
\end{center}
\begin{align*}
    \textcolor{Green}{\frac{u}{f}}\: & \textcolor{Green}{= \frac{X}{Z}}\\
    \textcolor{red}{\frac{v}{f}}\: & \textcolor{red}{= \frac{Y}{Z}}
\end{align*}
In camera coordinates, the camera center is the origin.
\[p_{2d} = \begin{bmatrix} u \\ v \end{bmatrix} \hspace{2cm} p_{3d} = \begin{bmatrix} X \\ Y \\ Z \end{bmatrix}\]
\begin{center} 
	\includegraphics*[width=\textwidth]{L1_17.png} \\
    \includegraphics*[width=\textwidth]{L1_18.png} 
\end{center}


\subsubsection*{Image Coordinate}
\begin{center} 
	\includegraphics*[width=\textwidth]{L1_19.png} 
\end{center}
Until now, we use 2D coordinate conventions that are \textbf{consistent} with the 3D camera coordinate.  However, if your application uses a different 2D coordiante, you'll need to further transform the $(u, v)$.\\\\
For example, consider the following cases where we change the direction of the axes and the position of the origin.
\begin{center} 
	\includegraphics*[width=\textwidth]{L1_20.png} \\
    \rule{\textwidth}{1pt}
    \vspace{1cm}
    \includegraphics*[width=\textwidth]{L1_21.png}
\end{center}

\subsubsection*{Popular Camera Coordinate Systems}
\begin{center} 
	\includegraphics*[width=\textwidth]{L1_22.png} 
\end{center}

\subsection*{Camera Projection:}
\[\lambda \begin{bmatrix} u \\ v \\ f \end{bmatrix} = \begin{bmatrix} X \\ Y \\ Z \end{bmatrix}\]
\begin{center} 
	\includegraphics*[width=\textwidth]{L1_23.png} 
\end{center}

\section*{Computer Vision}
A quick overview: what is computer vision?
\begin{center} 
	\includegraphics*[width=\textwidth]{L1_24.png} 
\end{center}

\subsection*{What Makes 2D Computer Vision Hard?}
\begin{center} 
	\includegraphics*[width=\textwidth]{L1_25.png} 
\end{center}
Variation: same cat, different poses, view points, \dots
\begin{center} 
	\includegraphics*[width=\textwidth]{L1_26.png} 
\end{center}
More variation: different cats, different shapes, colors, textures, \dots\\\\

\pagebreak
\textbf{Other factors:}
\begin{itemize}
	\item Illumination
	\item Occlusion: partial observation
	\item Ambiguity: some objects may look a lot like others; different perspectives may look like different objects
\end{itemize}

\subsection*{Human Vision}
\begin{center} 
	\includegraphics*[width=\textwidth]{L1_27.png} 
\end{center}

\subsection*{3D Vision}
How do humans and animals perceive depth?
\begin{itemize}
	\item Binocular vision: 2 eyes instead of 1
	\item Structure from Motion (SIM): walking around an object allows you to build a 3D model.
\end{itemize}
How do robots perceive depth?
\begin{itemize}
	\item Stereo camera
	\item Time of flight
	\item Structured light
\end{itemize}

\subsection*{2D to 3D Projection}
\begin{center} 
	\includegraphics*[width=\textwidth]{L1_28.png} 
\end{center}
\begin{itemize}
	\item Knowing just 2D coordinate $(u, v)$, we don't have enough information to compute the 3D point location $(X, Y, Z)$
	\item However, with an additional depth channel we can. (RGB-D image).
	\begin{itemize}
        \item The image on the right is an RGB-D image.  Each pixel records the depth value $Z$ (in meter or millimeter)
    \end{itemize}
\end{itemize}
We can combine the two images to form a single, 3D image.
\begin{center} 
	\includegraphics*[width=\textwidth]{L1_29.png} 
\end{center}
\begin{itemize}
	\item Depth image $\rightarrow$ 3D point clouds:
	\item A pixel with
	\begin{itemize}
        \item image coordinate $(u, v)$
        \item Depth value = $Z$
        \item Focal length $f$
    \end{itemize} 
    \item Its 3D location $(X, Y, Z)$ in camera coordinate can be computed by:
    \[X = \frac{u}{f} \cdot Z \hspace{2cm} Y = \frac{v}{f} \cdot Z\]
    \item $Z$: reading from the depth image
\end{itemize}

\subsection*{Summary:}
\begin{center} 
	\includegraphics*[width=\textwidth]{L1_30.png} 
\end{center}

\subsection*{World Coordinate to Camera Coordinate}
\begin{itemize}
	\item In order to apply the camera model we described so far, the 3D point $(X, Y, Z)$ must be expressed in \underline{camera coordinates} (i.e.; centered at the camera origin)
	\item However, the world coordinate can be different from the \textit{camera coordinates}.
	\item Requires an additional transformation
\end{itemize}
\begin{center} 
	\includegraphics*[width=\textwidth]{L1_31.png} 
\end{center}

\subsection*{Camera: Putting Everything Together}
\begin{center} 
	\includegraphics*[width=\textwidth]{L1_32.png} 
\end{center}
\[x = K[R|t]X\]
\begin{itemize}
	\item Map a 3D point $X$ into a 2D coordinate in image $x$
	\item How to describe its \textit{pose} in the world?  (extrinsic matrix)
	\item How to describe its internal parameters?  (intrinsic matrix)
\end{itemize}

\subsection*{Camera: Calibration}
Goal: estimate the camera parameters
\begin{itemize}
	\item Version 1: solve for projection matrix
	\[X = \begin{bmatrix} wx \\ wy \\ w \end{bmatrix} = \begin{bmatrix} * & * & * & * \\ * & * & * & * \\ * & * & * & * \end{bmatrix} \begin{bmatrix} X \\ Y \\ Z \\ 1 \end{bmatrix} = PX\]
    \item Version 2: solve for camera parameters separately
    \begin{itemize}
        \item Intrinsic (focal length, principle point, pixel size)
        \item Extrinsic (rotation angles, translation)
    \end{itemize}
\end{itemize}
To calibrate:
\begin{enumerate}
    \item Identify correspondance between image and scene
    \item Compute mapping from scene to image
\end{enumerate}
Requirement
\begin{enumerate}
    \item Must know geometry very accurately
    \item Must know correspondance
\end{enumerate}
\begin{align*}
    x_i &= PX_i \\
    \begin{bmatrix} u_i \\ v_i \\ 1 \end{bmatrix} &\cong \begin{bmatrix} m_{00} & m_{01} & m_{02} & m_{03} \\ m_{10} & m_{11} & m_{12} & m_{13} \\ m_{20} & m_{21} & m_{22} & m_{23} \end{bmatrix} \begin{bmatrix} X_i \\ Y_i \\ Z_i \\ 1 \end{bmatrix}
\end{align*}
Robust camera calibration is still an open challenge!



\end{document}