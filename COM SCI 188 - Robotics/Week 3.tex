% document formatting
\documentclass[10pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[left=1in,right=1in,top=1in,bottom=1in]{geometry}
\usepackage[T1]{fontenc}
\usepackage{xcolor}

% math symbols, etc.
\usepackage{amsmath, amsfonts, amssymb, amsthm}
\usepackage{xcolor}
\usepackage[dvipsnames]{xcolor}

% lists
\usepackage{enumerate}
\usepackage{tabularx}

% images
\usepackage{graphicx} % for images

% code blocks
\usepackage{minted, listings} 

% verbatim greek
\usepackage{alphabeta}

\newcommand{\dd}{\text{d}}

\graphicspath{{./assets/images/Week 3}}

\title{CS 188 Robotics Week 3} 

\author{Aidan Jan}

\date{\today}

\begin{document}
\maketitle 

\section*{Model Predictive Control}
\begin{center} 
	\includegraphics*[width=\textwidth]{L1_1.png} 
\end{center}
\textbf{Model predictive control (MPC)} is an optimal control technique in which the calculated control actions minimize a cost function for a constrained dynamical system over a finite, receding, horizon.
\begin{itemize}
	\item Predicts future system behavior using a \textbf{model}
	\item Solves an \textbf{optimization} problem at each step.
	\item Applies only the first control input at each step (iterative).
	\item Repeats this process continuously (receding horizon).
	\item Handles input and output \textbf{constraints} directly.
\end{itemize}

\subsection*{System Identification}
\textbf{\dots building a mathematical model} of a dynamic system \textbf{from measured data}

\begin{center} 
	\includegraphics*[width=0.7\textwidth]{L1_2.png} 
\end{center}

\subsection*{Model Predictive Contouring Control (MPCC++)}
\begin{center} 
	\includegraphics*[width=0.9\textwidth]{L1_3.png} 
\end{center}

\section*{Cameras and 2D Perception}
\subsection*{Color Camera}
\begin{itemize}
	\item Cameras are the primary sensor for many robotic platforms
	\item One of the cheapest and richest sensors is a camera
	\item Many other sensors are built on top of the color camera
\end{itemize}

\subsection*{Images Representation}
\begin{itemize}
	\item An image is basically a 2D array of intensity/color values
	\item Image types:
	\begin{center} 
        \includegraphics*[width=0.9\textwidth]{L1_4.png} 
    \end{center}
\end{itemize}

\subsubsection*{Grayscale Images}
A grid (2D matrix) of intensity values:
\begin{center} 
	\includegraphics*[width=\textwidth]{L1_5.png} 
\end{center}
\begin{itemize}
	\item \underline{Pixel:} A "picture element" that contains the light intensity at some location $(x, y)$ in the image Referred to as $I(x, y)$
	\item \underline{Image Resolution:} expressed in terms of Width and Height of the image
\end{itemize}

\subsection*{Camera and Image Formation}
\begin{itemize}
	\item How we get the image (Image formation)
	\item Pinhole Camera Model
	\item 2D computer vision tasks and challenges
\end{itemize}

\subsubsection*{Image Formation}
\begin{center} 
	\includegraphics*[width=\textwidth]{L1_6.png} 
\end{center}

\subsubsection*{Pinhole Camera Model}
\textbf{Pinhole image:} Natural phenomenon, known during classical period in China and Greece (e.g., 470 BCE to 390 BCE).  
\begin{itemize}
    \item Used for \underline{art creation} and religious ceremony in the ancient times.
	\item Expensive to record the image (drawing)
\end{itemize}
\begin{center} 
	\includegraphics*[width=0.8\textwidth]{L1_7.png} \\
    \includegraphics*[width=0.8\textwidth]{L1_8.png} 
\end{center}
\begin{itemize}
	\item Light sensitive material was used as film.  
	\item Hard to store, loses color after a while
\end{itemize}
\textbf{Today:} photon sensors are CCD, CMOS, etc.

\subsubsection*{Human Vision}
\begin{center} 
	\includegraphics*[width=0.8\textwidth]{L1_9.png} 
\end{center}
\subsubsection*{Why do we need a pinhole?}
\begin{center} 
	\includegraphics*[width=\textwidth]{L1_10.png} 
\end{center}
Light rays from many different parts of the scene strike the same point on the paper.
\begin{center} 
	\includegraphics*[width=\textwidth]{L1_11.png} 
\end{center}
Each point on the image plane \underline{sees light from only one direction} - the one that passes through the pinhole.

\subsubsection*{Problem with Pinhole Camera}
The pinhole size:
\begin{itemize}
	\item If large, blurry
	\item If small, not enough light
	\item When the pinhole size is extremely small, we will see the diffraction effect through the pinhole, resulting in the blurry image
\end{itemize}
\textbf{Solution:} refraction (lenses)
\begin{itemize}
	\item Essentially add multiple pinhole images
	\item Shift them to align using the light \textbf{refraction}
	\item However, this alignment works only for \underline{one depth} (need the object and image plane to stay in focus.)
\end{itemize}
\begin{center} 
	\includegraphics*[width=\textwidth]{L1_12.png} 
\end{center}

\pagebreak
\subsubsection*{Lenses Issues (depth of field)}
Only objects on focus plane are in "perfect" focus
\begin{center} 
	\includegraphics*[width=\textwidth]{L1_13.png} 
\end{center}
\[\frac{1}{D} + \frac{1}{D'} = \frac{1}{f}\]
where $D$ is the distance of a focus plane to the lens plane, $D'$ is the distance of the image plane to the lens plane, and $f$ is the focal length of the lens.
\begin{itemize}
	\item Objects close to the focus plane are in better focus
	\item Objects further away are not.
\end{itemize}

\subsection*{Camera Terminology}
These terms will be defined below.
\begin{itemize}
	\item Focal length
	\item Field of view
	\item Aperture
	\item Camera intrinsic
	\item Camera extrinsic
\end{itemize}

\subsection*{Pinhole Camera Geometry}
Motivation
\begin{itemize}
	\item Physics of real cameras are all different (too tedious to model all of them).
	\item But they all try their best to approximate pinhole camera.
	\item So \textbf{in most of computer vision subjects, we model all cameras mathematically as a pinhole camera.}
\end{itemize}

\subsubsection*{Field of View (FOV)}
\begin{center} 
	\includegraphics*[width=0.9\textwidth]{L1_14.png} 
\end{center}
\[\alpha = 2 \arctan \frac{d}{2f}\]
\begin{itemize}
	\item The unit of FoV $\alpha$ is a degree.
	\item Each camera has two FoV: vertical and horizontal.
\end{itemize}

\subsubsection*{Focal Length}
\begin{center} 
	\includegraphics*[width=0.9\textwidth]{L1_15.png} 
\end{center}
\subsubsection*{Camera Projection}
\begin{center} 
	\includegraphics*[width=\textwidth]{L1_16.png} 
\end{center}
\begin{align*}
    \textcolor{Green}{\frac{u}{f}}\: & \textcolor{Green}{= \frac{X}{Z}}\\
    \textcolor{red}{\frac{v}{f}}\: & \textcolor{red}{= \frac{Y}{Z}}
\end{align*}
In camera coordinates, the camera center is the origin.
\[p_{2d} = \begin{bmatrix} u \\ v \end{bmatrix} \hspace{2cm} p_{3d} = \begin{bmatrix} X \\ Y \\ Z \end{bmatrix}\]
\begin{center} 
	\includegraphics*[width=\textwidth]{L1_17.png} \\
    \includegraphics*[width=\textwidth]{L1_18.png} 
\end{center}


\subsubsection*{Image Coordinate}
\begin{center} 
	\includegraphics*[width=\textwidth]{L1_19.png} 
\end{center}
Until now, we use 2D coordinate conventions that are \textbf{consistent} with the 3D camera coordinate.  However, if your application uses a different 2D coordiante, you'll need to further transform the $(u, v)$.\\\\
For example, consider the following cases where we change the direction of the axes and the position of the origin.
\begin{center} 
	\includegraphics*[width=\textwidth]{L1_20.png} \\
    \rule{\textwidth}{1pt}
    \vspace{1cm}
    \includegraphics*[width=\textwidth]{L1_21.png}
\end{center}

\subsubsection*{Popular Camera Coordinate Systems}
\begin{center} 
	\includegraphics*[width=\textwidth]{L1_22.png} 
\end{center}

\subsection*{Camera Projection:}
\[\lambda \begin{bmatrix} u \\ v \\ f \end{bmatrix} = \begin{bmatrix} X \\ Y \\ Z \end{bmatrix}\]
\begin{center} 
	\includegraphics*[width=\textwidth]{L1_23.png} 
\end{center}

\section*{Computer Vision}
A quick overview: what is computer vision?
\begin{center} 
	\includegraphics*[width=\textwidth]{L1_24.png} 
\end{center}

\subsection*{What Makes 2D Computer Vision Hard?}
\begin{center} 
	\includegraphics*[width=\textwidth]{L1_25.png} 
\end{center}
Variation: same cat, different poses, view points, \dots
\begin{center} 
	\includegraphics*[width=\textwidth]{L1_26.png} 
\end{center}
More variation: different cats, different shapes, colors, textures, \dots\\\\

\pagebreak
\textbf{Other factors:}
\begin{itemize}
	\item Illumination
	\item Occlusion: partial observation
	\item Ambiguity: some objects may look a lot like others; different perspectives may look like different objects
\end{itemize}

\subsection*{Human Vision}
\begin{center} 
	\includegraphics*[width=\textwidth]{L1_27.png} 
\end{center}

\subsection*{3D Vision}
How do humans and animals perceive depth?
\begin{itemize}
	\item Binocular vision: 2 eyes instead of 1
	\item Structure from Motion (SIM): walking around an object allows you to build a 3D model.
\end{itemize}
How do robots perceive depth?
\begin{itemize}
	\item Stereo camera
	\item Time of flight
	\item Structured light
\end{itemize}

\subsection*{2D to 3D Projection}
\begin{center} 
	\includegraphics*[width=\textwidth]{L1_28.png} 
\end{center}
\begin{itemize}
	\item Knowing just 2D coordinate $(u, v)$, we don't have enough information to compute the 3D point location $(X, Y, Z)$
	\item However, with an additional depth channel we can. (RGB-D image).
	\begin{itemize}
        \item The image on the right is an RGB-D image.  Each pixel records the depth value $Z$ (in meter or millimeter)
    \end{itemize}
\end{itemize}
We can combine the two images to form a single, 3D image.
\begin{center} 
	\includegraphics*[width=\textwidth]{L1_29.png} 
\end{center}
\begin{itemize}
	\item Depth image $\rightarrow$ 3D point clouds:
	\item A pixel with
	\begin{itemize}
        \item image coordinate $(u, v)$
        \item Depth value = $Z$
        \item Focal length $f$
    \end{itemize} 
    \item Its 3D location $(X, Y, Z)$ in camera coordinate can be computed by:
    \[X = \frac{u}{f} \cdot Z \hspace{2cm} Y = \frac{v}{f} \cdot Z\]
    \item $Z$: reading from the depth image
\end{itemize}

\subsection*{Summary:}
\begin{center} 
	\includegraphics*[width=\textwidth]{L1_30.png} 
\end{center}

\subsection*{World Coordinate to Camera Coordinate}
\begin{itemize}
	\item In order to apply the camera model we described so far, the 3D point $(X, Y, Z)$ must be expressed in \underline{camera coordinates} (i.e.; centered at the camera origin)
	\item However, the world coordinate can be different from the \textit{camera coordinates}.
	\item Requires an additional transformation
\end{itemize}
\begin{center} 
	\includegraphics*[width=\textwidth]{L1_31.png} 
\end{center}

\subsection*{Camera: Putting Everything Together}
\begin{center} 
	\includegraphics*[width=\textwidth]{L1_32.png} 
\end{center}
\[x = K[R|t]X\]
\begin{itemize}
	\item Map a 3D point $X$ into a 2D coordinate in image $x$
	\item How to describe its \textit{pose} in the world?  (extrinsic matrix)
	\item How to describe its internal parameters?  (intrinsic matrix)
\end{itemize}

\section*{Camera: Calibration}
\begin{center} 
	\includegraphics*[width=\textwidth]{L2_1.png} 
\end{center}
Goal: estimate the camera parameters
\begin{itemize}
	\item Version 1: solve for projection matrix
	\[X = \begin{bmatrix} wx \\ wy \\ w \end{bmatrix} = \begin{bmatrix} * & * & * & * \\ * & * & * & * \\ * & * & * & * \end{bmatrix} \begin{bmatrix} X \\ Y \\ Z \\ 1 \end{bmatrix} = PX\]
    \item Version 2: solve for camera parameters separately
    \begin{itemize}
        \item Intrinsic (focal length, principle point, pixel size)
        \item Extrinsic (rotation angles, translation)
    \end{itemize}
\end{itemize}
During the hand-eye calibration process, we move robot to a known position in its base frame, and captures the image with camera and detects the robot's hand's position (approximated with the red ball)
\begin{center} 
	\includegraphics*[width=0.6\textwidth]{L2_2.png} 
\end{center}
To calibrate:
\begin{enumerate}
    \item Identify correspondance between image and scene
    \item Compute mapping from scene to image
\end{enumerate}
Requirement
\begin{enumerate}
    \item Must know geometry very accurately
    \item Must know correspondance
\end{enumerate}
\begin{align*}
    x_i &= PX_i \\
    \begin{bmatrix} u_i \\ v_i \\ 1 \end{bmatrix} &\cong \begin{bmatrix} m_{00} & m_{01} & m_{02} & m_{03} \\ m_{10} & m_{11} & m_{12} & m_{13} \\ m_{20} & m_{21} & m_{22} & m_{23} \end{bmatrix} \begin{bmatrix} X_i \\ Y_i \\ Z_i \\ 1 \end{bmatrix}
\end{align*}
You want to find a \textbf{rotation matrix} $\mathbb{R} \in SO(3)$, and a \textbf{translation vector} $t \in \mathbb{R}^3$ such that $p_i^B \equiv Rp_i^A + t$.  (This is the matrix above.)

\subsection*{The Kabsch-Umeyama Algorithm}
\begin{enumerate}
    \item Compute centroids
    \[\bar{p}^A = \frac{1}{N} \sum_{i = 1}^N p_i^A,\hspace{1cm} \bar{p}^B = \frac{1}{N} \sum_{i = 1}^N p_i^B\]
    \item Center the points
    \[q_i^A = p_i^A - \bar{p}^A, \hspace{1cm} q_i^B = p_i^B - \bar{p}^B\]
    \item Compute the cross-covariance matrix
    \[H = \sum_{i = 1}^N q_i^A(q_i^B)^T\]
    \item Apply SVD (singular value decomposition)
    \[H = U\Sigma V^T\]
    \item Construct the rotation matrix
    \[R = VU^T\]
    \begin{itemize}
        \item If $\det(R) < 0$, correct for reflection:
        \item $V$[:, 3] $\leftarrow -V$[:, 3], (flip the third column of $V$)
    \end{itemize}
    \item Compute the translation vector
    \[t = \bar{p}^B - R \bar{p}^A\]
    \item Output the transformation (combine rotation and translation into a rigid body transformation)
\end{enumerate}
Pseudo Code
\begin{verbatim}
    // compute centroids
    centroid_A = mean of A_points
    centroid_B = mean of B_points

    // center point sets
    A_centered = [Ai - centroid_A for Ai in A_points]
    B_centered = [Bi - centroid_B for Bi in B_points]

    // Compute cross-covariance matrix H:
    H = zero 3×3 matrix
    for i = 1 to N:
        H += outer_product(A_centered[i], B_centered[i])

    // Perform Singular Value Decomposition:
    [U, Σ, V_T] = SVD(H)

    // Compute rotation matrix R:
    R = V_T^T * U^T

    // Handle reflection case (det(R) < 0)
    if determinant(R) < 0:
        V_T[2] = -V_T[2] # flip the 3rd row of V_T
        R = V_T^T * U^T
    
    // Compute translation vector t:
    t = centroid_B - R * centroid_A
    
    RETURN R, t
\end{verbatim}

\section*{Computer Vision Basics}
\subsection*{Basic Image Processing}
There are many operations we can apply on a image.  Here are some examples:
\begin{itemize}
	\item Resize
	\item Color manipulation
	\item Contrast
	\item Brightness
	\item Gamma
	\item Filtering
	\item Denosing
\end{itemize}

\subsubsection*{Brightness (overall intensity of an image)}
Adjust brightness: $g(x) = f(x) + \beta$\\
Increasing (or decreasing) the $\beta$ value will add (or subtract) a constant value to every pixel.  Pixel values outside of the [0, 1] range will be saturated (i.e., a pixel value higher or lower than 1 or 0 will be clamped to 1 or 0).
\begin{center} 
	\includegraphics*[width=\textwidth]{L2_3.png} 
\end{center}

\subsubsection*{Contrast}
Adjust contrast: $g(x) = \alpha f(x)$\\
The $\alpha$ parameter will modify how the intensity distribution spread.  If $\alpha < 1$, the color levels will be compressed and the result will be an image with less contrast.
\begin{center} 
	\includegraphics*[width=\textwidth]{L2_4.png} 
\end{center}

\subsubsection*{Gamma Correction}
\begin{itemize}
	\item Gamma correction can be used to correct the brightness of an image by using a nonlinear transformation between the input values and the mapped output values: $g(x) = [f(x)]^{\frac{1}{\gamma}}$
	\item As this relation is nonlinear, the effect will not be the same for all the pixels and will depend to their original value.
	\item When $\gamma < 1$, the original dark regions will be brighter and the histogram will be shifted to the right.  (reducing contrast)
\end{itemize}
\begin{center} 
	\includegraphics*[width=0.5\textwidth]{L2_5.png} \\
    \includegraphics*[width=\textwidth]{L2_6.png} 
\end{center}


\subsubsection*{Image Filtering}
Image filtering: compute function of local neighborhood at each position
\begin{itemize}
	\item Filtering can be used for:
	\begin{itemize}
        \item Enhance images (denoise, increase contrast, etc.)
        \item Extract information from images (textures, edges, distinctive points, etc.)
        \item Detect patterns (template matching)
        \item Deep convolutional networks
    \end{itemize}
\end{itemize}

\subsection*{Convolution Operation}
\begin{itemize}
	\item Sliding the kernel (small 2D matrix) over the 2D image (big 2D matrix)
	\item Performing elementwise multiplication with the part of the input it is currently on
	\item Sum up the result into a single output pixel
	\item Slide to the next location
\end{itemize}
\begin{center} 
	\includegraphics*[width=\textwidth]{L2_7.png} 
\end{center}

\subsection*{Padding}
\begin{itemize}
	\item What about near the edge?
	\begin{itemize}
        \item the filter window falls off the edge of the image
    \end{itemize}
	\item Boundary \underline{padding}
	\begin{itemize}
        \item Zero (smaller value around boundary)
        \item Circular/warp (best for panorama)
        \item Replicate
        \item Symmetric        
    \end{itemize}
\end{itemize}
\begin{center} 
	\includegraphics*[width=0.6\textwidth]{L2_8.png} 
\end{center}

\subsection*{Image Filtering}
\begin{center} 
	\includegraphics*[width=\textwidth]{L2_9.png} 
\end{center}

\subsection*{Image Filtering: Box Filter}
\begin{itemize}
	\item What does it do?
	\begin{itemize}
        \item Replaces each pixel with an average of its neighborhood
        \item Achieve smoothing effect (remove sharp features)
    \end{itemize}
\end{itemize}
\begin{center} 
	\includegraphics*[width=\textwidth]{L2_10.png} 
\end{center}

\subsection*{Example: Convolution Kernels}
\begin{center} 
	\includegraphics*[width=0.8\textwidth]{L2_11.png} \\
    \includegraphics*[width=0.8\textwidth]{L2_12.png} 
\end{center}

\subsection*{Other Filters}
\subsubsection*{Gaussian Kernel}
\begin{center} 
	\includegraphics*[width=\textwidth]{L2_13.png} 
\end{center}
\subsubsection*{Edge Detection}
\begin{center} 
	\includegraphics*[width=0.9\textwidth]{L2_14.png} 
\end{center}
\begin{itemize}
	\item Goal: Identify sudden changes (discontinuities) in an image
	\item Why?  Intuitively, edges carry a lot of the semantic and shape information from the image.
\end{itemize}
\begin{center} 
	\includegraphics*[width=\textwidth]{L2_15.png} 
\end{center}

\subsection*{Edge Detection in Robotics}
\begin{center} 
	\includegraphics*[width=\textwidth]{L2_16.png} 
\end{center}

\pagebreak
\section*{Convolution Neural Networks}
\subsection*{Convnet Building Blocks}
\begin{center} 
	\includegraphics*[width=\textwidth]{L2_17.png} 
\end{center}

\subsection*{Convolutional Layer}
\begin{itemize}
	\item Input: an image
	\item Processing: convolution with \textbf{\underline{multiple}} filters
	\item Output: an image, ($\#$ channels = $\#$ filters), weighted sum of input
	\item \texttt{torch.nn.Conv2d(in\_channels, out\_channels, kernel\_size, stride=1, } \\\texttt{padding=0, padding\_mode='zeros')}
	\item \textbf{Convolve} the filter with the image i.e., "slide over the image spatially, computing dot products"
	\item A convolutional layer can have multiple filters.
\end{itemize}
\begin{center} 
	\includegraphics*[width=0.8\textwidth]{L2_18.png} 
\end{center}
If we had \textbf{6} \underline{5x5 filters}, we'll get 6 separate activation maps.  We stack these up to get a "new image" of size 28x28x6!

\subsection*{Kernel Size}
How big the filter for a layer is typically between 1x1 and 7x7.
\begin{itemize}
	\item Larger kernel aggregate information from larger local region (bigger receptive field)
	\item 1x1 is just linear combination of channels in previous image (no spatial processing)
	\item Filters usually have the same number of channels as the input image. (e.g., an RGB image has 3 channels, so one filter is a $n \times n \times 3$ rather than just a 2d kernel)
\end{itemize}

\subsection*{Padding}
\begin{itemize}
	\item Convolutions have problems on edges
    \item Do nothing: output a little smaller than input
    \item Pad: add extra pixels on edge
\end{itemize}
\begin{center} 
	\includegraphics*[width=\textwidth]{L2_19.png} 
\end{center}

\subsection*{Stride}
\begin{itemize}
	\item How far to move filter between applications
	\item We've done stride 1 convolutions up until now, approximately preserves image size
	\item Could move filter further, downsample image
\end{itemize}

\subsection*{Input - Output Size}
\begin{itemize}
	\item Input $C_{\text{in}} \times H_{\text{in}} \times W_{\text{in}}$ image
	\item \texttt{Conv2d(K, N, S, P)}: Number of kernels \texttt{K}, kernel size [\texttt{N} $\times$ \texttt{N}], stride \texttt{S}, padding \texttt{P}
	\item What's the output feature size ($C_{\text{out}}$, $H_{\text{out}}$, $W_{\text{out}}$)?
	\begin{itemize}
        \item $H_{\text{out}} = \lfloor \frac{H_{\text{in}} + 2 \cdot \text{padding} - (N - 1) - 1}{\text{stride}} + 1 \rfloor$
        \item $W_{\text{out}} = \lfloor \frac{W_{\text{in}} + 2 \cdot \text{padding} - (N - 1) - 1}{\text{stride}} + 1 \rfloor$
        \item $C_{\text{out}} = K$
    \end{itemize}
\end{itemize}

\subsection*{Pooling Layer}
\begin{itemize}
	\item Input: an image
	\item Processing: pool pixel values over region
	\item Output: an image, shrunk by a factor of the stride
\end{itemize}
Hyperparameters:
\begin{itemize}
	\item What kind of pooling?  Average, mean, max, min
	\item How big of stride?  Controls downsampling
	\item How big of region?  Usually not bigger than stride
\end{itemize}
Most common: maxpooling 2x2 stride of 2, or 3x3 stride of 3.
\begin{center} 
	\includegraphics*[width=0.7\textwidth]{L2_20.png} 
\end{center}

\subsection*{Fully Connected Layer (InnerProduct)}
Every input neuron connects to every output neuron

\begin{center} 
	\includegraphics*[width=\textwidth]{L2_21.png} 
\end{center}
\begin{itemize}
	\item Often used to go from image feature map $\rightarrow$ final output or map image features to a single vector
	\item Eliminates spatial information
\end{itemize}

\subsection*{Activation Layer}
\begin{itemize}
	\item Used to increase non-linearity of the network without affecting receptive fields of the conv layers.
	\item Most commonly used activation layer: ReLU
\end{itemize}
\begin{center} 
	\includegraphics*[width=\textwidth]{L2_22.png} 
\end{center}

\subsection*{Activation Layer: Softmax}
\begin{itemize}
	\item A special kind of activation layer, usually at the end of fully connected layer and before cross-entropy loss.
	\item Can be viewed as a fancy \underline{normalization function} that produce a \underline{probability distribution vector}
\end{itemize}
\begin{center} 
	\includegraphics*[width=\textwidth]{L2_23.png} 
\end{center}

\subsection*{Loss Function}
A loss function scores how far off a prediction is from the desired "target" output.
\begin{itemize}
	\item loss(prediction, target) returns a number called "the loss"
	\item Big Loss = Bad Error
	\item Small Loss = Minor Error
	\item Zero Loss = No Error
\end{itemize}

\begin{center}
\begin{tabularx}{\linewidth}{|X|X|}
    \hline \rule{0pt}{2.5ex}
    \textbf{Regression} & \textbf{Classification}\\
    \hline \rule{0pt}{2.5ex}
    Predict real-valued output & Predict category output\\
    \hline \rule{0pt}{2.5ex}
    What temperature will it be tomorrow? & Will it be sunny tomorrow?  Often predicts continuous values which are probabilities of different labels, use the one with max probabilities as predicted class.\\
    \hline \rule{0pt}{2.5ex}
    Loss function: squared error, L1 error, \dots & Loss function: cross-entropy\\
    \hline
\end{tabularx}
\end{center}

\subsection*{Regression}
\begin{center} 
	\includegraphics*[width=\textwidth]{L2_24.png} 
\end{center}
L1 loss is more robust to outliers, but its \textbf{derivatives around zero are not continuous}, making it inefficient to find the solution.  L2 loss is \textbf{sensitive to outliers}, but gives a more stable and closed form solution.
\begin{itemize}
	\item Smooth L1 is somewhat a combination of the two.
\end{itemize}

\subsection*{Classification: CrossEntropy}
\begin{center} 
	\includegraphics*[width=\textwidth]{L2_25.png} 
\end{center}
\begin{itemize}
	\item The network output prediction is compared to the ground truth vector
\end{itemize}
\begin{center} 
	\includegraphics*[width=\textwidth]{L2_26.png} 
\end{center}
\begin{itemize}
	\item Finally, weights update through backpropagation.
	\item Change the network weight $W$ so that the loss $L$ is minimized.
\end{itemize}

\subsection*{Summary of Convnet Building Blocks}
\begin{itemize}
	\item Convolutional layers:
	\begin{itemize}
        \item Used convolution operation to extract features
        \item Convolution with stride > 1 will downsample image
    \end{itemize}
	\item Pooling layers:
	\begin{itemize}
        \item Used to downsample feature maps, making processing more efficient
    \end{itemize}
	\item Fully connected layers:
	\begin{itemize}
        \item Often used as last layer, to map image features $\rightarrow$ prediction
        \item No spatial information
        \item Inefficient: lots of weights, no weight sharing
    \end{itemize}
    \item Activation layers: increase non-linearity of the network
    \item Loss function: classification and regression
\end{itemize}



\end{document}