# Week 8
## The Ethics of Computing
### Patents
* Atanasoff – Berry Computer
  * Iowa State College (Ames) 1937-1942
  * First attempt at an electronic digital computing machine
    * Binary arithmetic
    * Analog-digital input
    * Digital – analog output
    * Simple memory
    * Simple programming
    * A special purpose computer
    
---

* Atanasoff and Berry go off to war work in Washington
* Mauchly stays in Philadelphia, working on ENIAC, generally considered to be the first general purpose electronic computer
* Immediately after the war, Mauchly files patents ( with J. Presper Eckert) on key ENIAC components, even though many of the key concepts were Atanasoff’s.
* Mauchly sells out to Sperry Rand, who begins developing computers on its own, and licensing rights to other firms, such as IBM and Honeywell.
* Honeywell finds out about Atanasoff’s early work that is evident in Mauchly’s patents.
* Honeywell sues Sperry Rand in 1967, and in 1973 a federal judge in Minneapolis invalidated the Mauchly patents.<br>

  > “Eckert and Mauchly did not themselves first invent the automatic electronic digital computer, but instead derived that subject matter from one Dr. John Vincent Atanasoff.”<br>
    * U. S. District Court Judge Earl R. Larson

### Hacking
* Most hacking seems to be done by malicious individuals, not necessarily computer scientists. But why can’t the university-trained computer scientists among us provide better security around out computer systems?

### Privacy
* Personal privacy seems to be secondary in the developing of large computer systems. Government, industrial, banking, an educational, have all been compromised.
* Often, there seems to be a lack of any privacy-protecting design in the development of large data bases.
* *Quo vadis*?

### Risk and Reliability
* Therac-25
* Boeing 737-MAX
* Computing allows for more rebust risk calculations, especially in complex engineered systems such as power plants, large industrial installations, etc.

### Artificial Intelligence (AI)
* Facial Recognition
* Criminal Bail Calculations
* Autonomous Vehicles

#### Examples
* **Facial Recognition Arrests**
* This gentleman was arrested by the Detroit police based on a facial recognition hit and held for 24 hr until his alibi could be verified.
* Previous to his arrest, the District Attorney had notified the police he would not file criminal charges based on facial recognition identification.<br>
![image](https://github.com/ajan890/Notes/assets/66571533/ef0414bc-9629-45dc-9a46-7421c7344d70)<br>

* **Facial Recognition in China**<br>
![image](https://github.com/ajan890/Notes/assets/66571533/733ea643-0d82-403d-a6c3-0f7c6e56bbd8)<br>

* Do we want this level of surveillance in the United States?

#### Case Study: UBER Autonomous Vehicle Crash
* In 2018, a Volvo fitted with UBER’s Automatic Driving System (ADS) hit a woman walking her bicycle across the road.
* The test driver was playing with her cellphone at the time, and thus ignored the warnings from the ADS system. <br>
  > “At the time when the ADS detected the pedestrian for the first time, 5.6 seconds before impact, she was positioned approximately in the middle of the two left turn lanes ... Although the ADS sensed the pedestrian nearly 6 seconds before the impact, the system never classified her as a pedestrian—or predicted correctly her goal as a jaywalking pedestrian or a cyclist—because she was crossing the N. Mill Avenue at a location without a crosswalk; **the system design did not include a consideration for jaywalking pedestrians.** Instead, the system had initially classified her as an other object which are not assigned goals. As the ADS changed the classification of the pedestrian several times—alternating between vehicle, bicycle, and an other— the system was unable to correctly predict the path of the detected object.”<br> 

  > "Only when the ADS determined that the object’s currently detected location was on the path of the ATG vehicle—1.2 seconds before impact—the system recognized an emergency situation, an imminent collision. At that time, because preventing the collision would have required extreme braking or steering actions—beyond the design specifications—the ADS initiated suppression of its motion plan. One second later, the vehicle was still on the collision path with the pedestrian, and preventing the collision still required an extreme avoidance maneuverer; per design, the system did not engage emergency brakes, but rather provided an auditory alert to the vehicle operator as it initiated a plan for the vehicle slowdown.” <br>
    * NTSB Report, November 2019

* Can UBER's Automated Driving System understand that it's Grizzly 399 and her four cubs that have just bounded onto the road in Wyoming?<br>
![image](https://github.com/ajan890/Notes/assets/66571533/a7753246-aab6-42ba-9070-70d03c32960a)<br>
* Or can UBER's detection systems identify 399 and her cubs as they wander through downtown Jackson in the evening?<br>
![image](https://github.com/ajan890/Notes/assets/66571533/3a8334b7-0787-424a-acd7-cfd27a29cc25)<br>
* Or a four-year-old running into the street, since he's not in a crosswalk?<br>
![image](https://github.com/ajan890/Notes/assets/66571533/0b4b582f-1cd3-4e4c-b956-ab99bb81e340)<br>

### Other Issues in Computing
* Whistle Blowing
* Use in education – digital divide
* Use in warfare – autonomous drones
* Effect of e-commerce on Main Street
* Open racial and religious discrimination

#### Voting Technology
* Since the 2000 General Election, voting machines have been the subject of constant complaints. Early attempts at automated vote counting we based on punch cards, later evolving to “mark-sense” cards. Early electronic voting machines were plagued with machine interface problems, but by the 2020 election virtually all complaints against machine counts were unfounded, since the ability to count the paper ballots was a back up.
* Prior to about 1970, much of the United States used large paper ballots – mark your choice with an “X” – with the ballots counted in the precinct.
* An alternative – especially along the east coast – was the mechanical voting machine. Unfortunately, it left no paper trail and was thus subject to political manipulation.
* Beginning in the 1970s, many jurisdictions began using punch cards – Hollerith or IBM cards – with the voter punching out their choice.
  * **Aside: IBM Cards**
    * The voting cards could be sorted by an electronic sorter.
    * But on the day before the 1988 General Election, Ronnie Dugger, in the New Yorker, published a piece questioning the accuracy of punch-card voting.
    * Dugger’s predictions came true during the 2000 General Election, especially in Florida.
    * After the debacle in Florida, many jurisdictions switched to mark-sense voting – essentially a Scantron type card. In California, we kept mark-sense through the 2016 election in many cases.
* Some jurisdictions adopted early models of electronic voting machines, but many models had severe problems, especially those by Diebold – the cash register company.
* Electronic voting machines have evolved significantly over the last 20 years. There have been growing pains, but today’s machines seem accurate and reliable, especially since virtually all of them involve a physical paper trail.
* **The Future?**
  * Computer?
  * Cell phone?
  * All-absentee voting?

## Internet Connectivity
### Connectivity Milestones
* Two Periods
  * Before Internet (BI) vs. After Internet (AI)
  * Broadcast Media vs. Interactive Media
  * Audiences vs. Active Online Communities
* The Internet Begins
  * 1962 ARPA computer network vision
  * 1969 ARPANET (ARPA, BBN, UCLA)
  * 1981 Internet backbone evolves
  * 1984 Domain name system (>1000 hosts)
  * 1990 Hyper Text Markup Language (CERN)
  * 1991 World Wide Web (CERN)
  * 1993 Mosiac GUI (NCSA and Illinois University)
  * 1994 25 Year Anniversary: >3,800,000 hosts
  * 2004 35 Year Anniversary: >550 million users
  * 2019 50 Year Anniversary: Universality!

#### Packet Switching
![image](https://github.com/ajan890/Notes/assets/66571533/4990c3b3-c89b-4ac5-97f9-7aa55cedc6bb)<br>

#### Number of Internet Users
![image](https://github.com/ajan890/Notes/assets/66571533/05b77c33-5770-4b54-a95f-746cba9dc519)<br>

### Societal Effects
* Internet Services
  * Email
  * Browsers
* Information Distribution
  * Humongous repository
  * Virtually unlimited access
* Entertainment Modes
  * Online gaming
  * Video & music distribution
  * Personal publishing
* Social Networking
  * Friending and messaging
  * News sourcing
* Publishing and Grouping
  * Unlimited access
  * Group reinforcement
  * Hate amplification
  * Weaponization
* Cyber Crime
  * Illegal products/services
  * Ransomware

#### Newer Societal Effects
* **Sped up by COVID Pandemic**
* Work Models
  * Real-time collaboration
  * Distributed enterprises
  * Online meetings
* Education and Training
  * Serious games
  * Online courses & degrees
* Medicine
  * Digital Patient Records
  * Remote consulting/treatment
* Business Models
  * Search engines
  * Directed advertising
  * Distributed fund raising
  * Cryptocurrencies
  * *Gig Economy*
  * Sharing Economy
  * Influencing & live streaming
  * Click brokering
* Social Media
  * "A Place for Friends"
    * Friends share posts with each other
  * "A Place for Messages"
    * Social Media replacing text messaging
  * "A Place for Politics"
    * Trump used ~35000 tweets to talk directly to the US and world population
  * "A Call to Violence"
    * Used to incite racial, religious, anti-gay, and political violence in India and Indonesia
    * Jan. 6, 2021: Donald Trump rallied the capitol
* Dark Web - Illegal Business Activities
  * Silk Road: Anonymous marketplace
* Cyber Highway Robbers
  * Hackers encrypt data and demand bitcoin ransoms
* Internet of Things (IOT)
  * Cars
  * Televisions
  * Robots
  * Voice Assistants
  * Other equipment

### Five Ethical Cases:
#### Ethical Case 1: News and Publication
* Characteristics of News
  * Immediacy + Interactivity
  * Crowd Sourcing
  * Fake News vs. Real News
* Standards for Publication
  * Books, articles, movies, music
  * Ambiguous authority/accountability
  * Misinformation and information

#### Ethical Case 2: Groups and Hate
* Groups Benign, Bizarre, Dangerous
  * Ease of recognition and association
  * Conspiracy theories prevail
  * Internet of hate has emerged
* Industry Responds: Hate Awareness
  * Facebook implemented algorithms to detect hate messaging, same with many other platforms
  * Haugen testified that Facebook knowingly used algorithms that:
    1. Enhanced political divisiveness
    2. Led users to extreme conspiracy theories
    3. Were particularly effective on teenagers

#### Ethical Case 3: Weaponization
* Mass social media messages are used to influence personal preferences.
* An estimated 30% are created by bots with some degree of AI.
* Example: Posts manufactured in 2016 by "troll factory" Internet Research Agency in St. Petersburg, Russia created a demonstration outside a mosque to protest a visit by presidential candidate Clinton.  It was all fake news to inspire discord.
* **DARPA fights back with R&D program**
  * Influence Campaign Awareness and Sensemaking (INCAS)
  * Problems with Current Countermeasures
    – Manual and ad hoc primarily
    – Social listening and limited number of inputs are main sources
    – “Low and slow” campaigns are difficult to detect
    – Assessing influence with specific groups is very difficult
  * Proposed New Approach
    – TA1: Automated Influence Detection; identify influence indicators
    – TA2: Population Response Characterization; segment by psychographic and demographic attributes, worldviews, values and develop correlation statistics
    – TA3: Influence Campaign Modeling; techniques for analyst‐machine sensemaking
    – TA4: Data and Testbed Development; provide data feeds from many sources and testbed for demonstrating TA1, TA2, TA3 technologies<br>
![image](https://github.com/ajan890/Notes/assets/66571533/2108ce35-f0a9-4fe5-a1b2-3068703c84da)<br>

#### Ethical Case 4: Individual Behavior
* People say it’s easier to lie on a Blackberry or other messaging device<br>
  > 51% - Feel less guilty lying remotely<br>
* Have lied remotely at the workplace<br>
  > 67% - About something<br>
  > 43% - About being sick <br>
  > 23% - To hide incomplete work <br>
  > 18% - To hide a big mistake <br>
* Have lied remotely in personal life<br>
  > 40% - To family or partner <br>
  > 37% - About clothing and costs <br>
  > 35% - About looking good <br>
  > 35% - About eating and drinking <br>
  > 32% - About weight <br>
* From survey by UK firm 72 Point (http://news.yahoo.com, December 28, 2006)
* One can be anonymous as a sender causing untraceable harm to the receiver
  * Example: Malicious Presence (Disturbing)
    * Internet Trolls use the power of anonymity to target and harass individuals who most frequently are not personally known to them
* Legal and Ethical Issues are Open
  * Many people have been arrested for online etiquette that led to undesired effects
  * People fight back against the platforms for not protecting users
  * Perpetrators are publicly shamed online or doxxed

#### Ethical Case 5: Too Much Connectivity
* Commentators worry that we are losing the ability to "be in the moment" aside from posting our selfies, or to have normal people-to-people connections.
* Research studies suggest that the great amount of time spent in cyberspace and the overabundance of personal information may be major factors contributing the increase in serious psychological problems among teenagers and young adults
* Some believe it is on purpose; app designers apply psychological principles to keep users engaged
  * AI can be implemented based on preference out of an "individualized" feed
  * The cloud for a social media platform may feed clouds for search engines
  * Cloud for search engines may feed platforms for e-commerce
* **Is it already too late?**
  * People are starting to fight the big corporations
  * Public awareness
* **New Industry Design Ethics**<br>
  > Psychology should not be employed to promote app usage<br>
  > Success is not more "time in app"; Success is a better quality of life<br>
    * -Tristan Harris
  * Self regulation may not be enough; Facebook won't fix itself, there must be laws to govern all related problems with safety, privacy, and competition

### Summary: Ethical Issues of Internet Connectivity
* Harmful Behaviors
  * Can harmful Internet behaviors be prohibited by laws, like other types of threatening behaviors? If so, how would such laws fit in with accepted 1 st Amendment principles of free speech? And how would they be enforced?
* “Vigilante” Actions
  * Can group actions such as doxing or shaming substitute for strong laws? Are such actions fair and equitable? Who decides/controls?
* Web Addiction
  * Is limiting the amount of time people are spending online desirable? Is it possible?
  * Is it a special case for children and teens? What is the most equitable approach?
* Personal Ethical Framework
  * What can individual engineers do to help define and solve these problems?
